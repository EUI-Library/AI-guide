---
title: "Epistemology and Transparency"
format: html
lang: en
---

::: {.callout-note icon="false" title="<span style='font-size:1.2em; color: #133869ff;'>Content</span>"}
##### AI and the construction of scientific ‚Äútruth‚Äù
##### Interpretability and eXplainable Artificial Intelligence (XAI)
##### Limits of opaque architectures
##### Transparency as an epistemic condition
:::

Scientific research has historically been based on shared epistemological principles: *verifiability*, *falsifiability* and *methodological transparency*.  

These criteria, developed within the framework of logical positivism and subsequently consolidated through Karl Popper's falsificationism, have ensured that scientific results are <u>accumulable</u>, subject to <u>critical scrutiny</u> and open to <u>intersubjective control</u>. 

The introduction of AI, and in particular generative systems based on LLMs, now forces us to rethink these assumptions.  
AI does not merely provide advanced computational tools, but intervenes directly in the **production of knowledge** by *selecting*, *synthesising* and *organising sources*. 

This function reorients the processes of constructing "**scientific truth**‚Äù, redefining the relationship between data, theories and research communities.

## 1. AI and the construction of "scientific truth‚Äù

One of the most significant transformations concerns the way in which AI participates in the **selection** and **synthesis** of sources.  

Generative systems can draw on large text corpora, identify correlations and propose coherent syntheses, presenting them as information with scientific value. 

[ üëâ However, the probabilistic logic that governs these models does not coincide with traditional epistemic criteria: what is produced is not the result of deductive or inductive reasoning, but the projection of a statistical distribution learned from the data.]{.mark}

The notion of "*scientific truth*" risks being progressively replaced by a form of ‚Äú*linguistic plausibility*‚Äù, a discourse which, while presenting itself with syntactic coherence and rhetorical force, does not necessarily guarantee the <u>verifiability of its content</u>. 

This dynamic requires a clear distinction between **scientifically validated knowledge** and **output generated by AI systems**, to prevent the rhetorical power of AI-generated language from being mistaken for scientific evidence.  
From this perspective, AI does not merely produce texts, but acts as a <u>cognitive filtering device</u> that implicitly guides the *hierarchy of sources, the salience of concepts* and the *interpretative trajectories* considered relevant.   

::: {.callout}
<span style='font-size:1.2em'>
The result is a redefinition of the <u>criteria of epistemic relevance</u>, capable on the one hand of broadening access to knowledge and on the other of excluding minority perspectives or sources not represented in the training datasets.
</span> 
:::


## 2. Interpretability and eXplainable Artificial Intelligence (XAI)

The problem of the opacity of deep learning algorithms has given rise to a specific field of study known as **eXplainable Artificial Intelligence (XAI)**. 

The aim of this approach is to provide <u>methodological and technical tools</u> that make the *decision-making processes of models "interpretable"*, allowing researchers to understand why a particular inference or synthesis has been produced. 

[ üëâ Interpretability is not only about the readability of the model, but also has <u>epistemological significance</u>: without the ability to explain the reasons for an output, the traceability necessary to recognise a result as scientific is lost.]{.mark}

Key strategies include feature attribution techniques (such as [LIME](https://dl.acm.org/doi/pdf/10.1145/3578337.3605138) and [SHAP](https://cgarbin.github.io/machine-learning-interpretability-feature-attribution/)), which identify which variables have contributed most to a prediction, and intrinsically interpretable models, designed to prioritise readability over computational complexity. 

Although none of these approaches completely eliminates opacity, they provide sufficient <u>levels of transparency</u> to reintroduce accountability and verifiability criteria into research processes.

## 3. The limits of opaque architectures

Despite advances in XAI, the most advanced architectures remain largely ‚Äú**opaque**‚Äù.  
Deep learning models, especially those based on billions of parameters, cannot be fully interpreted by either developers or users.  

This **black box condition** introduces an epistemological divide. Science, traditionally anchored in the reconstructibility of processes, finds itself dependent on systems that produce inferences whose internal mechanisms cannot be explained.  

The problem is not only technical but also "conceptual".  
AI does not operate through *logical reasoning* but through the *reproduction of statistical patterns*.   
As a result, it does not distinguish between what is epistemically grounded and what is only probabilistically plausible.

::: {.callout-warning}
## This characteristic opens up the possibility of systematic errors, textual hallucinations and bias amplification, phenomena that undermine scientific credibility if not accompanied by rigorous control and validation practices.
:::

## 4. Transparency as an epistemic condition 

The issue of **transparency** is not limited to the technical readability of algorithms, but concerns the entire cycle of scientific production.  
It becomes necessary to <u>make explicit the criteria for data selection</u>, processing methods and circumstances of AI use, so that the results can be subjected to intersubjective control.   

Transparency assumes an eminently **epistemic function**, since only by ensuring the possibility of collective reconstruction can AI outputs be prevented from turning into opaque products, shielded from critical scrutiny.

::: {.callout-note icon="false" appearance="simple"}
<span style='font-size:1.2em'>
Alongside the *individual responsibility* of researchers, who must accurately **document the use of AI in their work**, there is also **institutional responsibility**.</span> 
:::


International organisations, funding agencies and Academic communities have begun to define <u>standards</u> and <u>guidelines</u> aimed at preserving epistemic integrity.  

[ üëâ These directives require <u>systematic disclosure of AI use</u>, together with <u>auditing</u> and <u>reporting protocols</u>.   
Publishing houses are also moving in this direction, requiring precise declarations on the use of generative systems in writing and editing processes.]{.mark}

## Further Reading

> See [How the machine ‚Äòthinks‚Äô: Understanding opacity in machine learning algorithms](https://journals.sagepub.com/doi/full/10.1177/2053951715622512)

> See [Towards A Rigorous Science of Interpretable Machine Learning](https://doi.org/10.48550/arXiv.1702.08608)

> See [Connecting ethics and epistemology of AI](https://doi.org/10.1007/s00146-022-01617-6)

> See [Digital epistemology: evaluating the credibility of knowledge generated by AI](https://ejournal.bamala.org/index.php/yudhistira/article/view/251/49)

> See [Towards a Manifesto for Cyber Humanities: Paradigms, Ethics, and Prospects](https://doi.org/10.48550/arXiv.2508.02760)

> See [The mythos of model interpretability](https://dl.acm.org/doi/pdf/10.1145/3236386.3241340)

> See [Explanation in Artificial Intelligence: Insights from the Social Sciences](https://www.sciencedirect.com/science/article/pii/S0004370218305988?via%3Dihub)
