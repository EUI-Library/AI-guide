[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AI training",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "1-20250710-GAI-evolution.html",
    "href": "1-20250710-GAI-evolution.html",
    "title": "2  The Evolution of Generative Artificial Intelligence and its Application in Academic Research",
    "section": "",
    "text": "2.1 1. Introduction to Generative Artificial Intelligence\nGenerative Artificial Intelligence (GAI) represents one of the most transformative developments in contemporary science and technology.\nIts evolution has accelerated significantly since 2018, with the birth of Large Language Models (LLMs) based on the Transformer architecture.\nTo could find more information about Transformers to these links:\n- Introduction to the Transformer\n- How Transformers work\nUnlike earlier technologies, these models can deeply understand linguistic and conceptual context, generating content that is coherent, well-structured, and often hard to tell apart from what a human might produce.\nIts evolution can be schematically articulated through 3 key phases:\n- Symbolic AI and supervised machine learning (until ~2010): systems relied on hand-coded rules and labeled datasets. They were effective for specific tasks like prediction or classification but were rigid and hard to adapt to new situations.\n- Deep learning and distributed representations (2010–2018): deep neural networks enabled computers to learn hidden patterns in data. This led to major breakthroughs in fields like image recognition, natural language processing, and bioinformatics.\n- Autoregressive and multimodal generative AI (2018 onward): Large language models (LLMs) began learning language and concepts from massive text datasets. New models emerged that can handle multiple formats—text, images, audio, and code—at the same time, opening up a wide range of applications.\nSource: Medium.\nThe image shows the relationships between AI, ML, DL, NLP, LLM, and Conversational AI.”*\nAcademic research has played a crucial role in this evolution (think of models such as BERT developed by Google Research or T5), but it is also being profoundly impacted methodologically and operationally by these tools.",
    "crumbs": [
      "Part 1: Fundamentals of Generative AI e Tools",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Evolution of Generative Artificial Intelligence and its Application in Academic Research</span>"
    ]
  },
  {
    "objectID": "1-20250710-GAI-evolution.html#current-applications-and-dissemination-of-gai-in-research",
    "href": "1-20250710-GAI-evolution.html#current-applications-and-dissemination-of-gai-in-research",
    "title": "2  The Evolution of Generative Artificial Intelligence and its Application in Academic Research",
    "section": "2.2 2. Current applications and dissemination of GAI in research",
    "text": "2.2 2. Current applications and dissemination of GAI in research\nGAI is progressively permeating academic workflows, with differentiated uptake across disciplines. Its impact is notable in the following domains:\n\n2.2.1 2.1 Literature conception and review\n\nAssisting in literature search using specialized AI agents (e.g., Elicit, Scite);\nGeneration of exploratory research questions;\nAutomatic synthesis of articulated corpora, useful for systematic and narrative reviews.\n\n\n\n2.2.2 2.2 Scientific text production and revision\n\nPreliminary drafting of text sections (e.g., abstract, methodology);\nAutomated linguistic revision for non-English speaking authors;\nControlled rewriting for stylistic adaptation and expository clarity.\n\nMethodological note: Any content generated should always be reviewed and validated, as models may introduce bias, errors or conceptual “hallucinations.”\n\n\n2.2.3 2.3 Data analysis support and code automation\n\nGeneration of scripts in R, Python, MATLAB from natural language descriptions;\nHelp in preliminary interpretation of results (e.g., graphs, output of statistical models);\nSimulation of scenarios or data transformation via NLP-to-code interfaces.\n\n\n\n2.2.4 2.4 Dissemination and Communication\n\nCreation of popular content for websites, presentations, infographics;\nLinguistic and multilingual adaptation of materials for different audiences;\nAssistance in preparing grant, pitch or open access articles.\n\n\n\n2.2.5 2.5 Diffusion and acceptance in the academic context\nThe adoption of generative AI is now heterogeneous across disciplines.\n\nWidely adopted in computational sciences, engineering, computational linguistics, biomedicine;\nIn exploratory stage in the social sciences and humanities, with increasing experimentation;\nStill being debated in high-end journals, especially in relation to authorship attribution and transparency of use.\n\nMany universities are developing internal guidelines to regulate its use, in parallel with an evolving ethical and regulatory framework.\n\n\n\n\n\n\nImportantTowards a new scientific literacy\n\n\n\n\n\n\n\n2.2.5.0.1 The introduction of generative AI into research is not neutral. It requires a rethinking of methodological skills as well as an updating of scientific ethics.Researchers are called to develop:\n\n\n2.2.5.0.2 - Prompt engineering skills, or the art of formulating effective and replicable input;*\n\n\n2.2.5.0.3 - Ability to critically evaluate generated content, to identify errors, omissions and bias;\n\n\n2.2.5.0.4 - Awareness of epistemological limitations: AI generates plausible content, but does not verify the truth of claims;\n\n\n2.2.5.0.5 - Transparent documentation of tool use, useful for reproducibility.*",
    "crumbs": [
      "Part 1: Fundamentals of Generative AI e Tools",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Evolution of Generative Artificial Intelligence and its Application in Academic Research</span>"
    ]
  },
  {
    "objectID": "1-20250710-GAI-evolution.html#adoption-of-new-research-methodologies",
    "href": "1-20250710-GAI-evolution.html#adoption-of-new-research-methodologies",
    "title": "2  The Evolution of Generative Artificial Intelligence and its Application in Academic Research",
    "section": "2.3 4. Adoption of new research methodologies",
    "text": "2.3 4. Adoption of new research methodologies\nGAI also enables novel experimental paradigms:\n\nLanguage-driven conceptual simulations;\nAutomated meta-analyses of scientific literature;\nMultimodal encoding and decoding (text, image, numerical data) to represent complex phenomena.\n\nWhen employed responsibly, generative AI does not replace scientific inquiry, but amplifies its reach and efficiency.",
    "crumbs": [
      "Part 1: Fundamentals of Generative AI e Tools",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Evolution of Generative Artificial Intelligence and its Application in Academic Research</span>"
    ]
  },
  {
    "objectID": "1-20250718-GAItools.html",
    "href": "1-20250718-GAItools.html",
    "title": "3  Updated Overview of Generative AI Tools",
    "section": "",
    "text": "3.1 1. Practical introduction to GAI Tools\nThe integration of Generative AI (GAI) tools into research activities requires an informed and conscious selection of the available resources. It is essential for a researcher to distinguish between different types of operational tools and to understand how they work in relation to specific research goals.\nWe can classify tools into three main categories:\nThe effective use of these tools requires not only technical knowledge but also the ability to assess their suitability based on:",
    "crumbs": [
      "Part 1: Fundamentals of Generative AI e Tools",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Updated Overview of Generative AI Tools</span>"
    ]
  },
  {
    "objectID": "1-20250718-GAItools.html#practical-introduction-to-gai-tools",
    "href": "1-20250718-GAItools.html#practical-introduction-to-gai-tools",
    "title": "3  Updated Overview of Generative AI Tools",
    "section": "",
    "text": "General-purpose Language Models (LLMs): mainly used for writing, editing, and language support tasks.\nSpecialized Documentary Tools: focused on the collection, synthesis, and analysis of scientific literature.\nIntegrated Automation Systems: plugins, APIs, or hybrid platforms that extend basic capabilities, adapting them to customized workflows.\n\n\n\nTransparency and traceability of results\nData source currency and validity\nLevel of customization allowed (advanced prompting, proprietary data training)\nCompliance with ethical and regulatory requirements specific to academic research\n\n\n\n\n\n\n\nImportantParticularly in SSH (Social Sciences and Humanities), these aspects are critical due to risks such as methodological biases, ambiguities in intellectual property management, and sensitive data handling issues.",
    "crumbs": [
      "Part 1: Fundamentals of Generative AI e Tools",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Updated Overview of Generative AI Tools</span>"
    ]
  },
  {
    "objectID": "1-20250718-GAItools.html#large-language-models-llms-for-research",
    "href": "1-20250718-GAItools.html#large-language-models-llms-for-research",
    "title": "3  Updated Overview of Generative AI Tools",
    "section": "3.2 2. Large Language Models (LLMs) for research",
    "text": "3.2 2. Large Language Models (LLMs) for research\nLLMs currently represent some of the most widely adopted AI tools in academic research. These models rely on deep learning architectures trained on extensive volumes of textual data and are designed to generate coherent linguistic content, supporting activities such as drafting, summarization, and data analysis.\n\n3.2.1 2.1 ChatGPT-4\nDeveloped by: OpenAI\nChatGPT-4 is a language model developed by OpenAI owned by Sam Altman, which was made available to the public in November 2022. It is based on state-of-the-art deep learning architectures and is designed for the production of textual content and human-machine interaction in a natural language.\nKey Features:\n- The ability to generate multilingual texts, with a high level of semantic and syntactic coherence even on very articulated and specialized texts;\n- The use in conversational mode optimized to be able to sustain prolonged dialogues while maintaining thematic and contextual coherence, progressively adapting the answers according to the user’s requests;\n- Integration with pre-configured APIs and platforms to allow the incorporation of the model into custom software and workflows, although deep customization of the model (fine-tuning) remains reserved for authorized parties with regulated access.\n- A wide availability of tools and ancillary resources, thanks to the wide diffusion and presence of a consolidated ecosystem of third-party applications, extensions and interfaces.\n\n\n\n\n\n\n\nStrengths\nLimitations\n\n\n\n\nHigh versatility in writing, editing, translation tasks\nNo native real-time data access except through plugins or external systems\n\n\nLarge user community\nOutputs always require researcher validation to avoid the risk of inaccuracies or hallucinations\n\n\nExtensive documentation\n\n\n\n\nRecommended uses cases:\n- Preliminary drafting of articles, abstracts, and project proposals\n- Linguistic and stylistic revision\n- Support in formulating research questions.\n\n\n3.2.2 2.2 Claude\nDeveloped by: Anthropic\nClaude is developed by Anthropic, known for its explicit orientation towards AI models called “constitutional”, i.e. designed to privilege security, transparency and respect for ethical principles defined a priori. The first model was released in March 2023.\nKey Features:\n- It prioritizes robustness and control of the content generated, reducing the risk of inaccurate or inappropriate responses, particularly on complex or ethically sensitive topics. - It is very versatile, as it can be applied in diversified activities, from the drafting of texts to the analysis of complex data, therefore suitable for academic tasks or for structured business uses. - It demonstrates a high capacity for contextual comprehension, being able to grasp nuanced contexts and nuances of human language, also being able to manage very extensive conversation contexts, revision of long documents, transcriptions, legal acts or articulated reports. In fact, it is designed to maintain consistency on higher text sequences than other LLMs, being able to contain up to 200 thousand tokens (equivalent to about 500 pages of text) in the basic version without losing the logical thread. - It allows regulated access, as it is accessible through APIs and proprietary platforms with a less widespread usage model than other LLMs and a fine-tuning policy reserved exclusively for entities with advanced security requirements, such as public bodies and universities. - It was developed with a strong focus on AI ethics and safety . During his training, careful and controlled sources were privileged, with limited exposure to open data, reducing the risk of bias and information biases. - The owner company guarantees regular updates of the model, aimed at the progressive improvement of capabilities, precision and operational safety, with the aim of keeping Claude always aligned with market and research needs.\n\n\n\n\n\n\n\nStrengths\nLimitations\n\n\n\n\nHigh safety and ethical standards, particularly suited to contexts involving sensitive data management\nNo direct access to real-time data; less widespread in Europe\n\n\n\nHigher average operating costs compared to other LLMs\n\n\n\nRecommended uses cases:\n- Drafting texts in fields governed by ethical codes\n- Critical review of content generated by other models\n- Interactions involving sensitive or highly specific data\n\n\n3.2.3 2.3 Gemini\nDeveloped by: Google DeepMind\nGemini is a multimodal language model developed by Google DeepMind, designed to integrate the processing of text, images, structured data, and other input modalities within a single architecture. Released during 2024, it represents one of the most recent evolutions in the field of Large Language Models (LLMs) with extensive applications in both academic and industrial settings.\nKey Features:\n- It has a multimodal architecture, as it has been designed to simultaneously process text, images, tabular data and other forms of input. This ability distinguishes it from models focused exclusively on natural language, expanding the application possibilities in interdisciplinary contexts, such as Digital Humanities or the analysis of complex datasets. - It has an advanced multilingual language capacity, comparable to that of ChatGPT-4, with the possibility of generating articulated content, summaries, translations and analysis of specialized texts. - It has native integration with the Google ecosystem: it has been designed to work in synergy with Google Workspace and other services of the Google Cloud platform, facilitating integration into the research and document management workflows already in use at many academic institutions. - Allows access via APIs and cloud tools. The model is available to users through Google Cloud Platform, with access and customization policies that provide specific configurations for universities, public bodies and companies. Fine-tuning personalization is only allowed in authorized environments, in line with Google’s security and compliance policies. Gemini benefits from constant updates, both at the data and architecture level, keeping the model aligned with evolving language, knowledge sources, and regulatory needs.\n\n\n\n\n\n\n\nStrengths\nLimitations\n\n\n\n\nAdvanced multimodal data analysis\nNot a real-time information retrieval system\n\n\nlinguistic capabilities suited to specialized text analysis\nRequires substantial infrastructure\n\n\n\nLess documentation available within SSH academic contexts\n\n\n\nRecommended uses cases:\n- Projects involving multimodal source analysis\n- Development of integrated educational and communication materials\n- Support in building complex datasets\n\n\n3.2.4 2.4 Perplexity AI\nDeveloped by: Perplexity AI\nPerplexity AI is a language model-based query system designed to complement the capabilities of LLMs with document search and source verification capabilities. Launched in 2022 and further developed until 2025, it differs from traditional LLMs for the combination of text generation and retrieval of updated information, with transparent indication of the sources consulted.\nKey Features:\n- Avere una architettura ibrida LLM + motore di ricerca. Combina l’elaborazione linguistica tipica dei modelli generativi con un sistema di interrogazione su database e fonti online, restituendo risposte accompagnate da riferimenti bibliografici o collegamenti diretti alle fonti originali. - E’ stato concepito specificamente per supportare attività di revisione della letteratura, fact-checking e interrogazioni esplorative, risultando quindi particolarmente utile in ambito giornalistico e accademico, essendo anche ottimizzazione per la ricerca bibliografica. - Possiede una capacità linguistica focalizzata perché pur basandosi su modelli linguistici comparabili a quelli di altri LLM, Perplexity AI privilegia risposte concise e orientate all’informazione piuttosto che alla generazione di testi lunghi o articolati. - E’ disponibile sia come applicazione web sia integrabile in sistemi di gestione documentale tramite API, con configurazioni specifiche per istituzioni di ricerca e aziende. La possibilità di personalizzazione mediante fine-tuning è limitata e subordinata ad accordi specifici. - A differenza di modelli addestrati su dati statici, Perplexity AI si aggiorna automaticamente grazie alla componente search integrata, garantendo risposte basate su fonti recenti. - Possiede una elevata trasparenza nel processo di generazione delle risposte.\n\n\n\n\n\n\n\nStrengths\nLimitations\n\n\n\n\nAutomatic updating via integrated search components\nLess suitable than other LLMs for drafting extended content or managing complex documents\n\n\nHigh transparency in the response generation process\nThe quality and relevance of answers depend on the availability and reliability of the queried sources.\n\n\n\nRecommended uses cases:\n- Preliminary research and verification\n- Exploratory bibliographic research\n- Fact-checking activities\n\n\n3.2.5 2.5 DeepSeek\nDeveloped by: DeepSeek AI\nKey Features:\n- Specifically designed with an emphasis on long-context understanding and multilingual processing, offering optimized performance for both general-purpose and domain-specific academic tasks.\n- Built on transformer architectures and trained on an extensive multilingual corpus (including scientific and technical texts), enhancing its applicability in research settings that require high linguistic precision.\n- Capable of handling extended input sequences beyond the standard limits of many mainstream LLMs, allowing for the processing of large documents, legal texts, or comprehensive datasets without fragmentation.\n- Provides configurable access through both web interfaces and APIs, with customization options such as proprietary dataset integration and specialized vocabulary training, particularly relevant for academic and institutional use.\n\n\n\n\n\n\n\nStrengths\nLimitations\n\n\n\n\nHigh reliability in multilingual academic writing and translation tasks, with particular attention to terminological consistency.\nAs of now, DeepSeek is less widespread in Western academic institutions compared to models like ChatGPT‑4 or Claude, partly due to access policies and licensing restrictions.\n\n\nEnhanced performance in long-document summarization, literature review synthesis, and complex query handling within structured academic frameworks.\nReal-time data retrieval capabilities are not natively embedded; reliance on external plugins or customized configurations is necessary for up-to-date information access.\n\n\nIntegration with knowledge base systems and research management platforms, supporting structured data extraction and semantic analysis workflows.\nRequires higher computational resources than standard LLMs, especially when operating at full capacity for long-context processing.\n\n\n\nRecommended uses cases:\n- Drafting and reviewing multilingual academic papers, particularly in contexts involving technical or regulatory terminology.\n- Summarization and synthesis of large volumes of research literature, including legal or policy documents.\n- Integration into academic platforms for semantic search, knowledge extraction, and multilingual document management.",
    "crumbs": [
      "Part 1: Fundamentals of Generative AI e Tools",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Updated Overview of Generative AI Tools</span>"
    ]
  },
  {
    "objectID": "1-20250718-GAItools.html#specialized-tools-for-bibliographic-and-document-research",
    "href": "1-20250718-GAItools.html#specialized-tools-for-bibliographic-and-document-research",
    "title": "3  Updated Overview of Generative AI Tools",
    "section": "3.3 3. Specialized Tools for bibliographic and document research",
    "text": "3.3 3. Specialized Tools for bibliographic and document research\nBeyond general-purpose LLMs, there are AI tools designed specifically to support bibliographic and document research. They integrate querying, classification, and synthesis functions aimed at optimizing literature review phases.\n\n3.3.0.1 \nElicit is a tool specifically developed to support bibliographic and document research activities by leveraging LLMs applied to structured academic corpora.\nUnlike traditional search engines, Elicit is capable of automatically extracting structured information from scientific articles, such as research questions, methods, key results, and citations.\nIts core functions include:\n- Automatic identification and extraction of research-relevant elements from articles (e.g., title, authors, research questions, methods, results, citations).\n- Synthesis of responses through thematic clustering and semantic categorization, facilitating the formulation of research hypotheses already informed by the state of the art.\n- Export of structured data in CSV or JSON formats, allowing integration with qualitative analysis software or reference management systems.\nFor example, using Elicit, researchers can generate summary tables containing:\n- Main articles identified based on a query\n- Automatically synthesized abstracts\n- Extracted research questions and primary findings\n- Structured citation metadata.\nThese features make Elicit particularly suitable for preparing systematic reviews and meta-analyses, offering researchers not only a list of sources but also structured overviews that can directly inform project design, grant proposal drafting, and academic writing processes.\nCompared to generic LLMs, Elicit provides:\n- Focused output: responses are oriented towards factual and structured information rather than general linguistic generation.\n- Source verification: results are derived from curated academic databases, enhancing the reliability and validity of the information obtained.\n- Workflow integration: data produced by Elicit can be incorporated into existing academic workflows, including qualitative analysis platforms and reference management systems.\nHowever, it is important to note that Elicit does not replace the critical evaluation of sources by the researcher. While the tool offers substantial support in reducing the time needed for literature search and data extraction, each piece of information should be validated according to the standards of academic rigor and methodology.\n\n\n3.3.0.2 \nScite is a specialized platform designed to support bibliometric analysis and literature evaluation by focusing on the citation context of academic publications.\nUnlike traditional bibliographic databases, which report citation counts without specifying their nature, Scite enables researchers to assess not only the number but also the type and content of citations received by a given article.\nIts principal functionalities include:\n- Analysis of citation context, distinguishing between supportive citations, contrasting citations, and neutral mentions.\n- Integration with academic databases to provide comprehensive and structured citation data across multiple disciplines.\n- Visualization of citation patterns through interactive dashboards, facilitating both individual article assessment and broader literature mapping.\nFor each publication, Scite provides:\n- The total number of citations, classified into supportive, contrasting, and mentioning categories.\n- Detailed reports including the text surrounding each citation, allowing for qualitative evaluation of the citing sources.\n- Metrics that can complement traditional bibliometric indicators, offering a more nuanced understanding of a publication’s impact.\nThese features make Scite particularly valuable in phases such as:\n- Literature review, where researchers need to identify not only frequently cited works but also those that are positively or negatively discussed within the academic community.\n- Research assessment and evaluation processes, where citation context can inform more sophisticated performance metrics.\n- Grant proposal preparation and institutional reporting, where evidence of scholarly impact requires qualitative as well as quantitative support.\nWhile Scite enhances citation analysis capabilities, it does not substitute for in-depth content analysis by domain experts. The interpretation of citation types and their implications remains the responsibility of the researcher. Additionally, coverage may vary depending on discipline and database integration, making it advisable to verify source completeness and relevance in each specific research context.",
    "crumbs": [
      "Part 1: Fundamentals of Generative AI e Tools",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Updated Overview of Generative AI Tools</span>"
    ]
  },
  {
    "objectID": "1-20250718-GAItools.html#plugins-and-apis-for-ssh-and-stem-when-and-why-to-use-them",
    "href": "1-20250718-GAItools.html#plugins-and-apis-for-ssh-and-stem-when-and-why-to-use-them",
    "title": "3  Updated Overview of Generative AI Tools",
    "section": "3.4 4. Plugins and APIs for SSH and STEM: when and why to use them",
    "text": "3.4 4. Plugins and APIs for SSH and STEM: when and why to use them\nIn Academic research contexts — whether in Social Sciences and Humanities (SSH) or STEM discipline — the use of plugins and APIs associated with GAI models plays a crucial role whenever functional integration with existing information systems or automation of recurring tasks is required.\nUnlike general-purpose tools, APIs (Application Programming Interface) allow direct incorporation of language model capabilities or specialized platforms into workflows already established within institutional or corporate environments.\nTypical applications include:\n- Automated classification and organization of textual data, achieved by integrating language models within document management systems or digital archives, allowing for more efficient handling and retrieval of large volumes of unstructured information.\n- Automated generation of metadata and descriptors for datasets, which facilitates the management and curation of research archives, bibliographic repositories, or complex digital collections by systematically assigning structured information to each resource.\n- Controlled multilingual content translation, through the use of AI integrated into editorial platforms or academic repositories. This approach enables not only the automatic translation of content but also the application of customized dictionaries or specialized glossaries to ensure terminological consistency and accuracy across different languages.\n- Semantic querying of complex databases, allowing researchers to submit natural language queries that surpass the limitations of traditional keyword-based search systems, thereby improving the precision and relevance of retrieved results.\nIn STEM fields, plugins are commonly adopted to extend functionalities such as:\n- Statistical analysis\n- Symbolic computation\n- Code generation\n- Data visualization.\nIn SSH contexts, plugins are particularly useful for:\n- Extending bibliographic database and historical archive querying capabilities\n- Integrating text mining and sentiment analysis tools\n- Automating the production of reports or summaries from complex linguistic corpora.\n\n\n\n\n\n\nTipImportant Considerations\n\n\n\nThe adoption of plugins and APIs involves specific evaluations concerning security, sensitive data management, and regulatory compliance, especially within the European academic context where regulations such as GDPR apply.\nYou can choose between plugins and APIs depending on your project’s nature.\nPlugins are easier to use because they come ready-made and already integrated. APIs, on the other hand, let you customize things more, but they require more technical skills to set up and manage.",
    "crumbs": [
      "Part 1: Fundamentals of Generative AI e Tools",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Updated Overview of Generative AI Tools</span>"
    ]
  },
  {
    "objectID": "1-20250718-GAItools.html#operational-recommendations-for-researchers-in-social-sciences",
    "href": "1-20250718-GAItools.html#operational-recommendations-for-researchers-in-social-sciences",
    "title": "3  Updated Overview of Generative AI Tools",
    "section": "3.5 5. Operational recommendations for researchers in Social Sciences",
    "text": "3.5 5. Operational recommendations for researchers in Social Sciences\nIntegrating GAI tools into social research processes requires a rigorous methodological approach and careful assessment of operational and ethical implications.\nBelow are some key recommendations to guide their effective and ethical integration:\n\n\n\n\n\n\nRecommendations\n\n\n\n\nClearly define the scope of LLM and AI tool usage\n\n\nSystematically verify content reliability—AI outputs always require critical review.\n\n\nPrefer tools with transparent source referencing (e.g., Perplexity AI, Scite).\n\n\nEnsure data adequacy, favoring SSH-specific corpora.\n\n\nDocument all AI tool usage explicitly in reports and publications.\n\n\nAddress ethical and legal profiles, ensuring compliance with GDPR, AI Act, and institutional policies\n\n\n\nIt is advisable for Research Institutions and Groups to establish clear internal AI use policies, updated regularly according to technological and regulatory evolution, ensuring adequate training for team members.",
    "crumbs": [
      "Part 1: Fundamentals of Generative AI e Tools",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Updated Overview of Generative AI Tools</span>"
    ]
  },
  {
    "objectID": "1-20250719-promptengineering.html",
    "href": "1-20250719-promptengineering.html",
    "title": "4  Prompt Engineering for Research: Techniques, Workflows, and Evaluation",
    "section": "",
    "text": "4.1 1 Introduction to Prompt Engineering",
    "crumbs": [
      "Part 1: Fundamentals of Generative AI e Tools",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Prompt Engineering for Research: Techniques, Workflows, and Evaluation</span>"
    ]
  },
  {
    "objectID": "1-20250719-promptengineering.html#introduction-to-prompt-engineering-1",
    "href": "1-20250719-promptengineering.html#introduction-to-prompt-engineering-1",
    "title": "4  Prompt Engineering for Research: Techniques, Workflows, and Evaluation",
    "section": "",
    "text": "4.1.1 1.1 What is a prompt?\nIn the context of Generative Artificial Intelligence (GAI), a prompt is a textual instruction given to the model to generate an output. It is not simply a question, but a structured linguistic command that defines objectives, constraints, register, and output format.\nIn academic research, prompt design is a crucial methodological moment, as the quality of the generated content largely depends on it.\n\n\n\n\n\n\nImportant\n\n\n\nIn SSH research (Social Sciences and Humanities), a well-crafted prompt typically:\n- Clearly defines the task objective (e.g.”Analyze the sentiment of…“)\n- Specifies the desired output format (e.g.”Return a bullet list of…“)\n- Provides examples or constraints, when needed (e.g.”Use formal Italian register”)\n\n\n\n\n4.1.2 1.2 The Prompt Engineering\nLarge Language Models (LLMs) respond to textual input with variable linguistic articulation depending on training, context, and prompt structure. The way the request is formulated directly influences the type of output. The same question can produce very different results depending on form, specificity, or presence of examples.\nPrompt Engineering is the discipline of designing and optimizing prompts to guide AI models—particularly LLMs—toward the desired output. It involves methodological principles such as clarity, relevance, structure, logical progression and iteration.\n\n\n4.1.3 1.3 Hallucinations and how to prevent them\n“Hallucinations” occur when the model generates unverified or entirely fabricated statements, often attributing fictitious details to nonexistent sources.\nThese errors stem from the statistical nature of Machine Learning and require strategies of cross-validation and guided reinforcement.\n\n\n\n\n\n\nTipAnti-hallucination strategies\n\n\n\n- Cross-source validation: ask the model for references, citations, or URLs and manually verify them.\n- Confirmation queries: include a verification clause such as: *“Are you sure?* Please briefly validate this data.”\n- Few-shot learning: provide 2–3 examples distinguishing between accurate and incorrect responses in order to guide its behavior and improve its accuracy.\n- Iterative refinement: generate a first draft and request successive revisions to address inaccuracies, documenting each iteration.\n\n\n\n\n4.1.4 1.4. Foundational principles for effective and reliable prompts\n\n4.1.4.1 Clarity and Specificity\n\nFrame the task unambiguously. Avoid vague requests like “write something about…”. Always indicate the desired result (e.g.,“return a table with columns X, Y, and Z”).\nExplicitly define the output format (e.g., bullet list, 100-word paragraph, numbered list).\n\n\n\n4.1.4.2 Full Context\n\nProvide all relevant information, such as text excerpts, definitions of technical terms, methodological constraints, or the target audience (e.g., “text aimed at SSH researchers”).\nSpecify tone and register (formal, didactic, expository) to ensure stylistic consistency.\n\n\n\n4.1.4.3 Guided Iteration\n\nDo not settle for the first draft. Test small variations to observe how each affects the output.\nTest A/B versions, saving each version for comparative analysis and preserve the various iterations for later benchmarking.",
    "crumbs": [
      "Part 1: Fundamentals of Generative AI e Tools",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Prompt Engineering for Research: Techniques, Workflows, and Evaluation</span>"
    ]
  },
  {
    "objectID": "1-20250719-promptengineering.html#few-shot-and-zero-shot-prompting-inline-examples-and-template-selection-1",
    "href": "1-20250719-promptengineering.html#few-shot-and-zero-shot-prompting-inline-examples-and-template-selection-1",
    "title": "4  Prompt Engineering for Research: Techniques, Workflows, and Evaluation",
    "section": "4.2 2. “Few-shot” and “Zero-shot” prompting: inline examples and template selection",
    "text": "4.2 2. “Few-shot” and “Zero-shot” prompting: inline examples and template selection\nPrompt design plays a crucial role in output quality when using LLMs for academic purposes.\nIn particular, zero-shot and few-shot prompting strategies represent two distinct but complementary approaches, aimed at orienting the behavior of the model in the absence or presence of explicit examples.\nThe choice between zero-shot and few-shot prompting must be guided by the nature of the task, the degree of formalization expected and the need to control the variability of the output.\nBoth modes can be adopted within more complex pipelines, integrated with validation, review and iterative refinement tools.\n\n4.2.0.1 Zero-shot Prompting\nZero-shot prompting uses explicit instructions without concrete examples. It assumes the model can infer the task solely from a clear command. This is useful for standard or generic tasks but more prone to ambiguity or result variability.\n\n\n4.2.0.2 Few-shot Prompting\nFew-shot prompting embeds one or more examples within the prompt, guiding the model to replicate a demonstrated pattern. In academic contexts, it is especially effective for standardizing formats (e.g., abstracts, bibliographic entries, methodological summaries) and maintaining stylistic coherence.\n\n\n4.2.0.3 Template Use\nTemplate selection and adaptation are central to this strategy. Recurring structures — e.g., “Question → Extract → Summary” or “Title → Objective → Methodology → Results” —facilitate comparable outputs and enhance compatibility with archival and analytical systems.",
    "crumbs": [
      "Part 1: Fundamentals of Generative AI e Tools",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Prompt Engineering for Research: Techniques, Workflows, and Evaluation</span>"
    ]
  },
  {
    "objectID": "1-20250719-promptengineering.html#prompt-chaining-and-complex-pipelines-a-modular-approach-to-output-construction",
    "href": "1-20250719-promptengineering.html#prompt-chaining-and-complex-pipelines-a-modular-approach-to-output-construction",
    "title": "4  Prompt Engineering for Research: Techniques, Workflows, and Evaluation",
    "section": "4.3 3. Prompt chaining and complex pipelines: a modular approach to output construction",
    "text": "4.3 3. Prompt chaining and complex pipelines: a modular approach to output construction\nAcademic tasks involving progressive information processing — such as literature reviews, thematic analysis or argument construction — benefit significantly from complex pipelines based on prompt chaining.\nThis approach entails sequential execution of multiple prompts, each serving a specific function within a structured workflow.\nThe output from each stage becomes the input for the next, following a modular, cumulative logic.\n\n\n\n\n\n\nNoteComplex pipelines\n\n\n\nComplex pipelines differ markedly from the isolated use of LLMs, as they aim to structure a distributed cognitive process, in which each step contributes to the construction of a coherent, documentable and verifiable final result.\nTheir adoption makes it possible not only to divide cognitively dense tasks into more manageable units, but also to improve methodological control and transparency of automatic processing processes.\n\n\n\n\n\n\n\n\n\nApplication example\n“Systematic review of the literature”\n\n\n\n\nRetrieving relevant sources\nUsing a prompt to query databases or tools such as Elicit or Perplexity AI, in order to identify relevant articles on a given topic.\n\n\nStructured metadata extraction\nPrompts aimed at extracting and organizing information such as title, authors, date, methodology, type of study, subject area.\n\n\nTheme recognition and clustering\nApplication of prompts aimed at identifying recurring concepts, semantic classification and building thematic maps.\n\n\nComparative synthesis of results\nSynthesis command to produce an integrated view of the evidence, with comparison between approaches, results and theoretical positions.\n\n\nGeneration of the final output\nLast prompt to transform the collected material into a finished product, such as a thematic overview, a bibliographic annotation or an articulated abstract.\n\n\n\n\n\n\n\n\n\nWarningComplex pipelines differ from isolated prompt use: they aim to construct a distributed cognitive process.\n\n\n\nBenefits include: Modularity - Reproducibility - Methodological transparency - Output traceability.",
    "crumbs": [
      "Part 1: Fundamentals of Generative AI e Tools",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Prompt Engineering for Research: Techniques, Workflows, and Evaluation</span>"
    ]
  },
  {
    "objectID": "1-20250719-promptengineering.html#advanced-prompts-metadata-extraction-outline-generation-stylistic-paraphrasing-1",
    "href": "1-20250719-promptengineering.html#advanced-prompts-metadata-extraction-outline-generation-stylistic-paraphrasing-1",
    "title": "4  Prompt Engineering for Research: Techniques, Workflows, and Evaluation",
    "section": "4.4 4. Advanced prompts: metadata extraction, outline generation, stylistic paraphrasing",
    "text": "4.4 4. Advanced prompts: metadata extraction, outline generation, stylistic paraphrasing\nThe use of LLMs in Academia is not limited to the simple production of texts, but can be extended to more sophisticated functions through the use of advanced prompts.\nThese allow you to guide the model in carrying out structured, analytical or transformative tasks, which require a more precise configuration of the prompt and a greater awareness of the semantic capabilities of the model.\nAmong the most relevant applications are:\n- The automatic extraction of metadata from scientific articles or other structured documents.\n\n\nThrough targeted prompts, it is possible to isolate information such as author, year of publication, methodological context, reference discipline or type of study. This type of operation is particularly useful for the construction of bibliographic datasets, the compilation of structured repertoires and the automated analysis of literature.\n- The generation of outlines.\nDuring the planning or drafting of scientific contributions, it is possible to ask the model to build logical schemes, argumentative structures or section plans consistent with disciplinary standards. These outlines can then be integrated, modified or expanded by the researcher, acting as a support for the design of articles, reports, project proposals or theses.\n- Targeted stylistic paraphrases.\nThese are reformulations of existing content according to a specific style: formal, technical, popular, or compliant with certain disciplinary registers. This functionality is used in the revision of texts, in the linguistic adaptation for international publications or in the production of multiple versions of the same content for teaching, editorial or communication purposes.\n\n\n\n\n\n\n\nWarningApplications include:\n\n\n\n\nStructured metadata from PDFs (author, date, methods).\n\nAcademic outlines following disciplinary logic.\n\nStylistic rewriting (formal → plain, Italian → English, etc.).\n\n\n\n\n\n\n\n\n\n\n\n\nObiettivo:\nScrivere una sintesi tematica a partire da un corpus bibliografico\n\n\n\n\n\n\nFunzione\nTecnica\n\n\n1\nInserimento articoli\nprompt + upload PDF / DOI\n\n\n2\nEstrazione metadati (autore, anno, metodo)\nprompt strutturato\n\n\n3\nRiconoscimento dei temi ricorrenti\nprompt di classificazione\n\n\n4\nSintesi comparativa dei risultati\nprompt di sintesi con vincolo stilistico\n\n\n5\nOutput finale in formato accademico\ntemplate APA o report Markdown\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nEffective use of advanced prompts requires a fine understanding of the model’s capabilities and limitations, as well as a design ability geared toward controlling the output.\nThis is a particularly promising area of experimentation for the world of research, in which AI is used not to replace writing, but to enhance its preparatory, analytical and stylistic phases.\n\n\n\nSee: Giray,L.”Prompt Engineering with ChatGPT: A Guide for Academic Writers”..\nSee: Generative Artificial Intelligence Prompt Engineering Overview.",
    "crumbs": [
      "Part 1: Fundamentals of Generative AI e Tools",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Prompt Engineering for Research: Techniques, Workflows, and Evaluation</span>"
    ]
  },
  {
    "objectID": "1-20250719-promptengineering.html#iteration-and-evaluation-of-output-quality",
    "href": "1-20250719-promptengineering.html#iteration-and-evaluation-of-output-quality",
    "title": "4  Prompt Engineering for Research: Techniques, Workflows, and Evaluation",
    "section": "4.5 5. Iteration and Evaluation of output quality",
    "text": "4.5 5. Iteration and Evaluation of output quality\nThe effectiveness of a prompt cannot be considered a static datum, but the result of an iterative optimization process.\nThe interaction with the model requires an experimental and progressive logic, in which the answers obtained must be constantly subjected to verification, reformulation and comparison.\nIteration consists of the targeted repetition of the prompting with incremental changes as variations in the vocabulary, in the instructions order, in the level of specificity or in the structure of the expected format.\nThis process refines the quality of the output, reducing the interpretative ambiguities of the model and improving consistency with the researcher’s objectives.\nMore than a simple linguistic refinement, it is a methodological mechanism that allows you to explore the sensitivity of the model to the different input parameters.\nThe evaluation of the quality of the answers requires clear and shared criteria.\nIn the academic field, the analysis includes not only formal and grammatical correctness, but also other aspects such as:\n- conceptual accuracy (absence of factual errors or unjustified inferences)\n- relevance to the demand\n- argumentative cohesion\n- adherence to stylistic or disciplinary standards\n- transparency of sources and implicit assumptions (where relevant).\nEvaluation cannot be entrusted to generic indicators or intuitive judgments, but must be based on grids or reference models compatible with scientific research and communication practices.\nIn particular, when the results are used as preliminary materials for publications, systematic reviews or teaching support, it is advisable to document the choices made, justify any reformulations and point out the limits of the output generated.\n\n\n\n\n\n\nWarningIn summary, employing LLMs in the academic field means applying reflective and responsible logic to ensure the reliability and relevance of the content produced.Prompt Engineering in research must be treated as a methodological discipline, not a casual interaction. It supports transparency, reproducibility, and epistemic control in AI-assisted scholarship.",
    "crumbs": [
      "Part 1: Fundamentals of Generative AI e Tools",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Prompt Engineering for Research: Techniques, Workflows, and Evaluation</span>"
    ]
  },
  {
    "objectID": "1-20250719-promptengineering.html#guidelines-for-creating-academic-prompts-1",
    "href": "1-20250719-promptengineering.html#guidelines-for-creating-academic-prompts-1",
    "title": "4  Prompt Engineering for Research: Techniques, Workflows, and Evaluation",
    "section": "4.6 6. Guidelines for creating academic prompts",
    "text": "4.6 6. Guidelines for creating academic prompts\nBelow a structured in 6 steps useful for obtaining maximum results in academic research.\n\n\n\n\n\n\nNote1 - Precise definition of the purpose of the research\n\n\n\nYou need to be transparent about what you want to achieve. A vague prompt leads to vague answers, and you have to turn the search questions into direct instructions for the AI model.\nThe goal must be clear:\n(incorrect) “Give me an overview of X.”\n(corrected) “Identify the most recent peer-reviewed articles on topic X.”\nThe question must be transformed into an instruction.\nTo the question “What is the role of AI in qualitative research?”, the prompt should be:\n“List the 5 peer-reviewed articles published from 2022 to 2024 on the role of AI in qualitative research, indicating title, authors, journal, and DOI.”\n\n\n\n\n\n\n\n\nNote2 - Provide context and selection criteria\n\n\n\nArtificial intelligence needs the necessary context to focus the search and provide relevant results.\nIt is important to include the following parameters:\n- Disciplinary scope: e.g., “in the context of social sciences” or “with a focus on bioinformatics”.\n- Time range: e.g., “from the last three years” or “before 2020”.\n- Type of output desired: “Return a CSV table with columns: Title, Authors, Year, DOI.”\n- “Generate a bulleted list.”\n- “Provide a detailed explanation.”\n\n\n\n\n\n\n\n\nNote3 - Provide examples (Few-Shot prompting)\n\n\n\nShowing the AI one or two examples of the desired output, as if it were a model to follow, allows for a more accurate and coherent response.\nExamples must be clear, promptly including a few pre-filled rows in the desired format: Title, Authors, Year, DOI\n“AI in Social Sciences: A Review”, Rossi et al., 2023, 10.1234/ai.socsci.2023.01\n“Qualitative Artificial Intelligence Methods”, Smith & Lee, 2022, 10.5678/qual.ai.2022.02\nThen ask:\n“Now continue with three more articles following the same format.”\n\n\n\n\n\n\n\n\nNote4 - Break the task into multiple steps (Chain of Thought)\n\n\n\nFor complex or multidisciplinary tasks, break the workflow into sequential steps to help the model reason and reduce hallucinations.\nHow to guide AI reasoning:\nstep 1) “Search for articles using keywords X, Y, Z.”\nstep 2) “Filter for high-impact journals (Q1/Q2).”\nstep 3) “Sort results in reverse chronological order.”\nstep 4) “Summarize the main findings of each article in 50 words.”\n\n\n\n\n\n\n\n\nNote5 - Implement subsequent consistency checks\n\n\n\nIt is advised not to blindly trust the outputs received from AI. You can ask the model to verify its own results or justify its choices.\nYou may proceed as follows:\n- Request for justification: “Briefly explain why each article is relevant to my research on X.”\n- Fact-Check: “For each DOI, confirm its existence and validity on CrossRef.”\n- Cross-Review: “Review the table and flag any inconsistencies or potential hallucinations.”\n\n\n\n\n\n\n\n\nNote6 - Iterate and document\n\n\n\nBuilding effective prompts can be considered an iterative process, as it is unlikely to achieve the desired result on the first try.\n- Track versions: record each prompt variant with date and changes to identify best practices.\n- Compare results: keep only the prompts that produce the most accurate and complete outputs.\n- Analyze errors: when hallucinations occur (e.g., non-existent articles, incorrect DOIs), refine your prompt by adding specific constraints (e.g., “use only Scopus or PubMed databases” or “exclude predatory journals”).",
    "crumbs": [
      "Part 1: Fundamentals of Generative AI e Tools",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Prompt Engineering for Research: Techniques, Workflows, and Evaluation</span>"
    ]
  },
  {
    "objectID": "1-20250726-ricercaSSH.html",
    "href": "1-20250726-ricercaSSH.html",
    "title": "5  Use Cases in Social Sciences and Humanities (SSH) Research",
    "section": "",
    "text": "5.1 1. Text Mining and Topic Modeling applied to textual corpora in SSH\nThe application of AI-based tools — particularly language models and automated language processing techniques — has opened new perspectives for textual analysis within the social sciences and humanities.\nUnlike STEM disciplines, where computational tools are well established, integrating AI into SSH research processes requires careful and contextualized methodological reflection.\nA primary area of application concerns text mining and topic modeling on textual corpora.\nThese techniques enable the automatic identification of latent thematic structures within large volumes of text, allowing researchers to explore discursive patterns, lexical frequencies, and semantic co-occurrences.\nAutomatic text analysis through computational methods is increasingly relevant in SSH research, especially when dealing with large, unstructured, and heterogeneous textual sources.\nIn this context, text mining and topic modeling emerge as two distinct but complementary tools for the systematic exploration of linguistic corpora.\nThe analysis can be conducted on different sources (interviews, newspaper articles, administrative documents, parliamentary proceedings) and yields interpretable visual outputs (thematic maps, dendrograms, conceptual networks).\nHowever, it is essential that these tools be employed not as substitutes but as complements to theoretically grounded analysis.",
    "crumbs": [
      "Part 1: Fundamentals of Generative AI e Tools",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Use Cases in Social Sciences and Humanities (SSH) Research</span>"
    ]
  },
  {
    "objectID": "1-20250726-ricercaSSH.html#text-mining-and-topic-modeling-applied-to-textual-corpora-in-ssh-1",
    "href": "1-20250726-ricercaSSH.html#text-mining-and-topic-modeling-applied-to-textual-corpora-in-ssh-1",
    "title": "5  Use Cases in Social Sciences and Humanities (SSH) Research",
    "section": "",
    "text": "5.1.1 1.1 Text Mining: structured extraction from textual data\nText mining refers to the set of computational techniques aimed at the automatic extraction of structured information from natural language text.\nIn SSH contexts, the goal of text mining is to transform textual documents into quantitative, interpretable representations that can feed exploratory, descriptive, or inferential analyses.\nTypical applications include:\n- Analysis of political language in parliamentary speeches.\n- Study of rhetoric in media or institutional communication.\n- Exploration of collective narratives in historical, memorial, or literary sources.\n- Thematic mapping of open-ended interview datasets.\nThis approach relies on Natural Language Processing (NLP) tools such as:\n- Tokenization\n- Lemmatization\n- Part-of-speech (POS) tagging\n- Named Entity Recognition (NER)\n- Lexical co-occurrence analysis\n- Construction of Document-Term Matrices (DTM)\n\nSee: “Part-of-speech (POS) tagging” - Stanford University.\nSee: “Named Entity Recognition (NER)” - IBM.\n\n\n\n5.1.2 1.2 Topic Modeling: identifying latent structures\nTopic modeling encompasses statistical methods that automatically uncover the principal themes within a large collection of documents without prior specification.\nIt operates by identifying groups of words that frequently co-occur, presumed to represent a common topic. Each document is viewed as a mixture of topics, and each topic is represented by a distribution of semantically related terms. In practice, the model does not “understand” meaning as a human would but computes word co-occurrence patterns and infers thematic presence accordingly. The researcher then interprets each word cluster and assigns a label or meaning.\nThe most widely adopted model is “Latent Dirichlet Allocation (LDA), which assumes each document to be a mixture of topics and each topic a distribution over terms.\n\n\n\n\n\n\nNoteIn SSH topic modeling allows to:\n\n\n\n\nHighlight primary themes in a corpus without a priori coding.\n\nTrack thematic evolution over time (e.g., temporal topic modeling).\n\nCompare thematic emphasis across document groups (e.g., governmental vs. civic sources).\n\nSupport category identification in qualitative studies with large datasets.\n\n\n\n\nInterpretation of topics remains a fundamentally qualitative operation, requiring theoretical expertise and domain knowledge.\nTopic modeling does not replace interpretive analysis but extends its scope and reproducibility.\n\n5.1.2.1 Example Schema: Topic Modeling on a News Article Corpus\n\n\n\n\n\n\n\nContext\nCorpus of 1,000 articles from national newspapers, published between 2020 and 2024, related to the topic of environment and climate policies.\n\n\n\n\nObjective\nTo identify the main themes present in the articles,/ without defining them from the start.\n\n\n(1) Text pre-processing\nText cleanup (removal of punctuation, stopwords, etc.) / Reduction of words to the basic form (lemmatization) / Construction of the document-term matrix (which words appear in which articles, and how often)\n\n\n(2) Application of the model (e.g. LDA – Latent Dirichlet Allocation)\nThe model statistically processes co-occurrences and returns a series of “topics”, each consisting of a list of frequent words.\n\n\n\n\n\n5.1.2.2 Example of synthetic output:\n\n\n\n\n\n\n\n\nTopic\nMost Representative Words\nInterpretation (by the Researcher)\n\n\n\n\n1\nemissions, CO₂, fuels, industry, coal\nEnergy transition and decarbonization\n\n\n2\ndrought, extreme events, flooding, heatwave, damage\nImpacts of climate change\n\n\n3\nCOP26, agreements, UN, targets, Paris\nInternational climate policy and conferences\n\n\n4\nmobility, transportation, electric vehicles, incentives, public transit\nUrban sustainability policies\n\n\n\n\n\n\n\n\n\n\n(3) Analysis and visualization\nDistribution of topics by year → which themes increase or decrease over time / Comparison by newspaper → which newspapers emphasize certain themes / Mapping of topics within political discourse → e.g. overlapping with parliamentary discourses\n\n\n(4) Limitations and Critical Use\nThe model suggests groups of words, not “meanings”: the interpretation of the themes is the responsibility of the researcher. The results depend a lot on the quality of the corpus and the technical choices (number of topics, filter parameters, etc.)\n\n\n\n\n\n\n5.1.3 1.3 Methodological Considerations and Limitations\nBoth techniques present critical constraints. We have to pay attention to:\n- Preprocessing quality (cleaning, normalization) significantly affects results\n- Topic interpretation demands careful, theory-informed human mediation\n- Parameter choices (number of topics, filtering thresholds) are inherently arbitrary and must be transparently justified\n\n\n\n\n\n\nImportantMethodological Tip: Document all preprocessing decisions, triangulate with other data sources, and reflect epistemologically on the limits of automated social inference.\n\n\n\n\n\n\n\n\n5.1.4 1.4 Practical Applications in SSH Research\nTo better understand its use, some concrete areas of application are suggested, by way of example:\n- Cultural History: applying topic modeling to 19th-century correspondence to identify recurring themes among European intellectuals.\n- Sociology of Communication: analyzing digital platform comments to detect discursive patterns and thematic polarization.\n- Political Science: studying parliamentary speeches to trace party lexicon evolution and contrast with media framing.\n- Digital Anthropology: text mining on thematic forums to extract native categories and semantic maps related to care, consumption, and identity.\n\n5.1.4.1 Recommended Open Source Tools for SSH\n\ntmtoolkit (Python & R): The most comprehensive for SSH researchers, with direct support for text mining and topic modeling workflows, R/Python interoperability, consistency measures, and visualizations tmtoolkit.\nBERTopic: Semantic embedding + clustering for advanced thematic discovery. Ideal when corpora require advanced semantic processing and non-trivial topic detection.\nMALLET: High-scale LDA implementations in Java.\nscikit-learn + Gensim: Effective combination for educational or grassroots projects, with large community and established workflows.\nInfraNodus: Recommended if you want exploratory visualizations, semantic maps, and narrative analysis.\nTop2Vec: Combine document embedding and topic modeling, automatically detect the number of topics. Useful in exploratory phases where the thematic structure is not known a priori.\n\n\n\n\n\n\n\nWarningSome tools (e.g. Top2Vec, InfraNodus) are less well-established in mainstream SSH methodology, but are valuable when used critically.Tools such as BERTopic are based on newer technologies (semantic embedding), therefore they require more attention in the interpretation phase.Performance varies based on pre-processing quality and corpus type. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTool / Model\nType\nRecommended usage context\n\n\n\n\nLDA (via Gensim / scikit-learn)\nProbabilistic topic modeling\nStructured corpora, homogeneous language, standard thematic analysis\n\n\nBERTopic\nSemantic topic modeling\nComplex corpora with semantic variation, specialized or multidisciplinary discourse\n\n\nTop2Vec\nUnsupervised embedding-based topic modeling\nPreliminary exploration of large text corpora, automatic topic count detection\n\n\nMALLET\nGibbs sampling topic modeling\nLarge-scale datasets requiring high computational efficiency\n\n\nInfraNodus\nNetwork visualization and analysis\nExploratory phase, narrative or cultural analysis, conceptual mapping\n\n\ntmtoolkit\nIntegrated analytical toolkit\nPre-processing, coherence evaluation, end-to-end Python workflow\n\n\nspaCy\nNLP library\nTokenization, lemmatization, syntactic parsing\n\n\n\n\n\n\n5.1.5 1.5 Guidelines for use in SSH Projects\nThe use of topic modeling and text mining techniques in the contexts of SSH requires the adoption of a series of methodological measures that guarantee the quality, transparency and reproducibility of the analyses.\nThe main aspects to consider include:\n\nTextual pre-processing phase.\nThe preliminary processing of textual data is a necessary condition for the reliability of the results. Operations such as tokenization, elimination of stopwords, lemmatization and lexical normalization must be carried out rigorously and explicitly documented. Tools such as spaCy (for language processing) and tmtoolkit (for integrated management of analytical pipelines) allow you to automate these steps, reducing the variability introduced by manual interventions.\nModel selection and appropriateness with respect to the corpus.\nThe choice of the topic modeling algorithm must be consistent with the nature of the corpus and with the analytical objectives of the project. Traditional probabilistic models such as LDA, available through libraries such as Gensim or scikit-learn, are appropriate for well-structured and linguistically homogeneous corpus analyses. Models based on semantic embeddings (e.g. BERTopic, Top2Vec) offer greater sensitivity to lexical nuances and are suitable for complex corpora or corpora characterized by high discursive variability. In computationally intensive scenarios, tools such as MALLET stand out for their efficiency and scalability. For research oriented towards the networked visualization of emerging topics, InfraNodus provides an innovative interface useful for the exploratory phase.\nEvaluation of the quality of the model.\nThe quality of the results cannot be taken for granted and requires a multi-level evaluation. From a quantitative point of view, it is advisable to use thematic consistency metrics Mimno, Wallach et al.,2011, many of which are implemented directly in tmtoolkit. However, these metrics must always be accompanied by qualitative validation by the researcher, based on the critical reading of the topics and their interpretability with respect to the theoretical context and the empirical field of reference.\n\n\n5.1.5.1 References\nGeneral references on topic modeling and text mining\n&gt; See: Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent dirichlet allocation. Journal of machine Learning research, 3(Jan), 993-1022.\n➤ The foundational paper on the LDA model, the theoretical basis for most of the current topic modeling.\n&gt; See: Grimmer, J., & Stewart, B. M. (2013). Text as data: The promise and pitfalls of automatic content analysis methods for political texts. Political analysis, 21(3), 267-297.\n➤ Excellent methodological overview of the use of text mining in political and social sciences.\n&gt; See: Chang, J., Gerrish, S., Wang, C., Boyd-Graber, J., & Blei, D. (2009). Reading tea leaves: How humans interpret topic models. Advances in neural information processing systems, 22.\n➤ The paper introduces new quantitative methods, validated by large studies with users, to measure the semantic significance of topics extracted from probabilistic topic models.\nTechnical references and specific tools\n&gt; See: Řehůřek, R., & Sojka, P. (2010). Software framework for topic modelling with large corpora.\n&gt; See: Angelov, D. (2020). Top2vec: Distributed representations of topics. arXiv preprint arXiv:2008.09470.\n&gt; See: Grootendorst, M. (2022). BERTopic: Neural topic modeling with a class-based TF-IDF procedure. arXiv preprint arXiv:2203.05794.\nRecommended for courses or self-training\n&gt; See: Jockers, M. L. (2013). Macroanalysis: Digital methods and literary history. University of Illinois Press.\n➤ Application of topic modeling and other digital techniques in the humanities.\n&gt; See: Silge, J., Robinson, D., & Robinson, D. (2017). Text mining with R: A tidy approach (p. 194). Boston (MA): O’reilly\n➤ Introductory but comprehensive, widely used for teaching in digital humanities courses.",
    "crumbs": [
      "Part 1: Fundamentals of Generative AI e Tools",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Use Cases in Social Sciences and Humanities (SSH) Research</span>"
    ]
  },
  {
    "objectID": "1-20250726-ricercaSSH.html#assisted-qualitative-analysis-automatic-coding-and-sentiment-analysis-1",
    "href": "1-20250726-ricercaSSH.html#assisted-qualitative-analysis-automatic-coding-and-sentiment-analysis-1",
    "title": "5  Use Cases in Social Sciences and Humanities (SSH) Research",
    "section": "5.2 2. Assisted Qualitative Analysis: Automatic Coding and Sentiment Analysis",
    "text": "5.2 2. Assisted Qualitative Analysis: Automatic Coding and Sentiment Analysis\nThe use of AI tools in qualitative analysis is progressively changing the way textual material is processed and interpreted in the SSH context. Qualitative analysis assisted by NLP tools can expand the scale and efficiency of analytical work, but requires a high level of methodological control, transparency in operational choices and critical reflection on the results produced. Two relevant applications in this area are automatic content coding and sentiment analysis, both intended as forms of support, and not a replacement, for the researcher’s interpretative activity.\n\n5.2.1 2.1 Automatic Coding\nIt consists of assigning labels or thematic categories to segments of text based on learned rules (through supervised training) or lexical correspondences (unsupervised approaches).\nThis is particularly useful in projects based on interviews, focus groups, testimonials, or open-ended responses, where the analyst has to handle large volumes of data.\nTools such as spaCy, tmtoolkit or Transformers by HuggingFace allow you to automate the semantic annotation phase, facilitating the construction of replicable and systematic encodings. However, analytical validity depends on the precision of the models and the consistency between the categories produced and the theoretical framework of reference.\nReferences:\n&gt; See: Saldana, J. (2025). The Coding Manual for Qualitative Researchers. SAGE Publications Limited\n➤ Theoretical-methodological manual for qualitative coding, with reflections on automatic integration.\n&gt; See: Benoit, K., Watanabe, K., Wang, H., Nulty, P., Obeng, A., Müller, S., & Matsuo, A. (2018). quanteda: An R package for the quantitative analysis of textual data. Journal of Open Source Software, 3(30), 774-774. 8\n➤ R tool that supports complete workflows for text encoding and classification.\n\n\n5.2.2 2.2 Sentiment Analysis\nTraditionally more widespread in commercial fields, it now also finds relevant applications in social research. It makes it possible to identify the emotional or evaluative polarity expressed in the texts (positive, negative, neutral), or to detect more complex emotions (anger, trust, anxiety, etc.).\nClassical sentiment analysis methods are based on predefined dictionaries of words marked with polarity (e.g. “happy” → +1, “disappointed” → -1).\nMore recently, models based on neural networks (e.g. BERT, RoBERTa) are able to grasp polarity by considering the semantic context of the sentence, with greater robustness on long or ambiguous texts.\nTools such as NRC Emotion Lexicon or EmoLex allow you to detect complex emotions (joy, fear, anger, trust…), not just positive or negative feelings.\nThis approach is useful for the analysis of narratives, testimonies or political speeches, in which the affective dimension is articulated in a more nuanced way. In qualitative research, it can be used to analyze attitudes towards social phenomena, public policies, institutions or public figures, in contexts such as social media, interviews or historical archives.\n\n\n\n\n\n\nWarning\n\n\n\nHowever, it should be emphasized that sentiment analysis, in particular, suffers from linguistic ambiguities, irony, and dependence on the textual domain. For this reason, many studies recommend its triangulated use: as a support for manual analysis, as a preliminary exploratory filter or as an integrated technique in mixed designs.\n\n\nReferences:\n&gt; See: Mohammad, S. M., & Turney, P. D. (2013). Crowdsourcing a word–emotion association lexicon. Computational intelligence, 29(3), 436-465. 8\n&gt; See: Pang, B., & Lee, L. (2008). Opinion mining and sentiment analysis. Foundations and Trends® in information retrieval, 2(1–2), 1-135.",
    "crumbs": [
      "Part 1: Fundamentals of Generative AI e Tools",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Use Cases in Social Sciences and Humanities (SSH) Research</span>"
    ]
  },
  {
    "objectID": "1-20250803-writingAI.html",
    "href": "1-20250803-writingAI.html",
    "title": "6  Artificial Intelligence for Academic Writing: Strategies, Applications, and Limitations",
    "section": "",
    "text": "6.1 1. Integration of LLM-based Tools in academic writing\nThe integration of tools based on advanced language models (LLMs) into academic writing is significantly transforming the practices of scientific composition, from exploratory phases to text production.\nAlthough writing remains an irreplaceable skill in the process of knowledge construction, AI can be employed as a co-authorial tool to assist researchers with specific tasks, while maintaining human control over content and argumentation.",
    "crumbs": [
      "Part 1: Fundamentals of Generative AI e Tools",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Artificial Intelligence for Academic Writing: Strategies, Applications, and Limitations</span>"
    ]
  },
  {
    "objectID": "1-20250803-writingAI.html#integration-of-llm-based-tools-in-academic-writing",
    "href": "1-20250803-writingAI.html#integration-of-llm-based-tools-in-academic-writing",
    "title": "6  Artificial Intelligence for Academic Writing: Strategies, Applications, and Limitations",
    "section": "",
    "text": "ImportantTransparency and traceability in the use of AI in academic contexts\n\n\n\nIn recent years, the use of GAI in the production of scientific content has raised significant issues regarding research ethics, authorship attribution and methodological transparency.\nAs a result, several International Institutions, Funding Agencies, and Scientific Journals have issued guidelines that recommend—or in some cases mandate—explicit disclosure of the use of automated language models such as ChatGPT within research outputs.\n\n\n\n6.1.0.1 Key Policies and Guidelines\nThe main legislative and regulatory references include:\n\nCOPE – Committee on Publication Ethics\nIt published a guideline in 2023 stating that authors should clearly state whether, how and at what stage of writing they used AI tools, specifying that models cannot be credited as co-authors. (COPE position – Authorship and AI, 2023\nEuropean University Institute (EUI)\nThe 2024 document Guidelines for the Responsible Use of AI in Research requires that any substantial use of AI in the writing, summarizing or editing of academic texts must be explicitly documented, at least in metadata, acknowledgments, or a dedicated methodological section. (EUI Ethics Committee 2024\nNature, Science, Springer, Elsevier\nLeading international scientific journals, including Nature, Science and Elsevier, prohibit the attribution of co-authorship to AI models and require, in methods or acknowledgements, a technical note specifying whether parts of the text (e.g., abstract, bibliography, reformulations) were generated or assisted through LLMs tools.\nEuropean Funding Bodies (e.g. ERC, Horizon Europe, MSCA - Marie Skłodowska‑Curie)\nWhile not yet enforcing strict rules, some funding agencies include the disclosure of automated tools used in the preparation of proposals within ethical guidelines, in line with FAIR (Findable, Accessible, Interoperable, Reusable) principles and the European Code of Conduct for Research Integrity (ALLEA 2023.)\n\nIn this scenario, the use of AI for academic writing cannot be considered neutral or implicitly accepted: researchers must transparently state the extent and nature of technological involvement, clarifying whether AI served as linguistic support, a paraphrasing tool, or contributed to the generation of structured portions of the text.\nThis new responsibility reflects a broader transformation in traceability criteria within contemporary scientific research, where tools—as well as results—are subject to accountability. (Novelli, Taddeo, Floridi, AI & Society, 2024).\n\n\n6.1.1 1.1 Generation of the ‘First Draft’: title, abstract, introduction\nThe elaboration of a first draft constitutes one of the most critical and often burdensome phases of the academic writing process.\nIn this context, GAI can offer significant support by facilitating the initiation of text production through the automated generation of initial drafts related to standardized sections of a scientific article, such as the title, abstract, and introduction.\nThis intervention is configured as a form of structural assisted writing, in which the model does not provide conceptually autonomous content but proposes plausible textual templates based on data provided by the user, such as disciplinary field, research objective, employed methodology, or research question.\nFor example:\n\nFor a title, AI can suggest alternatives with varying degrees of specificity, synthesis and attractiveness, while respecting disciplinary style conventions.\nFor an abstract, generation can follow predefined structures (e.g., IMRaD), or adapt to specific calls (e.g., Horizon Europe, ERC), incorporating key elements: problem, methodology, expected results, and implications.\nFor an introduction, AI can assist in setting up a logical-argumentative sequence, articulating the relevance of the topic, a synthetic state of the art, and an initial statement of the objectives or hypothesis.\n\nThe value of an AI-generated first draft does not lie in its final quality—which generally remains inferior to that produced by an experienced researcher—but rather in its heuristic and propulsive function: it helps overcome the initial creative block, offers a textual skeleton to work from, and suggests coherent linguistic structures, also useful for non-native authors.\n\n\n\n\n\n\nTipIt must be emphasized, however, that the automatic first draft should be considered an unvalidated product, subject to full scientific and stylistic revision.\n\n\n\n\n\n\nThe greatest risk is the generation of text that is formally plausible but conceptually vacuous (synthetic plausibility), which may include generic statements, theoretical gaps or fabricated citations*(“hallucinations”).\n\n\n\n\n\n\nNoteTherefore, the use of a first draft must be accompanied by metalinguistic and methodological skills that enable the researcher to critically assess its epistemic coherence, rhetorical adequacy, and disciplinary relevance.\n\n\n\n\n\n\n\n6.1.1.1 Comparison diagram of first draft with AI and by the researcher’s\n\n\n\n\n\n\n\n\nAspect\nAI-Generated First Draft\nResearcher-Reviewed Version\n\n\n\n\nArgumentative structure \nFormally coherent, but tends to be generic\nCoherent with the project’s theoretical and logical framework\n\n\nDisciplinary vocabulary \nAdequate in general terms, but often superficial\nAligned with the vocabulary of the specific discipline\n\n\nConsistency with the research project\nRisk of irrelevant generalizations\nAligned with objectives, context, and research questions\n\n\nMethodological accuracy\nStandard methodological descriptions, not contextualized\nAccurate, with reference to methods actually used\n\n\nStyle and linguistic register\nUniform and readable, but not always discipline-appropriate\nTailored to the target (journal, grant call, academic audience)\n\n\nCitations and references\nSometimes missing or generated imprecisely\nProperly inserted and verifiable\n\n\nOriginality of contribution\nLow: tends to reproduce frequent and neutral patterns\nExplicit articulation of the original contribution to the literature\n\n\n\n\n\n\n6.1.2 1.2 Key differences between AI-generated drafts and researcher-reviewed academic writing\n\nArgumentative Structure\nThe AI‐generated draft often reproduces standard rhetorical patterns but frequently lacks conceptual grounding. The revised version establishes a logically coherent structure aligned with the project’s hypothesis and theoretical framework.\nDisciplinary Language\nAlthough language models mimic an academic register, they tend to rely on generic or overused terms. The researcher refines the vocabulary to adhere strictly to the conventions and epistemological precision required by the discipline.\nAlignment with Research Design\nThe AI draft may introduce unwarranted generalizations or assumptions. Human revision restores internal consistency by confining the text to the actual objectives, methods, and research questions of the study.\nMethodological Accuracy\nAI often defaults to generic methodological descriptions (e.g., “qualitative interviews,” “thematic analysis”) without contextual detail. The researcher amends these sections to reflect the specific methodological choices actually employed.\nStylistic Appropriateness\nWhile the automated draft is syntactically fluent, its tone is stylistically neutral. The human editor introduces modulations of register, emphasis, and tone according to the target audience (e.g., scholarly journal readers, funding panel, academic committee).\nCitations and Source Integrity\nModels can omit references or generate them inaccurately. The researcher verifies each citation’s accuracy and integrates all sources coherently into the narrative.\nOriginality of Contribution\nAI outputs tend to default to safe, conventional formulations. The researcher highlights the original contribution, theoretical novelty, or methodological innovation that distinguishes the work.\n\n\n\n6.1.3 2. Co-editing and stylistic refinement: advanced linguistic assistance in academic writing\nBeyond the automatic generation of content, one of the most impactful applications of AI in scholarly writing lies in the co-editing phase, understood as targeted support for the linguistic and rhetorical polishing of pre-existing text.\nIn this role, AI does not function as an autonomous knowledge generator but as a parametric editorial assistant, capable of proposing localized revisions or comprehensive reformulations depending on the usage context and communicative objectives.\nCo-editing proves especially effective in two critical sections of academic production: the methodological description and the discussion.\nIn the methodological section, AI can enhance expository clarity by eliminating lexical ambiguities and smoothing the presentation of adopted methods and techniques.\nIn the discussion section, it can facilitate the articulation of inferences and the linkage between results, literature and theoretical implications—thereby improving argumentative coherence and syntactic readability.\nAdvanced assisted writing tools allow users to parameterize the rewriting process by specifying desired levels of formality, target length, scientific register or communicative tone (e.g., assertive, descriptive, popular).\nThis makes the AI intervention highly adaptable to different communication contexts, whether preparing a peer-review journal abstract, a public presentation, or a policy brief for institutional stakeholders.\n\n\n\n\n\n\nTipIt is, however, essential that these operations not be regarded as neutral automations: every AI-generated suggestion must undergo critical evaluation, taking into account conceptual consistency, terminological fidelity, and adherence to the original communicative intent.\n\n\n\nThe collaboration between author and system must remain reflective and deliberate, ensuring human oversight over argumentative density, epistemic alignment, and the rhetorical identity of the text.\n\n\n\n6.1.3.1 Operational features of automated co-editing\nThe AI platforms available today for academia offer a growing range of advanced features.\nIt’s notable to remember:\n\nControlled rewriting (constrained paraphrasing): the user can request rewrites with specific stylistic or rhetorical constraints, such as increased conciseness, variation in tone or translation from technical to popular language. This feature is useful for adapting the same content to different audiences (e.g., specialist journal, European call, internal communiqué).\nSelective synthesis: AI is able to detect redundant or excessively dense passages-for example, verbose methodological sections or scattershot theoretical descriptions-and propose a more essential version, while maintaining informational relevance and respecting any editorial space constraints.\nTextual cohesion and logical transitions: by analyzing thematic and rhetorical progression, the system can suggest the addition or modification of connectives, transitions, and textual markers, improving the internal fluency of academic discourse, especially in texts intended for international reviewers or in multilingual contexts.\n\n\n\n\n6.1.4 3. Guided drafting for calls and grants: examples of applicable prompts\nThe use of LLMs in the drafting phase of project proposals, application forms or grant abstracts is today one of the most strategic areas for the academic and competitive research sector.\nThese tools are configured as semi-automatic writing assistants, capable of generating preliminary versions (first drafts) of complex textual sections, starting from suitably calibrated prompts. This functionality is particularly effective in supporting strategic writing, that is, writing geared toward calls with specific formal and rhetorical requirements, where clarity of objectives, measurability of impacts, methodological relevance, and compliance with the language of the funder are decisive elements.\n\n6.1.4.0.1 Structured prompts: how to drive textual generation\nThe effectiveness of the result depends largely on the quality and accuracy of the prompt.\nThe following are some effective prompts in grant writing:1\n✏️ Prompt for project title definition (concise and focused)\n“Propose five possible titles for a European project exploring the impact of digital platforms on youth political participation. The title should be short, clear, evocative and suitable for a Horizon Europe call”\n✏️ Prompt for generating project abstract (first draft)\n“Write an abstract of up to 250 words for a project that aims to promote social cohesion through digital interventions in marginalized urban settings. The text should include objectives, methodological approach, expected impacts and relevance to the call.”\n✏️ Prompt for academic calls (e.g., ERC, Marie Skłodowska-Curie)\n“Draft a scholarly synthesis for an MSCA application studying the building of language capital among migrants and educational institutions in Europe. The text should be consistent with the European format and use a formal but accessible tone.”\n✏️ Prompt for methodological sections\n“Reformat this methodological paragraph (paste text) in a more technical way, with academic vocabulary, while maintaining clarity and avoiding redundancy. Specify that this is a qualitative design based on semi-structured interviews.”\n✏️ Prompt for language adaptation\n“Rewrite this text (paste) in formal academic English, suitable for a submission in the humanities. Maintain conceptual precision and disciplinary vocabulary, avoiding literal translations.”\n✏️ Prompt for call for paper or announcement\n“Draft a short description (max 250 words) for a call for paper on “Ethics and AI in Social Research.” The paper should attract theoretical and empirical contributions, with an authoritative and inclusive tone.”\n\n\n\n6.1.5 3.1 Ethical considerations and statement of use.\nIt is also essential to distinguish between linguistic assistance and the generation of original scientific content: only the former can be delegated, while the construction of the argumentation, the selection of sources, and the definition of the thesis remain tasks proper to scientific authorship.\n\n\n\n\n\n\nImportantUsage considerations\n\n\n\nIt is essential to remember that these texts should not be considered final products, but intermediate working tools: drafts on which to critically intervene to correct, deepen, position.\nIn addition, templates can be gradually “trained” to better meet user needs through iterative prompts and progressive refinements of context (e.g., by providing pre-written sections or bullet points with project data).\nThe use of AI in academic writing, particularly in evaluation (grant, peer review), requires methodological transparency.\nIt is recommended to state, in the methodological notes or metadata of the proposal, the possible use of AI tools, especially when intervening in substantial stages of writing.\n\n\n\n\n6.1.6 Tools for academic writing\n🔎 JENNI\n🔎 RYTR\n🔎 SCALENUT\n🔎 JASPER\n🔎 PROWRITINGAID\n🔎 TEXTCORTEX\n🔎 QUILLBOT\n🔎 HYPERWRITEAI\n🔎 COPY\n🔎 PAPERPAL\n\n6.1.6.1 Comparison of the features of the suggested tools\n\n\n\n\n\n\n\n\n\n\nTool\nFree Plan/Trial Available\nSupported Languages\nBest for\nAccess Type\n\n\n\n\nRytr\nFree plan\nMore than 30 languages\nPlagiarism and coherence check\nWeb, Chrome extension\n\n\nScalenut\nFree trial\nEnglish only\nThesis rewriting and editing\nWeb\n\n\nJenni AI\nFree plan\nMultiple languages\nCreative writing solutions\nWeb\n\n\nJasper AI\nFree trial\nMore than 30 languages\nMultilingual thesis writing\nWeb\n\n\nProWritingAid\nFree plan\nEnglish only\nCitations and references\nWeb, browser extensions\n\n\nTextCortex\nFree trial\nMore than 25 languages\nStrengthening arguments\nWeb, Chrome extension\n\n\nWritesonic\nFree trial\nMore than 25 languages\nAdvanced thesis writing with GPT-4\nWeb\n\n\nQuillbot\nFree plan\nMore than 30 languages\nLanguage refinement and enhancement\nWeb, extensions\n\n\nHyperWriteAI\nFree trial\nAny language\nThesis rewriting and editing\nWeb\n\n\nCopy.ai\nFree trial\nMore than 30 languages\nThesis generation and argument enhancement\nWeb\n\n\nPaperpal\nFree version\nMore than 30 languages\nAcademic translations and coherence check\nWeb, MS Word add-in\n\n\n\n\n\n\n6.1.7 References\n\nSee: Pividori M, Greene CS. (2024) A publishing infrastructure for Artificial Intelligence (AI)-assisted academic authoring..\nSee: Kwon, D. (2025). Is it OK for AI to write science papers? Nature survey shows researchers are split.\nSee: Salvagno M, Taccone FS, Gerli AG. (2023) Can artificial intelligence help for scientific writing? See: Khalifa, M., & Albadawy, M. (2024). Using artificial intelligence in academic writing and research: An essential productivity tool. Computer Methods and Programs in Biomedicine Update, 5, 100145.\nSee: Hosseini, M., Rasmussen, L. M., & Resnik, D. B. (2023). Using AI to write scholarly publications. Accountability in Research\nSee: Meyer, JG, Urbanowicz, RJ, Martin, PCN et al.(2023)ChatGPT and large language models in academia: opportunities and challenges.\nSee: Godwin, R. C., DeBerry, J. J., Wagener, B. M., Berkowitz, D. E., & Melvin, R. L. (2024). Grant drafting support with guided generative AI software.\nSee: Kasierski, B., & Fagnano, E. (2024, October). Optimizing the Grant Writing Process: A Framework for Creating a Grant Writing Assistant Using ChatGPT 4.\nSee: Panda, S., & Kaur, D. N. (2024). Exploring the role of generative AI in academia: Opportunities and challenges",
    "crumbs": [
      "Part 1: Fundamentals of Generative AI e Tools",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Artificial Intelligence for Academic Writing: Strategies, Applications, and Limitations</span>"
    ]
  },
  {
    "objectID": "1-20250803-writingAI.html#section",
    "href": "1-20250803-writingAI.html#section",
    "title": "6  Artificial Intelligence for Academic Writing: Strategies, Applications, and Limitations",
    "section": "6.2 ",
    "text": "6.2",
    "crumbs": [
      "Part 1: Fundamentals of Generative AI e Tools",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Artificial Intelligence for Academic Writing: Strategies, Applications, and Limitations</span>"
    ]
  },
  {
    "objectID": "1-20250803-writingAI.html#footnotes",
    "href": "1-20250803-writingAI.html#footnotes",
    "title": "6  Artificial Intelligence for Academic Writing: Strategies, Applications, and Limitations",
    "section": "",
    "text": "Grant writing is the structured process of writing a project proposal aimed at obtaining funding from public bodies, private foundations, international organizations, or research agencies. It is a highly specialized form of writing, combining rhetorical, design, normative and strategic skills.\nOperationally, grant writing is not limited to writing the text, but also involves: - the analysis of the relevant call for proposals and its eligibility and evaluation criteria; - the ability to translate a scientific idea into a clear and convincing project narrative; - the structuring of content according to predefined formats (e.g., abstract, state of the art, methodology, impacts, budget, work packages); - the adaptation of style and vocabulary to the expectations of the funding body (policy-driven, evidence-based, stakeholder-oriented).\nIn academia, grant writing is a strategic cross-cutting skill, as it conditions access to resources critical to research, career, and scientific visibility. The ability to write effective proposals is now considered, for all intents and purposes, an integral part of the researcher profile, particularly in national and international competitive contexts (e.g., Horizon Europe, ERC, Marie Skłodowska Curie, PRIN, PNRR).↩︎",
    "crumbs": [
      "Part 1: Fundamentals of Generative AI e Tools",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Artificial Intelligence for Academic Writing: Strategies, Applications, and Limitations</span>"
    ]
  },
  {
    "objectID": "1-20250812-workflow.html",
    "href": "1-20250812-workflow.html",
    "title": "7  Mapping the Workflow of an Academic Paper: Integrating AI at Every Stage",
    "section": "",
    "text": "7.1 1. Introduction: why a systemic approach to AI use\nIntegrating GAI into scholarly writing — given its capabilities, limitations, and constraints — cannot be reduced to the occasional use of stand-alone tools.\nThe proliferation of applications that automate, assist or augment various phases of scientific production demands a comprehensive methodological reflection on the overall structure of intellectual work. In research, especially within the Social Sciences and Humanities (SSH), writing is not merely a final step but a cognitive device through which analytical results are consolidated, articulated, and debated.\nAccordingly, adopting LLM-based technologies capable of generating text, reformulating concepts, synthesizing sources or suggesting alternative phrasings can fundamentally reshape the editorial process.\nA systemic approach enables researchers to:",
    "crumbs": [
      "Part 1: Fundamentals of Generative AI e Tools",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Mapping the Workflow of an Academic Paper: Integrating AI at Every Stage</span>"
    ]
  },
  {
    "objectID": "1-20250812-workflow.html#introduction-why-a-systemic-approach-to-ai-use-1",
    "href": "1-20250812-workflow.html#introduction-why-a-systemic-approach-to-ai-use-1",
    "title": "7  Mapping the Workflow of an Academic Paper: Integrating AI at Every Stage",
    "section": "",
    "text": "WarningSystematically mapping AI–writing interactions is essential to ensure transparency, epistemological control, and effective valorization of the human contribution.\n\n\n\n\n\n\n\n\nPinpoint where AI can be employed both effectively and responsibly\n\nAvoid inappropriate delegation that risks originality or theoretical coherence\n\nDevelop a sustainable, replicable strategy guiding scholars from project design to final publication.",
    "crumbs": [
      "Part 1: Fundamentals of Generative AI e Tools",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Mapping the Workflow of an Academic Paper: Integrating AI at Every Stage</span>"
    ]
  },
  {
    "objectID": "1-20250812-workflow.html#stages-of-academic-writing-and-ai-touchpoints",
    "href": "1-20250812-workflow.html#stages-of-academic-writing-and-ai-touchpoints",
    "title": "7  Mapping the Workflow of an Academic Paper: Integrating AI at Every Stage",
    "section": "7.2 2. Stages of academic writing and AI “Touchpoints”",
    "text": "7.2 2. Stages of academic writing and AI “Touchpoints”\nAcademic writing unfolds as a multi-stage process rather than a single, linear task. Each stage entails distinct cognitive, linguistic, and organizational demands, which can be supported — if methodologically integrated — by AI tools.\nA structured map of principal AI “touchpoints” across the editorial workflow is given.\nThis section aims to provide a structured mapping of the main points of interaction (‘touchpoints’) between GAI-based technologies and the various stages of the academic writing process.\n\n\n\n\n\n\nNoteWhen used critically and in context, these tools can support, optimise, or strengthen scientific writing activities, while preserving the irreplaceable role of theoretical reflection and epistemological control by the researcher.\n\n\n\n\n\n\nThe GAI should be integrated at specific points in the editorial process, with differentiated functions — from suggestions to assistance, partial automation or validation — avoiding a totalising or automatic approach. Its value lies in its ability to intervene at targeted stages of the process, acting as a methodological ally in the most critical or time-consuming steps, without compromising scientific rigour.\nThe proposed structure adopts a logical-procedural approach and is developed through 6 operational segments (editorial workflow), corresponding to six strategic junctures in the cycle of elaboration and formalisation of the academic text. It is a composite process, and each step involves conceptual, operational and stylistic choices.\nAlthough these phases do not necessarily follow a strictly linear order (as they may involve returns, revisions or restructuring), it is possible to identify a shared procedural grid that allows the writing cycle to be mapped from initial design to publication.\n\n7.2.1 a. Defining scope and formulating the research question\n\nThe first phase involves identifying the relevant subject area, defining the research problem and preliminarily structuring the objectives.\n👉🏻 In this phase, AI can be useful as an exploratory tool for generating provisional titles, summarising areas of interest that emerge from the literature or comparing alternative formulations of the same hypothesis.\n\n\n\n\n\n\nWarningLLM-based tools can help refine search queries, provided that human intervention is geared towards verifying their epistemological consistency, originality and disciplinary relevance.\n\n\n\n\n\n\n\n\n\n7.2.2 b. Finding, selecting and summarising scientific literature\n\nOne of the fundamental components of academic writing is the construction of a theoretical framework.\n👉🏻 Recent applications of AI in the field of bibliography allow for the automation, at least in part, of certain operations: searching for primary and secondary sources, automatic extraction of abstracts and keywords, construction of concept maps, identification of related or contrasting studies.\n\n\n\n\n\n\nWarningHowever, the critical selection of sources, the analysis of references and the assessment of relevance to one’s project remain highly cognitive activities that cannot be delegated.\n\n\n\n\n\n\n\n\n\n7.2.3 c. Planning the argumentative structure (outline)\n\nOnce the field of investigation has been defined and the bibliographic material acquired, the text is structured by defining an outline.\nThis step requires the ability to logically organise the sections (e.g. IMRaD or narrative structure), assign argumentative functions to each paragraph and anticipate consistency between passages.\n👉🏻 AI can assist in generating outlines from a summary description of the project, suggesting coherent internal structures.\n\n\n\n\n\n\nWarningIt is the researcher’s responsibility to adapt the generated proposal to the specifics of their theoretical and methodological framework.\n\n\n\n\n\n\n\n\n\n7.2.4 d. Composition of the first draft\n\nThe drafting of the first draft is the moment when analyses, references and hypotheses are consolidated. Starting from refined prompts, GAI allows you to obtain preliminary versions of text sections, with a certain adherence to formal and structural criteria.\n\n\n\n\n\n\nWarningHowever, it is essential to emphasise that the quality of the content produced depends largely on the clarity of the input and the researcher’s ability to integrate, modify or reject what is proposed. In this perspective, AI takes on the role of editorial assistant, not author.\n\n\n\n\n\n\n\n\n\n7.2.5 e. Content review and stylistic refinement\n\nThis fifth phase involves editing, rephrasing and improving readability.\nAI can be used to rephrase complex sentences, harmonise the linguistic register, suggest logical transitions or optimise information density.\nSome tools also allow you to set specific parameters such as level of formality, tone, length or type of audience.\n\n\n\n\n\n\nWarningHowever, any proposed stylistic or syntactic changes must be evaluated in relation to the function that the text performs within the scientific discourse.\n\n\n\n\n\n\n\n\n\n7.2.6 f. Final check and preparation for submission\n\nThe final stage includes activities such as checking the accuracy of citations, consistency between the text and bibliography, adaptation to specific editorial styles (APA, MLA, Chicago, etc.), final linguistic verification and, where applicable, adaptation of the text to the standards required for submission.\nGAI can also provide support to facilitate formal standardisation, suggest target journals based on the content covered, and generate short accompanying texts (cover letters, abstracts for online submission).\n\n\n\n\n\n\nWarningEven at this stage, there is still a need for accurate human checking, especially in highly formalised contexts.",
    "crumbs": [
      "Part 1: Fundamentals of Generative AI e Tools",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Mapping the Workflow of an Academic Paper: Integrating AI at Every Stage</span>"
    ]
  },
  {
    "objectID": "1-20250812-workflow.html#step-by-step-case-studies-practical-applications-of-ai-in-the-editorial-workflow",
    "href": "1-20250812-workflow.html#step-by-step-case-studies-practical-applications-of-ai-in-the-editorial-workflow",
    "title": "7  Mapping the Workflow of an Academic Paper: Integrating AI at Every Stage",
    "section": "7.3 3. Step-by-step case studies: practical applications of AI in the editorial workflow",
    "text": "7.3 3. Step-by-step case studies: practical applications of AI in the editorial workflow\nTo encourage the informed adoption of GAI in the academic context, it is useful to propose a series of case studies, each focusing on a strategic moment in the writing cycle.\nThese examples are not prescriptive models, but rather operational scenarios that illustrate the potential applications of AI tools, highlighting their advantages, limitations and optimal conditions of use.\n👉 Each case is presented as a sequence of operations with an indication of the type of tool that can be used, the type of prompt or input required, and the expected contribution to the production or revision of the content.\n\n7.3.0.1 Case studies\n\n\n\n\n\n\n1️⃣ – Automatic Extraction of Key SSH Articles\nObjective: Quickly identify an initial selection of relevant sources to define the state of the art.\nProcedure:\n1. Query a semantic AI engine (e.g., Elicit, Scite Assistant) with a natural-language prompt (e.g.,“Key debates on digital inclusion in education”)\n2. Evaluate returned items—titles, abstracts, reliability metrics\n3. Manually confirm relevance; archive full texts.\nOutput: An initial systematic map of key sources, useful for building the theoretical background.\n\n\n\n\n\n\n\n\n\n2️⃣ – Guided generation of structured outlines\nObjective: Build a coherent argumentative structure for the scientific article, following the IMRaD format or equivalent.\nProcedure:\n1. Provide a prompt containing a provisional title, research objectives and expected output type (e.g. “Create an outline for an empirical social science paper on X”).\n2. Request a division into sections (Introduction, Methods, Results, Discussion) with brief descriptions of the contents of each.\n3. Verify that the proposals are consistent with the research design and adapt the structure according to the target (call for proposals, journal, conference).\nOutput: Flexible outline, consistent with the editorial standards of the discipline.\n\n\n\n\n\n\n\n\n\n3️⃣ – Draft introductory paragraph with critical review\nObjective: Produce a first draft of the introductory paragraph to be used as a working basis.\nProcedure:\n1. Enter the main research questions, theoretical references and empirical context into the prompt.\n2. Request a controlled generation, indicating length, tone and level of formality (e.g. “Generate a formal academic introductory paragraph, max 150 words”).\n3. Critically evaluate the text produced: relevance, quality of vocabulary, accuracy of statements.\n4. Modify, rewrite or integrate according to your own style and theoretical framework.\nOutput: A text that is consistent in form, to be refined in content to ensure originality and rigour.\n\n\n\n\n\n\n\n\n\n4️⃣ – Automatic verification of logical consistency between sections\nObjective: Check the consistency between the introduction, objectives and conclusions, highlighting any logical discrepancies or redundancies.\nProcedure:\n1. Upload the partial or complete text to an AI environment (e.g. Claude or ChatGPT with document upload mode).\n2. Enter a prompt such as: ’Assess whether the conclusions are consistent with the objectives stated in the introductory section. Indicate any omissions or inconsistencies.\n3. Analyse the feedback provided and verify the validity of the observations, adapting the relevant sections.\nOutput: Suggestions for strengthening the linearity of argumentation and logical clarity of scientific texts.\n\n\n\n\n\n\n\n\n\n5️⃣ – Generation of a cover letter for submission to an academic journal.\nObjective: Write a formal cover letter, in accordance with editorial standards, to be sent together with the manuscript.\nProcedure:\n1. Enter the main data (article title, journal name, summary of the contribution, authors’ affiliations).\n2. Request a formal cover letter, using a prompt such as: “Create a cover letter for the submission of a scientific article in the field of sociology, addressed to the editorial board of [journal name].”\n3. Review the proposed text, customising it according to the specifics of the manuscript and the target journal.\nOutput: A concise, professional letter, consistent with academic editorial practices.\n\n\n\n\n\n\n\n\n\n6️⃣ – Stylistic adaptation of the text to an international target audience\nObjective: Rewrite an article written in Italian into standardised scientific English, suitable for international publication.\nProcedure:\n1. Translate the text into English using specialised tools (e.g. DeepL Write, Papercup, Paperpal).\n2. Enter selected sections of the text into the AI platform and indicate the desired stylistic parameters: level of formality, length, disciplinary terminology.\n3. Review the proposals, checking for terminological accuracy, adherence to international standards and syntactic fluency.\nOutput: A coherent and readable version in English, ready for human proofreading or editorial review.",
    "crumbs": [
      "Part 1: Fundamentals of Generative AI e Tools",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Mapping the Workflow of an Academic Paper: Integrating AI at Every Stage</span>"
    ]
  },
  {
    "objectID": "1-20250812-workflow.html#from-map-to-method-recommendations-for-researcher-autonomy-1",
    "href": "1-20250812-workflow.html#from-map-to-method-recommendations-for-researcher-autonomy-1",
    "title": "7  Mapping the Workflow of an Academic Paper: Integrating AI at Every Stage",
    "section": "7.4 4. From map to method: recommendations for researcher autonomy",
    "text": "7.4 4. From map to method: recommendations for researcher autonomy\nThe representation of touchpoints between AI and the editorial process should not be understood as a simple descriptive diagram.\nOn the contrary, it represents the start of a methodological reflection that needs to be translated into conscious and intentional operational practices.\nA map is useful insofar as it becomes a method: a structured set of choices, procedures and checks that allow the researcher to interact with AI tools in a manner consistent with the principles of scientific research.\nFrom this perspective, the adoption of GAI cannot be interpreted as an automatic delegation of editorial skills, but as an opportunity to develop a new form of epistemic agency.\nThis agency refers to the ability of the research subject to consciously, reflectively and responsibly guide the process of knowledge production, selecting tools, interpreting data and evaluating content in light of explicit theoretical and methodological goals.\nIt implies critical autonomy in governing technical mediations, without relinquishing conceptual and ethical control over one’s scientific work.\n\n\n\n\n\n\nImportantEpistemic agency is a form of cognitive responsibility that distinguishes the producer of knowledge (the researcher) from a passive executor of technical automatisms.\n\n\n\nIt involves:\n• not delegating the construction of scientific argumentation entirely to AI or other external tools\n• knowing how to interpret and filter the information and suggestions generated independently\n• preserving theoretical intentionality, i.e. consistency between operational choices and the cognitive objectives of the research.\n\n\nThis results in an active and reflective approach, based on a balance between technical automation and theoretical control.\n\n\n\n\n\n\nWarningGAI can play an advanced supporting role in scientific production only if it is part of a solid methodological framework capable of preserving the integrity, authorship and originality of scientific contributions.\n\n\n\nThe challenge, therefore, is to transform the use of AI into a cognitive alliance guided by methodological awareness, rather than a standardised, opaque or disempowering process.\nOnly in this way will it be possible to integrate technological innovation without compromising the critical and design value of academic writing.\n\n\n\n7.4.1 Recommendations\n❗️ Esplicitare il ruolo dell’AI nel flusso di lavoro\n👉🏻 Annotare in quali fasi e per quali scopi si è ricorso a strumenti generativi, contribuendo alla trasparenza metodologica.\nTale documentazione, integrata nel protocollo metodologico, contribuisce a garantire la riproducibilità, la trasparenza e l’integrità scientifica dell’elaborato, in linea con le raccomandazioni dei principali enti di finanziamento e riviste peer-reviewed.\n❗️ Valutare criticamente ogni output prodotto\n👉🏻 Nessun contenuto generato dall’AI dovrebbe essere accettato in modo automatico, ma analizzato in termini di coerenza con il progetto, adeguatezza scientifica e rigore formale.\nL’adozione acritica di contenuti sintetici o stilisticamente corretti, ma privi di profondità teorica o coerenza argomentativa, può compromettere la qualità complessiva del testo e alterarne il valore scientifico.\n❗️ Coltivare l’ibridazione tra competenze tecniche e conoscenze disciplinari\n👉🏻 Un uso efficace dell’AI nella scrittura accademica richiede una doppia alfabetizzazione: da un lato, la familiarità con le logiche di funzionamento dei modelli generativi (es. prompt design, temperature, contesto), dall’altro, la padronanza delle convenzioni linguistiche e argomentative della propria area di studio.\nSolo attraverso questa integrazione il ricercatore può operare scelte consapevoli, selezionare strumenti appropriati e orientare l’AI verso risultati epistemicamente validi.\n❗️ Garantire la tracciabilità delle revisioni\n👉🏻 È buona prassi mantenere un archivio delle modifiche apportate ai contenuti generati dall’AI, indicando quali porzioni sono state riscritte, adattate o validate.\nCiò consente non solo di conservare il controllo autoriale sul testo, ma anche di ricostruire a posteriori il processo redazionale in caso di valutazioni, revisioni o verifiche etiche. La tracciabilità si configura, in questo senso, come un criterio fondamentale di responsabilità scientifica e integrità editoriale.",
    "crumbs": [
      "Part 1: Fundamentals of Generative AI e Tools",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Mapping the Workflow of an Academic Paper: Integrating AI at Every Stage</span>"
    ]
  },
  {
    "objectID": "1-20250812-workflow.html#references",
    "href": "1-20250812-workflow.html#references",
    "title": "7  Mapping the Workflow of an Academic Paper: Integrating AI at Every Stage",
    "section": "7.5 References",
    "text": "7.5 References\n\nSee: Flanagin, A., Kendall-Taylor, J., & Bibbins-Domingo, K. (2023). Guidance for Authors, Peer Reviewers, and Editors on Use of AI, Language Models, and Chatbots.\nSee: Rodafinos, A. (2025). The Integration of Generative AI Tools in Academic Writing: Implications for Student Research.\nSee: Hanafi, A. M., Al-mansi, M. M., & Al-Sharif, O. A. (2025). Generative AI in Academia: A Comprehensive Review of Applications and Implications for the Research Process.\nSee: Johnson, C. W., & Paulus, T. (2024). Generating a Reflexive AI-Assisted Workflow for Academic Writing.\nSee: Bairagi, M., & Lihitkar, S. R. (2025). Empowering Research Workflows and Information Retrieval in Academic Libraries through AI Tools.",
    "crumbs": [
      "Part 1: Fundamentals of Generative AI e Tools",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Mapping the Workflow of an Academic Paper: Integrating AI at Every Stage</span>"
    ]
  },
  {
    "objectID": "2-20250820-AIcreativepartner.html",
    "href": "2-20250820-AIcreativepartner.html",
    "title": "8  AI as a Creative Partner",
    "section": "",
    "text": "8.1 1. Advanced Brainstorming",
    "crumbs": [
      "Part 2: Advanced applications of AI in Research and Creativity",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>AI as a Creative Partner</span>"
    ]
  },
  {
    "objectID": "2-20250820-AIcreativepartner.html#advanced-brainstorming-1",
    "href": "2-20250820-AIcreativepartner.html#advanced-brainstorming-1",
    "title": "8  AI as a Creative Partner",
    "section": "",
    "text": "8.1.1 1.1 Definition and Potential of Advanced Prompting\nThe brainstorming stage is a strategic moment in research design and development processes, permitting the initial exploration and comparison of a wide spectrum of conceptual and operational possibilities.\nGenerative AI (GAI) serves as a tool for cognitive amplification by offering researchers original proposals based on prompts composed in natural language.\nThe success of this procedure depends critically on the quality of the prompt that is, on the precision and clarity of the textual instructions provided to the model.\nWe refer to advanced prompting when the input text incorporates structural elements that guide the model towards a targeted and relevant output.\nSpecifically, a meticulously constructed prompt details:\n- The objective of the proposal\n- The method envisaged for its implementation\n- The expected impact within the application context.\nThis methodological approach transcends generic or unguided brainstorming, steering the AI–researcher interaction towards a structured co-design process rooted in clear aims and explicit methodological criteria.\n\n\n\n\n\n\nNoteAdvanced prompting thus functions as an epistemic instrument, orienting the model to produce conceptually rigorous and operationally viable outcomes.\n\n\n\n\n\n\n\n\n8.1.2 1.2 Practical Application\nStructured advanced brainstorming involves directing the AI (e.g. ChatGPT) to generate ideas within strategic parameters, using precise instructions that channel its creative capacities.\nThis is not free-form or arbitrary ideation but a focused and purposeful exercise.\na. An advanced prompt does not simply request “ideas” but it formulates a composite query specifying:\n- The theme or problem (e.g. the central concept under investigation)\n- The constraints (e.g. budgetary limits, available tools, contextual boundaries)\n- The role (e.g. “respond as a researcher specialising in social pedagogy”).\n\n\n\n\n\n\nNoteThis compels the AI to produce pertinent and inventive proposals within a defined framework, akin to a genuine design workshop.\n\n\n\n\n\n\nb. Upon submission of the carefully crafted prompt, the AI returns a series of proposals, which may include:\n- An original research question\n- A feasible pilot project - A novel theoretical hypothesis\n- A hybrid methodological approach.\n\n\n\n\n\n\nNoteIn effect, this mirrors traditional brainstorming but is enhanced by the model’s capacity to synthesise vast sources, registers, and logical structures.\n\n\n\n\n\n\nc. As part of an iterative dialogue, the researcher can then:\n- Select and elaborate on particular ideas\n- Request alternative proposals\n- Impose new constraints (e.g. “Exclude approaches already detailed in the literature”, “Focus only on qualitative-visual methods”)\n- Combine multiple suggestions.\n\n\n\n\n\n\nNoteThus a dynamic, generative conversation unfolds between researcher and AI—reminiscent of peer brainstorming but conducted at high speed with great diversity.\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIn this way, AI becomes a co-designer, generating hypotheses, scenarios, and strategies that enrich the researcher’s repertoire and stimulate critical reflection.\n\n\n\n\n8.1.3 1.3 Construction of prompts with objective, method and expected impact\nWhen using GAI to generate project ideas or research questions, the formulation of the prompt is a crucial step.\nA prompt is not a simple question, but a strategic linguistic artefact, whose effectiveness depends on the user’s ability to translate a project intention into understandable and stimulating textual instructions for the model.\nA particularly effective structure for generating relevant and coherent content involves articulating the prompt into three fundamental components:\n1️⃣ Objective\nDefinition of the purpose of the proposal to be generated.\nIt can be educational (e.g. promoting critical thinking in primary school), social (e.g. promoting the inclusion of students with special educational needs), technological (e.g. experimenting with immersive environments for subject teaching) or scientific (e.g. exploring new variables in qualitative research).\n👉🏻 The objective clarifies the general purpose towards which the generation of ideas should be directed, semantically delimiting the model’s field of action.\n2️⃣ Method\nIllustration of the operational process or methodology to be used to achieve the objective.\nThis may refer to pedagogical strategies (e.g. project-based learning, flipped classroom, gamification), research approaches (e.g. case study, participant observation, controlled experimentation) or theoretical models (e.g. situated learning theory, socio-constructivist approach).\n👉🏻 Including the method in the prompt allows the model to structure proposals according to a consistent procedural logic, facilitating executive design.\n3️⃣ Expected impact\nDefine the desired effects of the proposal in educational, cognitive, social or scientific terms.\nThis may refer, for example, to improving engagement, developing cross-cutting skills, generating new empirical data or validating theoretical hypotheses.\n👉🏻 Specifying the impact encourages the model to generate proposals that are not only formally consistent but also geared towards an explicit transformative goal.\nThe integration of these three elements within the prompt results in a more precise semantic definition, allowing the model to produce higher quality outputs that are better aligned with specific contexts of use. This structure also facilitates the replicability of the process, paving the way for a systematisation of human-machine interaction in the design field.\n\n\n\n\n\n\nTipFor example, a prompt structured according to this logic could be as follows:\n\n\n\n👉🏻 “Generate three project ideas for lower secondary school with the aim of developing digital citizenship skills, using the debate methodology, with the expected impact of improving critical thinking and peer argumentation.”\nThrough a prompt of this type, the model is able to propose solutions that integrate educational objectives, operational tools and training goals, transforming automatic generation into a guided and dialogical process.\n\n\n\n\n8.1.4 1.4 Types of prompts for generating new ideas\nOnce they have acquired expertise in formulating structured prompts, users are able to consciously select from different types of prompts, each designed to stimulate specific forms of creative generation by the GAI.\nThe choice of prompt type must be calibrated in relation to the design requirements, the cognitive processes involved and the exploratory nature of the task.\nThe targeted adoption of these different modes of interaction allows the automatic generation to be directed towards specific goals, activating a dialogical process between AI and human design thinking.\nThe ability to select the most appropriate type of prompt for the context is, in this sense, an indicator of methodological maturity in the strategic use of artificial intelligence as a creative partner.\nAmong the main types of prompts:\n🔍 Exploratory prompts\nExploratory prompts are designed to increase the conceptual possibilities on a given topic.\nThey are used in the initial phase of a project or when you want to gain a broad, flexible and unconventional overview of the options available.\nThese prompts stimulate AI to generate divergent ideas, going beyond habitual cognitive paths.\n✏️ Example:\n“Identify and describe five unconventional theoretical frameworks or methodological approaches through which to explore the relationship between environmental education and social justice in school contexts. Explicitly exclude standard curricular approaches and favour critical, decolonial or situated paradigms.”\n🧬 Transformative prompts\nThis category of prompts aims to reformulate established practices in an innovative way.\nAI is guided towards reconfiguring existing models in order to produce original solutions based on already known elements.\nTransformative prompts are particularly useful in contexts of pedagogical innovation.\n✏️ Example:\n“Critically rethink the flipped classroom model in light of educational inequalities linked to the digital divide, with particular reference to marginalised or fragile school contexts. Develop a theoretical-methodological proposal that, while maintaining the dialogical and participatory structure of the original model, transforms its operating methods to make it culturally sensitive, accessible and applicable in the absence of stable digital infrastructure.”\n⁉️ Hypothetical or conditional prompts\nHypothetical prompts invite the model to explore imaginary scenarios or contexts that do not yet exist, which are useful for building conceptual prototypes or anticipating emerging problems.\nTheir strength lies in their ability to activate lateral thinking and mental simulation.\n✏️ Example:\n“Imagine a completely decentralised university system, with no physical locations and based exclusively on immersive environments and artificial intelligence technologies. What pedagogical models, evaluation criteria and social interaction devices should be developed to ensure the fairness, quality and sustainability of the educational path?”\n⚖️ Comparative prompts\nComparative prompts require the model to compare approaches, tools or strategies, highlighting their advantages, limitations and possible hybridisations.\nThis type of instruction is useful for stimulating analytical thinking and identifying areas for improvement in existing models.\n✏️ Example:\n“Compare the effectiveness of the phenomenological and experimental approaches in studying the subjective experience of learning in digital contexts. Highlight strengths, methodological limitations, and possibilities for integration into a mixed research design.”\n⛓ Restrictive prompts\nIn some cases, the originality of proposals emerges despite – or thanks to – a series of imposed constraints.\nRestrictive prompts limit the scope of AI, imposing strict conditions that force it to come up with novel solutions.\nThis mode is useful for testing creativity under pressure or in contexts with limited resources.\n✏️ Example:\n“Design an interdisciplinary research project on the theme of energy transition, to be carried out in a context with low technological access, no connectivity and extremely limited economic resources (maximum budget: £100). Indicate objectives, method, data collection tools and potential impacts.”\n\n\n8.1.5 1.5 Practical applications: advanced research, interdisciplinary design, academic innovation\nThe use of advanced prompting in research is not a theoretical exercise for its own sake, but a strategic tool for stimulating the generation of hypotheses, tracing design paths and anticipating theoretical and methodological choices.\nLLMs, in particular, can support the exploratory phase by expanding the possibilities for investigation and promoting a critical reorganisation of the initial paradigms.\nIn the Academic and Post-Doctoral context, GAI can be used to:\n\nexplore new interdisciplinary configurations, for example by formulating hypotheses of contamination between sociological, pedagogical and critical technology studies approaches;\n\ngenerate original research questions based on emerging and ambiguous themes that are difficult to address using traditional disciplinary logic;\n\nsimulate a preliminary mapping of the state of the art, highlighting gaps, theoretical overlaps and areas that have not yet been systematically addressed;\n\nproduce methodological design proposals compatible with field constraints, ethnographic contexts, marginal populations or fluid digital environments;\n\ngenerate impact scenarios or speculative hypotheses useful for the design of interdisciplinary grants, including in a foresight or anticipatory key.\n\nThe conscious use of prompting also allows the conceptual soundness of a project proposal to be tested, encouraging comparison between alternative options and stimulating the reformulation of weak or implicit passages.\nIn this context, AI does not produce knowledge, but organises raw materials, theoretical languages and hypotheses of connection between domains of knowledge, acting as a dialogical and generative tool.\n\n\n\n\n\n\nWarningThe effectiveness of the interaction depends on the epistemological maturity of the researcher, who remains responsible for selecting, validating and transforming the proposals generated. In this sense, advanced prompting is a cross-cutting skill of high strategic value, destined to become established in contemporary research as a reflective, project-based and productive practice.\n\n\n\n\n\n\n\n8.1.5.1 Examples of disciplinary use of prompting\n• Sociology\nGeneration of alternative scenarios based on complex qualitative datasets (interviews, ethnographic observations), or exploration of hypotheses of correlation between emerging social phenomena (e.g. technologies and inequalities) from an intersectional perspective.\n• History.\nSimulation of divergent interpretations of controversial events; hypotheses for reinterpreting sources according to alternative theoretical paradigms (e.g. environmental history, history from below); exploration of the methodological implications of microhistory in the digital age.\n• Education sciences\nDesign of innovative training devices for marginal contexts; exploration of the educational implications of AI in non-school environments; hybridisation between critical pedagogies and transformative approaches.\n• Philosophy\nGeneration of questions based on contemporary ethical dilemmas related to artificial intelligence, the posthuman, or deep ecology; comparison between schools of thought on notions such as autonomy, intelligence, responsibility.\n• Cultural studies and media studies\nCritical reformulation of key concepts (identity, agency, representation) in the context of digital cultures and algorithms; simulation of discursive analyses based on multimodal media corpora.",
    "crumbs": [
      "Part 2: Advanced applications of AI in Research and Creativity",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>AI as a Creative Partner</span>"
    ]
  },
  {
    "objectID": "2-20250820-AIcreativepartner.html#multi-level-argumentative-schemes",
    "href": "2-20250820-AIcreativepartner.html#multi-level-argumentative-schemes",
    "title": "8  AI as a Creative Partner",
    "section": "8.2 2. Multi-Level Argumentative Schemes",
    "text": "8.2 2. Multi-Level Argumentative Schemes\n\n8.2.1 2.1 Text mapping: concepts, relationships and hierarchies\nThe representation and organisation of ideas are a fundamental component in the processes of design, academic writing and critical thinking. In this context, GAI can be used to create textual concept maps that can convey the semantic complexity of a topic or research question in a structured form.\nUnlike traditional graphic maps, multi-level textual mapping is organised into a hierarchical structure of nodes, expressed through natural language and organised according to levels of depth.\n\n\nExample of a multilevel text mapping scheme\n\nThe main node represents the generative idea or central theme; sub-nodes derive from it, explaining thematic articulations, implications, related variables, counter-arguments or application examples. The entire map is configured as an orderly network of logical and semantic relationships, useful for both theoretical reflection and operational planning.\nGAI is capable of generating this structure from a simple prompt, provided that the instruction accurately indicates the mapping objective and the desired level of articulation.\n\n\n\n\n\n\nTipThe result can be used in a variety of contexts: designing teaching units, structuring scientific articles, defining project proposals, analysing complex issues.\n\n\n\n\n\n\n✏️ Prompt example to generate a multi-level text map:\n“As a model language, your main task is to generate a multi-level text map on the following topic: ’[Insert topic]”. Organise the map according to this structure: the first node is the central topic; the first-level nodes represent the main categories; the second-level nodes contain insights, examples or implications. If necessary, you can add additional levels.”\n\n\n8.2.2 2.2 Instructional strategies for developing text maps\nIn order for GAI to generate effective mapping, the user must provide a prompt with clear, structured, and conscious instructions.\n\n\n\n\n\n\nTipThe quality of the prompt plays a decisive role in guiding the model’s output towards a coherent, organised form that is functional for its intended use.\n\n\n\n\n\n\nIn particular, it is useful to specify:\n\nthe central theme to be developed\nthe type of conceptual relationships to be highlighted (causes, effects, implications, alternatives, etc.)\nthe level of depth required for each conceptual node.\n\n✏️ Prompt examples:\n“Develop a multi-level textual map from the concept of [topic], detailing objectives, pedagogical strategies, digital tools, and potential challenges.”.\n“Organise the topic of digital citizenship education into a multi-level textual structure, distinguishing between theoretical principles, practical applications in schools and ethical implications.”\nThis type of instruction allows the model to generate a map organised according to logical criteria, reflecting systemic thinking patterns.\nFurthermore, the flexibility of natural language allows elements typical of concept maps, such as hierarchies and relationships, to be combined with elements typical of academic outlines (argumentative sequences, levels of detail, examples).\nThe request for textual mapping can be repeated iteratively, prompting the model to refine the structure, add nodes, propose counter-arguments or make explicit any interconnections between concepts.\nIn this way, the map is not a static product, but an evolving artefact, generated through a cyclical interaction between human and machine. Furthermore, automatic text mapping serves a dual function: on the one hand, it allows for rapid exploration of the possible articulations of a concept; on the other, it promotes the consolidation of knowledge through the reformulation and systematisation of the generated content.\n\n\n8.2.3 2.3 Iteration and dialogue with the model: refinement of patterns\nThe construction of multi-level argumentative schemes through the use of GAI is not limited to a single automatic generation, but is configured – as is often the case with other requests – as an iterative and dialogical process.\nThe user does not simply passively receive the generated map, but can (and must) actively interact with the model, prompting it to rework, deepen, clarify or restructure the content produced.\nThis interaction develops through a sequence of successive prompts that progressively improve the quality of the initial request.\nRequests can usually be:\n• Expansion: the user can ask the model to elaborate on a specific sub-node:\n✏️ “Elaborate on the “digital tools” node within the multi-level text map on formative assessment. Include concrete examples, emerging methodologies, and comparative application contexts.”\n• Clarification: the user can detect conceptual ambiguities or generic formulations, asking the model to rephrase or specify:\n✏️ “Rephrase the explanation regarding the distinction between “teaching strategies” and “assessment methods”, improving terminological precision and internal consistency.”\n• Reorganisation: if the structure is weak from a logical or hierarchical point of view, it is possible to request a new articulation of the nodes:\n✏️ “Restructure the hierarchical map by assigning three main conceptual axes: theoretical, operational and critical, and redistribute the second-level nodes according to this logic.”\n• Comparison: to enrich the map, the user can prompt the model to integrate alternative points of view or competing approaches:\n✏️ “Integrate sections dedicated to critical perspectives, emerging alternative approaches and hybrid models with respect to dominant digital skills into the map on digital citizenship.”\nThrough these successive interactions, the textual map becomes progressively more robust, coherent and articulated, transforming itself from a simple thematic list into a true argumentative structure.\n\n8.2.3.1 Iterative prompt engineering: epistemic cycle\nThe coordinated sequence of prompts aimed at expansion, clarification, reorganisation and comparative analysis transforms interaction with AI into an advanced epistemic process.\nIt is a model in which the user does not simply receive output, but iteratively designs, tests and refines content according to criteria of academic rigour.\nThe cyclical flow\n\nThematic expansion\nThe user asks the model to detail specific conceptual nodes, enriching them with concrete examples, comparative contexts or disciplinary references.\nSemantic clarification\nPotential ambiguities, reformulations or terminologically weak concepts are addressed, increasing precision and internal consistency.\nLogical-structural reorganisation\nIf hierarchical weaknesses emerge, a conceptual redistribution consistent with new thematic axes or narrative levels is required.\nComparison and plurality of perspectives\nThe model is invited to integrate alternative perspectives, critical approaches or emerging voices, enriching the map with plurality of arguments.\n\n\n\n\n\n\n\nNoteThis prompt-driven mode allows an initial draft to be transformed into a robust argumentative structure, which progressively evolves in terms of consistency, completeness and depth.\n\n\n\n\n\n\n\n\n\n8.2.4 2.4 Argumentative structures: causes, effects, implications, limitations\nOne of the most significant uses of GAI in the construction of multi-level textual schemas is the possibility of developing actual argumentative structures, organised according to logical and relational categories.\nIn this case, the textual map is not limited to representing a conceptual taxonomy, but takes the form of an articulated discursive organisation, useful for supporting a thesis, exploring a problem or structuring a critical analysis.\nIt is possible to orient the model from the outset towards a defined argumentative grid, for example:\n\nCauses: structural factors that generate the phenomenon under study\n\nEffects: observed or predicted consequences\n\nImplications: ethical, social, political, pedagogical or technological repercussions\n\nLimitations: critical issues, constraints, potential grey areas.\n\n\n\n\n\n\n\nNoteThis structure is particularly useful in the design of academic texts, scientific articles, research plans, educational interventions or strategic documents.\n\n\n\n\n\n\nThe user can specify the desired argumentative framework from the outset, e.g. in a prompt such as this:\n✏️ “As an SSH researcher, generate a multi-level textual map on a topic such as “Environmental education and urban justice”. Articulate the map according to structural causes, educational effects, social implications and limitations. Respond in Markdown with a heavily indented list and key points. Include references to critical paradigms (e.g. eco-pedagogy, postcolonialism).”\n\n\n\n\n\n\nNoteThe model will return a coherent representation, capable of supporting complex reasoning and serving as a basis for written or oral elaboration.\n\n\n\n\n\n\nThis mode promotes the development of critical thinking and awareness of the problematic dimensions of the phenomenon analysed.\nThis approach generates a coherent and solid map, capable of supporting complex discourse, that becomes both an argumentative support and a basis for written constructions or oral presentations.\nMulti-level mapping is inspired by established practices in Social Science and Humanities (SSH) research, reflecting formal approaches to academic writing (APA, MHRA, Turabian) and Donald Schön’s models of reflective practice.\n\nSee Schön, D. A. (2017). The reflective practitioner: How professionals think in action\n\n\n\n\n\n\n\nTipThe critical treatment of logical structures through the textual map activates a deep metacognitive process.\n\n\n\n\n\n\nFor Researchers\nThe multi-level textual map allows you to systematically explain complex relationships between causes, effects, implications and future prospects.\nThis structure is particularly useful for:\n- systematically explaining complex relationships between causes, effects and implications\n- designing chapters of theses and academic articles\n- building funding proposals (grants)\n- organising complex theoretical and methodological structures.\nFor Metacognitive Activities (Students and Doctoral Candidates)\nIncorporating argumentative maps into training exercises promotes:\n- the critical reformulation of existing maps\n- the production of interdisciplinary counter-maps\n- the comparison of competing theoretical positions.\n👉🏻 These activities promote genuine reflection on the process of thinking and planning, which is essential in advanced SSH courses.",
    "crumbs": [
      "Part 2: Advanced applications of AI in Research and Creativity",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>AI as a Creative Partner</span>"
    ]
  },
  {
    "objectID": "2-20250820-AIcreativepartner.html#self-feedback-on-generated-content",
    "href": "2-20250820-AIcreativepartner.html#self-feedback-on-generated-content",
    "title": "8  AI as a Creative Partner",
    "section": "8.3 3. Self-feedback on generated content",
    "text": "8.3 3. Self-feedback on generated content\nOne of the most promising developments in GAI is the possibility of using language models not only for content production, but also for internal and iterative critical evaluation.\nSelf-feedback, also known as self-evaluation, is a way of using LLMs that goes beyond mere text generation, involving them in the systematic critique of their own output.\nGiven clearly defined inputs, the model performs a self-referential evaluation of previous outputs and suggests improvements.\n\n\n\n\n\n\nNoteIn computational terms, LLMs do not possess metacognitive awareness, but they can simulate evaluative processes through the generation of texts that take the form of critical commentary, review or reasoned comparison.\n\n\n\n\n\n\nSelf-feedback activates a reflective chain: the model analyses its own output against specific criteria (e.g. consistency, clarity, originality), generates an internal review, and proposes an improved version.\nThis cycle of judging–reformulating–judging transforms initial content into more robust outputs, without the need for fine-tuning. This type of output, if well trained, can constitute a first level of analysis useful for improving the communicative effectiveness, internal consistency, and originality of the materials generated.\nFrom an educational and scientific point of view, self-feedback represents a significant innovation: it introduces an evaluative instance within the generative interaction itself, enabling a virtuous cycle of production-reflection-reformulation.\n\n8.3.1 3.1 Main techniques and evidence\n a. Self Refine (Iterative Refinement with Self Feedback)\nThe Self Refine technique involves a LLM generating an initial output, critically evaluating it against defined criteria, and then reformulating it in an iterative cycle: generate → feedback → refine.\nAll this is done with the same model, without the need for supervised data or a fine-tuning phase.\nEach iteration keeps track of the output + feedback history, which the model draws on to avoid repetition and constantly improve.\n\n\n\n\n\n\nTipExperimental evaluations on GPT 3.5 and GPT 4 show an average improvement of about 20% over one-step generation on over seven types of tasks (reasoning, responses, rewriting) See article\n\n\n\n\n\n\n✏️ Prompt example:\n“Review the following paragraph according to academic criteria of terminological accuracy and argumentative rigour. Propose a reworded sentence that is clearer and more coherent.”\nb. Self-Critique Guided Reasoning\nIn this strategy, the model guides its own reasoning with chain of thought steps followed by critical self-evaluation.\nThis promotes conceptual consistency, logical precision, and the reduction of significant omissions.\nIt is useful in complex contexts where internal consistency is crucial. See article\n✏️ Prompt example:\n“You have just generated a text/project. Now analyse it critically. Indicate at least three points of possible improvement in terms of logical consistency and original content.”\nc. Auto-evaluation (LLM-as-a-Judge)\nHere, the model takes on the role of an impartial authority: it evaluates output according to explicit disciplinary criteria (logical consistency, relevance, accuracy, originality, clarity), assigning a score and generating structured feedback.\n\n\n\n\n\n\nTipStudies applied in customer service scenarios show that the LLM-as-a-Judge approach improves the quality of responses by over 8%, with a correlation 3–5 times higher than standard prompts. See Article 1 and Article 2\n\n\n\n\n\n\n✏️ Prompt example:\nThis prompt can be used to monitor and standardise the reliability of automatic interaction in SSH writing contexts.\n“You are an expert academic evaluator. Your task is to judge the quality of the following output produced by an AI model in response to a research question. Base your assessment on the following criteria: logical consistency, relevance to the question, accuracy of data, originality of reasoning, and clarity of presentation. Provide a score from 1 to 5 (where 1 = very poor, 5 = excellent) followed by a bullet-point summary of strengths and weaknesses.. Research question:\n{input}\nModel output:\n{output}\nRespond with:\n*Score: __*\nBullet-point feedback:\n• …“*\nd. Self Restraint (ReSearch)\nThe ReSearch technique, or self-restraint, teaches the model to independently detect its own uncertainty. Through a utility function and an iterative self-prompting process, the model learns to refrain from acting when it perceives low confidence.\n👉🏻 This significantly reduces hallucinations on complex topics, without additional inference costs.\nSee article\n✏️ Prompt example:\nThis prompt activates the ReSearch strategy: the model assesses its own uncertainty and, if necessary, refrains from making predictions or signals caution. In literature, this approach has been shown to significantly reduce hallucinations without additional inference costs. See article\n“You have just generated a section of theoretical analysis on a complex interdisciplinary topic. Now critically analyse the text and reflect on your level of confidence: if there are any uncertain or potentially incorrect parts, abstract them (‘I am not sure’; ‘I will not answer’). Report:\n1. Areas where your knowledge is limited or uncertain.\n2. Parts that may be ambiguous or incorrect.\n3. Decision whether to continue or withdraw.\nGenerated text:\n{text}\nRespond with a bulleted list:\n- Section …\n- Confidence: high/medium/low\n- Notes on any errors or missing data”\n\n\n8.3.2 3.2 Advantages in the SSH context\n a. Strengthening internal consistency and originality\nPrompts geared towards internal verification of argumentative sequences help identify logical gaps, hidden contradictions and misalignments between objectives and expected results.\nOne of the most important aspects in evaluating a text, whether it is a project, theoretical or argumentative, concerns its internal consistency.\nThis term refers to the logical adherence between the parts of the text, the absence of obvious contradictions, the linearity of reasoning and the congruence between stated objectives, methods used and expected results.\n👉🏻 Through specific prompts, it is possible to ask the AI to simulate an internal review of the text itself.\n✏️ Prompt examples:\n“Analyse the following educational project and report any inconsistencies between objectives, methodology and assessment tools.”\n“Check whether the following paragraphs are consistent with each other from a logical and semantic point of view.”\nThe model is able to detect structural misalignments, significant omissions, redundant or ambiguous passages. Although it cannot guarantee an infallible assessment, its pattern recognition capability allows it to quickly identify the main formal critical issues, providing a starting point for manual review.\nIn addition to consistency, a second fundamental evaluation axis concerns the degree of originality of a proposal or content.\n\n\n\n\n\n\nNote\n\n\n\nAlthough AI does not have direct access to up-to-date bibliographic databases or an autonomous critical sense, it can be trained to compare a proposal with common approaches, highlighting similarities, repetitions or possible innovations*.\n\n\n✏️ Prompt examples:\n“Assess how much the following project deviates from traditional teaching models and indicate any innovative elements.”\n“Compare the proposed strategy with established educational practices and point out how it differs.”\n\n\n\n\n\n\nNote\n\n\n\nIn response, the model can produce a structured analysis, highlighting elements of originality (unprecedented combinations, emerging applications, new contexts) or, conversely, pointing out excessive similarity with pre-existing models.\n\n\nb. Encouraging epistemic reflection\nDesigning cycles in which AI suggests revisions involves reflecting on one’s own critical thinking: this is essential in SSH contexts in order to develop robust methodological awareness.\n✏️ Prompt Example 1: Critically review the output\nThis approach allows the model to assemble a true metacognitive cycle: it identifies gaps, implicit assumptions, reasons at the epistemic level, and then proposes changes.\n“You have just generated a paragraph (or section) on a research topic. Reread it critically and answer these questions:\n- What implicit assumptions appear and need to be argued or verified?\n- Which logical steps were unclear or ambiguous?\n- What alternative perspectives or theoretical approaches are missing?\n- Suggest at least two methodological revisions or additions to make the text more epistemically robust.”\n✏️ Prompt Example 2: Structured reflection for disciplinary and comparative purposes\nThis framework is inspired by frameworks such as Reflexion, which structure verbalised feedback to reinforce critical skills in LLM models, including in the epistemic-social context.\n“Act as a reflective expert in the field of SSH. Analyse the text provided in the following points:\n- What underlying theoretical paradigms emerge (explicitly or implicitly)?\n- Are there any disciplinary biases or implicit ideological orientations? Where?\n- How consistent is the argumentative position with known theoretical sources?\n- Indicate two passages where an interdisciplinary or critical approach could be useful and suggest additions.”\n\n\n\n\n\n\nTipWhy use these prompts?\n\n\n\nBoth templates:\n- guide the model to take an epistemic stance, not just a technical-informative one;\n- promote self-assessment that induces critical awareness of the generated text;\n- encourage interdisciplinary integration and the detection of implicit biases.\n👉🏻 You can use them in sequence (first A, then B) for a cycle of progressive reflection: from logical gaps to disciplinary biases to structured critical intervention.\n\n\nc. Scalability and timeliness of feedback\nIn collaborative learning or writing processes, AI can provide personalised feedback in real time, overcoming limitations associated with limited human resources See article\n✏️ Prompt Example 1: Real-time feedback on text drafts.\nIdeal for collaborative writing cycles: provides immediate feedback on paragraphs or drafts, facilitating quick and effective revisions.\n“Access the following text immediately and provide structured feedback on: - strengths\n- areas for improvement\n- practical suggestions for revision\n- any methodological or theoretical gaps\nLimit your response to approximately 300 words.\nText to be evaluated: {text}”\n✏️ Prompt Example 2: Rapid assessment with disciplinary criteria\nThis approach is ideal for collaborative writing cycles in academia: it allows for immediate feedback on drafts or paragraphs, facilitating rapid and detailed revisions.\nIt is particularly useful for thesis sections, abstracts or funding proposals in SSH disciplines, as it offers accurate and replicable assessments with scientifically rigorous criteria.\n“Analyse this paragraph/draft relating to [abstract/grant title/thesis section title] according to the following criteria:\n- Consistency between objectives, content and conclusions.\n- Clarity of expression according to disciplinary vocabulary.\n- Originality compared to common writing in the field.\n- Conceptual relevance to the specified SSH context.\n- Respond in bullet points, indicating practical suggestions for a quick revision”\n✏️ Prompt Example 3: Automated incremental improvement cycle\nGreat for SSH teaching based on iterative writing: immediate alternation between feedback and rewriting to refine content.\n“For the text provided:\n- Indicate 3 structural or conceptual issues\n- Suggest 3 specific rewrites to resolve them\n- Release a revised version (max 250 words)\nFormat:\n- Pointed feedback\n- Revision proposals\n- New summary version\nText to be revised: {text}”\nd. Critical support for reformulation\n‘Self-Refine’ or ‘Self-Critique’ prompts help to reorganise texts while keeping their content intact, but improving their clarity and argumentative rigour.\n✏️ Prompt Example 1 – Self-Refine (Iterative Refinement with Self Feedback)\n\n\n\n\n\n\nNote\n\n\n\nStudies indicate that this mode increases output quality by about 20%, without any external fine-tuning.\n\n\n“Examine the following paragraph and report any critical issues from an argumentative, structural or stylistic point of view. Then rewrite it incorporating the revisions:\n- Critical feedback\n- Reformulated, clearer and more coherent version\nOriginal paragraph:\n{text}\n✏️ Prompt Example 2 – Self Critique Guided Reasoning\nThis mode reinforces conceptual consistency and reduces logical errors through guided self-criticism, in line with recognised practices in advanced prompting.\n“You have generated the following text on a complex academic topic.\nNow: i)Indicate at least three possible weaknesses or ambiguities (e.g., unclear reasoning, implicit assumptions, risky logical leaps). ii)Suggest concrete revisions for each. iii) Finally, rephrase the paragraph incorporating the suggested improvements. Text:\n{text}”\n\n\n\n\n\n\nTipBoth strategies can be used sequentially or independently, if the goal is to review draft chapters, abstracts, articles, or research papers.\n\n\n\n\n\n\n\n\n8.3.3 3.3 Educational and Academic applications\nIn the field of advanced educational and academic applications, the strategic integration of GAI effectively supports both teachers and doctoral students in the SSH. Thanks to the use of asymmetric prompts between production and internal evaluation, it is possible to design multi-level argumentative maps, pedagogical analysis structures and complex thesis chapters, benefiting from automatic feedback support that can assist the human author with methodological rigour and epistemic continuity.\nSimilarly, the paradigm of automated self-evaluation proves to be fundamental during learning and educational research: students and doctoral candidates can submit individual sections, drafts or papers to the AI.\nThe latter, guided by defined evaluation rubrics, identifies critical issues, proposes structured revisions and transforms the simple production of text into an active metacognitive experience, reinforcing critical reflection and epistemic awareness.\nFurthermore, the synergistic combination of prompts for production and prompts dedicated to feedback generates an iterative flow of qualitative improvement in outputs.\nIn this configuration, academic writing is no longer a solitary act, but a continuous dialogue in which AI takes on the role of a semi-autonomous interlocutor: it generates, evaluates, reformulates and enriches content in close dialogue with the user. This mechanism is similar to robust research methodologies, where scientific argumentation is constructed through successive approximations, revisions, and structural refinements.\n\n\n\n\n\n\nTipThe integration of automatic generation prompts and feedback allows for an increase in the epistemic quality of written and design production in the SSH field.\n\n\n\nThe systematic application of this approach supports:\n- reflective design (argumentative maps, articles, thesis chapters) with automatic validation\n- a reflective and formative teaching practice, in which students become autonomous in reviewing their own work\n- the strengthening of coherence, originality and rigour, thanks to explicit cognitive cycles of generation, evaluation and progressive reformulation.\nThis model of interaction with AI respects and values the role of the human researcher, acting as a true epistemic partner in the construction of knowledge.",
    "crumbs": [
      "Part 2: Advanced applications of AI in Research and Creativity",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>AI as a Creative Partner</span>"
    ]
  },
  {
    "objectID": "2-20250820-AIcreativepartner.html#further-reading",
    "href": "2-20250820-AIcreativepartner.html#further-reading",
    "title": "8  AI as a Creative Partner",
    "section": "8.4 Further reading",
    "text": "8.4 Further reading\n\n“In educational settings, self-generated feedback from LLM has been shown to improve student writing performance and motivation compared to static feedback”.\nSee Article\n“A study has shown that LLM-driven feedback, when guided by well-designed prompts, outperforms that of beginners and, in some categories, even that of experts in the context of teacher training education, offering faster and more specific feedback”\nSee Article\n“The self-contrast paradigm has shown positive effects in comparing divergent solutions generated by the same model, improving stability and accuracy”\nSee Article\n“An analysis of over 2,600 users in tutor/student lessons shows that LLM-on-demand explanatory feedback improves performance with significant standardised effects (0.28–0.33), with equivalent response times and high user satisfaction”\nSee Article\n“A systematic review highlights how automated GenAI feedback systems reduce the teacher’s workload, enabling effective cognitive and emotional interventions, and improve educational interaction with students in large-scale contexts”\nSee Article",
    "crumbs": [
      "Part 2: Advanced applications of AI in Research and Creativity",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>AI as a Creative Partner</span>"
    ]
  },
  {
    "objectID": "2-20250825-AIprogettazione.html",
    "href": "2-20250825-AIprogettazione.html",
    "title": "9  GAI as a Design support tool",
    "section": "",
    "text": "9.1 Introduction\nIn recent years, GAI has established itself in the world of research, in a position not only as an operational tool, but also as a support for project planning.\nIts use goes far beyond automatic writing or text synthesis: it is at the heart of the Academic process, contributing to the most delicate and creative phase of “planning”.\nIn the Social Sciences and Humanities (SSH), where knowledge construction involves language, interpretation and interdisciplinarity, AI does not offer definitive answers, but it can:\n- suggest research directions that are not immediately visible\n- help clarify nuanced or theoretical concepts\n- offer stylistic and structural variations\n- stimulate new associations between authors, currents or approaches.\nIn these contexts, the value of AI moves away from technicalities and becomes cognitive and dialogical, acting as a generative interlocutor that relaunches, expands and compares.\nUnlike STEM fields, where AI is often used to optimise quantitative processes, analyse structured data or automate complex calculations, in SSH its value is mainly evident in the linguistic, exploratory and conceptual dimensions.\nIn these contexts, AI is not so much a tool for “solving” as it is for “stimulating”: it helps to generate ideas, formulate new questions, refine theoretical vocabulary and compare methodological approaches.\nIn the SSH, AI does not simplify a problem, but enriches the thought process surrounding it, becoming a tool for amplifying the researcher’s abilities, supporting the exploratory, generative and compositional steps typical of Academic work in this area.\n👉🏻 A concrete example is the use of structured prompts, which allow for guided interaction with the linguistic model. Thanks to these, it is possible to obtain proposals for research questions, formulations of hypotheses, methodological alternatives, and even simulations of different argumentative styles.",
    "crumbs": [
      "Part 2: Advanced applications of AI in Research and Creativity",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>GAI as a Design support tool</span>"
    ]
  },
  {
    "objectID": "2-20250825-AIprogettazione.html#introduction",
    "href": "2-20250825-AIprogettazione.html#introduction",
    "title": "9  GAI as a Design support tool",
    "section": "",
    "text": "NoteThe output is never definitive, but must always be critically analysed, adapted, and refined.\n\n\n\nGAI thus becomes a silent co-designer, accompanying the researcher in the initial phase of research, supporting thought without ever replacing it. When used consciously, it can enhance creativity, accelerate the production of ideas* and promote greater stylistic and methodological awareness.",
    "crumbs": [
      "Part 2: Advanced applications of AI in Research and Creativity",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>GAI as a Design support tool</span>"
    ]
  },
  {
    "objectID": "2-20250825-AIprogettazione.html#gai-in-the-construction-of-the-research-project-1",
    "href": "2-20250825-AIprogettazione.html#gai-in-the-construction-of-the-research-project-1",
    "title": "9  GAI as a Design support tool",
    "section": "9.2 1. GAI in the construction of the Research Project",
    "text": "9.2 1. GAI in the construction of the Research Project\nResearch design is one of the most delicate and significant aspects of Academic activity, particularly in the SSH disciplines, where the construction of the object of study, theoretical elaboration and the definition of research methodologies take on a highly reflective, linguistic and interpretative dimension.\nGAI can provide valuable support in the initial phase of Research Design, particularly in this Academic context.\nUsing structured prompts GAi allows you to:\na. Explore and refine research questions, stimulating alternative and more precise formulations\nb. Construct coherent and verifiable hypotheses, consistent with the chosen theoretical and methodological approach\nc. Outline a preliminary methodological framework, suggesting research designs, tools and techniques appropriate to the context.\nThe effectiveness of the interaction between researcher and GAI, depends largely on the quality of the dialogue established through the prompts.\nThis process becomes iterative, critical and assisted, where each output produced by the GAI is subjected to careful evaluation, reinterpretation and, if necessary, reformulation.\n\n\n\n\n\n\nNoteIn this context, GAI never assumes the role of a substitute for scientific authorship, but rather that of an “auxiliary tool” for reflection and project development.\n\n\n\n\n\n\nThe use of GAI in the Research D**esign phase should not be seen as a shortcut, but rather as an opportunity to strengthen the logical structure and internal consistency of the project.\nWhile leaving the researcher with full theoretical and methodological responsibility*, AI is able to:\n- facilitate the exploration of alternatives\n- prompt new hypotheses\n- identify conceptual inconsistencies or underdeveloped areas\n- support the reformulation process in a dynamic and productive way.\nIn order for this potential to be realised in a meaningful way, it is necessary to adopt an accurate and conscious questioning strategy.\nIn this sense, the development of structured prompts is an essential practice for guiding linguistic generation towards relevant, contextually grounded and scientifically useful answers.\n\n9.2.1 1.1 The research question as a fundamental node\nThe research question represents the generative core of every scientific project: it is not only an operational starting point, but a theoretical and conceptual act that profoundly conditions the entire research framework.\nIn the field of SSH, its formulation is not limited to identifying a theme or describing a phenomenon, but involves an epistemological choice, i.e. an implicit (or explicit) determination of the criteria by which it is defined what is knowable, how and with what interpretative tools.\nFormulating a research question therefore means positioning oneself in relation to a disciplinary field, recognising a gap in the literature or an open question, and proposing a possible direction for exploring it.\nThis question then leads to:\n- the selection of sources\n- the construction of the theoretical framework\n- the choice of method\n- the form of scientific language used.\n\n\n\n\n\n\nImportantIn this process, GAI can play a stimulating and supporting role, without replacing the researcher’s epistemic responsibility.\n\n\n\n\n\n\nWhen queried with appropriate prompts, GAI is able to:\n- propose alternative formulations to an initial intuition\n- help to more precisely define an overly broad scope of investigation\n- generate new angles from which to observe a familiar problem.\n\n\n\n\n\n\nImportantThese contributions should not be taken as definitive solutions, but as material for dialogue, to be critically evaluated within the individual elaboration process.\n\n\n\n\n\n\n\n\n\n\n\n\nTipIn the Academic context, and in particular in doctoral training, working on the quality of the research question is equivalent to strengthening the logical and conceptual foundations of the project.\n\n\n\nInteraction with AI at this stage can take the form of guided co-construction, in which AI becomes an interlocutor in the exploratory phase, stimulating clarifications, precisions, and variations that enrich the depth and relevance of the question.\n\n\n\n\n9.2.2 1.2 From question to hypothesis: exploring relationships\nOnce the research question has been formulated in a clear, outlined and theoretically grounded manner, the next step in the design process involves constructing the hypothesis, which is not merely a technical step, but a conceptually strategic one.\nThe hypothesis represents a first provisional and reasoned answer to the question posed, which the researcher intends to test using an empirical or theoretical approach.\nIn the field of SSH, hypotheses do not necessarily follow the rigid structure of the hypothetical-deductive model, but can take more flexible forms, such as an argumentative conjecture, an exploratory hypothesis or an interpretative construct, depending on the epistemological and methodological framework of reference.\nIn all cases, however, it performs an orientative function, as it helps to define what is observed, how it is observed and under what conditions a given relationship can be considered significant.\nGAI can be a useful support in this phase, especially if it starts from a well-formulated research question.\nThanks to structured prompts, AI is able to suggest:\n\nhypotheses that are logically compatible with the question posed\n\ndistinguish between causal, correlative or descriptive relationships\n\nhelp to clarify implicit assumptions that the researcher might overlook\n\npropose alternative formulations that highlight latent variables, contextual influences or mediating factors not immediately considered.\n\n\n\n\n\n\n\nImportantIt is essential, however, that such hypotheses are not accepted uncritically, but are subjected to theoretical and methodological scrutiny by the researcher.\n\n\n\n\n\n\n👉🏻 The role of GAI should be understood as a cognitive stimulus and a tool for comparison, not as a substitute for the inferential process.\nFurthermore, the plurality of hypotheses obtainable through AI can itself become a resource for exploration: comparing alternative hypotheses allows you to test the logical soundness and internal consistency of your project.\n\n\n9.2.3 1.3 Methodology development: consistency and practicality\nThe definition of the methodology constitutes the operational translation of the theoretical assumptions and hypotheses formulated.\n👉🏻 It involves the conscious and reasoned choice of a set of strategies, techniques and tools that make the investigation possible, while ensuring its internal consistency, epistemic relevance and practicality on an empirical level.\n\n\n\n\n\n\nWarningMethodology is not a technical module to be applied retrospectively, but a logical and consistent extension of the Conceptual Design of the Research.\n\n\n\n\n\n\nIn the SSH field, methodological construction often deals with complex, contextualised objects of study that are subject to multiple interpretations.\nTherefore, the selection of qualitative, quantitative or mixed approaches, the identification of the sample, the timing of the investigation, and the tools for data collection and analysis must be discussed and justified in relation to the theoretical framework adopted and the nature of the phenomenon under investigation.\n\n\n\n\n\n\nTipThe methodology does not therefore respond to criteria of mechanical standardisation, but rather to criteria of epistemological relevance, logical rigour and operational sustainability.\n\n\n\n\n\n\nWhen queried with specific and detailed prompts, GAI can provide methodological design hypotheses consistent with the hypotheses formulated, suggesting sampling strategies, data collection techniques (interviews, questionnaires, document analysis, observation), validation tools and possible analytical approaches.\nGenAI also allows you to:\n- simulate different configurations of the same survey\n- anticipate implementation issues\n- assess the impact of certain methodological choices on the expected results.\n👉🏻 Through iterative and critical use of prompting, the researcher can progressively refine the methodological design, validating its consistency with the initial question and the hypotheses to be tested.\n\n\n\n\n\n\nWarningHowever, the methodology cannot ignore the relationship between means and ends, between tools and objects, between theoretical intention and empirical feasibility.\n\n\n\nIn this sense, collaboration with a generative system can become an opportunity to question one’s own implicit assumptions, strengthen the argumentative transparency of the choices made, and increase the epistemological awareness of the design as a whole.",
    "crumbs": [
      "Part 2: Advanced applications of AI in Research and Creativity",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>GAI as a Design support tool</span>"
    ]
  },
  {
    "objectID": "2-20250825-AIprogettazione.html#structuring-calls-and-grants-with-the-support-of-ai-1",
    "href": "2-20250825-AIprogettazione.html#structuring-calls-and-grants-with-the-support-of-ai-1",
    "title": "9  GAI as a Design support tool",
    "section": "9.3 2. Structuring calls and grants with the support of AI",
    "text": "9.3 2. Structuring calls and grants with the support of AI\nIn contemporary research, the ability to draft competitive project proposals is a crucial skill for accessing funding, participating in international networks and consolidating one’s academic profile.\nCalls, grants and research tenders require not only high scientific quality in terms of “content”, but also formal mastery of project writing: effective summarisation, clarity of presentation, logical consistency and stylistic adaptation to the expectations of the funding body.\nIn this context, GAI can play an important role as a linguistic assistant, rhetorical simulator and generator of textual variants.\nStarting from concrete examples or structured guidelines, AI can support researchers in drafting abstracts, executive summaries, impact descriptions and other elements typical of Academic applications.\nThere are prompting techniques (including those based on the few-shot paradigm) that can be used strategically to generate preliminary or alternative versions of project texts.\nThe issue of tone-matching is important, i.e. adapting the communicative register to the type of call for proposals, the evaluating audience and the organisational culture of the funding body.\n\n9.3.1 2.1 Few-shot approach: learning from examples to generate design variants.\nIn the context of Academic writing and designing for competitive calls for proposals, the production of short and highly strategic texts – such as abstracts or executive summaries – is a crucial skill.\nThese texts require not only mastery of the content, but also the ability to condense, highlight and make the originality and impact of the proposal immediately readable.\nGAI can offer targeted support at this stage through the use of a technique known as few-shot prompting.\nThe few-shot paradigm See Article 1 and Article 2 is based on presenting the language model with one or more well-formulated examples (abstracts or extracts from previously accepted project proposals), followed by a prompt asking the AI to generate a new version on a different content, while maintaining a consistent style, structure and tone.\nUnlike the zero-shot approach, in which the model is required to produce a text from scratch without explicit references, the few-shot mode provides a sample text model on which the AI can calibrate its generation.\nThis approach is particularly effective in supporting researchers who are less familiar with drafting international proposals or calls for proposals, where the form of the text is as important as the content.\nThrough guided imitation, GAI is able to replicate not only the discursive structure of the abstract (opening-problem-objective-method-expected results), but also the linguistic intonation required by specific programmes (e.g. Horizon Europe, Marie Skłodowska-Curie, ERC Starting Grant).\n\n\n\n\n\n\nTipIt is essential to emphasise that the quality and effectiveness of the output depend largely on the quality and relevance of the examples provided.\n\n\n\nThe starting abstracts must be carefully selected, preferably relating to the same disciplinary field and compatible in tone and purpose with the new project to be developed.\n\n\nGAI should therefore be understood as a real tool for authorial relaunch, capable of proposing variations that the researcher will have to critically analyse, adapt and refine.\nFew-shot prompting can also be understood as a training practice.\nComparative analysis between the example provided and the variant generated by GAI can stimulate reflection on the rhetorical structure of the draft text, effective lexical choices, and argumentative coherence.\n👉🏻 In this sense, the linguistic model functions as an imitative interlocutor, useful not only for writing but also for learning to write better.\n\n\n9.3.2 2.2 Tone-matching: adapting style and register to the call for proposals or the funding body\nOne of the most underestimated aspects of project writing is the need to adapt tone, register and communicative rhetoric to the implicit expectations of the call for proposals and the identity of the funding body.\nIn addition to the quality of the “content”, what often affects the outcome of the evaluation is the applicant’s ability to express their project in a linguistic form that is consistent with the language, values and organisational culture of the institutional interlocutor.\n\n\n\n\n\n\nImportantGAI becomes an important resource thanks to the conscious use of prompts geared towards so-called tone-matching, i.e. the ability to modulate the communicative style of the text to make it relevant to the specific context.\n\n\n\n\n\n\nUnlike simple formal rephrasing, tone-matching implies a deep understanding of the target audience: an abstract addressed to a private Foundation with a Humanistic mission will require a different register than one intended for a European Agency focused on technological innovation.\nThrough specific inputs, GAI can be guided to rewrite an existing text by varying its tone, emphasis, technical density and argumentative structure.\n\n\n\n\n\n\nImportantFor example:\n\n\n\nAI can be asked to transform a highly specialised text into a more accessible and informative version for a call for proposals with social purposes, or to increase the formality and terminological precision in view of a presentation to an international scientific committee.\n\n\nTone-matching is also particularly useful in the final stages of writing, when the content is defined but needs to be refined according to criteria of rhetorical effectiveness.\nIn this process, GAI acts as a simulated editor, able to propose stylistic alternatives, reduce ambiguity, and harmonise the register between the various sections of a project document.\n\n\n\n\n\n\nWarningHowever, the effectiveness of tone-matching depends on the researcher’s ability to provide clear guidance on the type of audience, the identity of the institution and the objectives of the call.\n\n\n\nAlso in this case, interaction with AI must be understood as an iterative dialogue: it is the quality of the prompt, combined with the critical awareness of the writer, that determines the relevance of the output.\n\n\nFinally, tone-matching is not only about the form of the text, but also its ability to communicate belonging to a scientific community, sensitivity to the evaluation criteria adopted, and adherence to the strategic objectives of the funding body.\n\n\n\n\n\n\nNoteIn this respect, AI can promote impact-oriented stylistic refinement, helping to bridge the gap between scientific content and its persuasive presentation.",
    "crumbs": [
      "Part 2: Advanced applications of AI in Research and Creativity",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>GAI as a Design support tool</span>"
    ]
  },
  {
    "objectID": "2-20250830_GrantAutom.html",
    "href": "2-20250830_GrantAutom.html",
    "title": "10  Writing calls, grants, abstracts, and intelligent automations",
    "section": "",
    "text": "10.1 Introduction\nIn the context of growing pressure for continuous and measurable scientific productivity, accompanied by the proliferation of competitive calls for proposals and increasingly tight deadlines, the strategic integration of AI-based tools represents a concrete opportunity to streamline and enhance editorial practices in academic research.\nIn this scenario, where the needs for clarity of expression, thematic relevance and persuasiveness converge, AI can act as a lever to combine operational efficiency and argumentative quality.\nThe conscious use of intelligent systems in Academic writing flows – from design to revision – makes it possible to lighten the executive load associated with the most repetitive phases of intellectual work, allowing researchers to focus their attention on activities with high cognitive value, such as critical analysis, argumentation and theoretical construction.\n👉🏻 This results in greater reflectiveness in decision-making processes and better communication in the texts produced.\nIn particular, the writing of calls for papers (project proposals for competitive grants and scientific abstracts) can be significantly facilitated by intelligent automation, capable of supporting the entire text production cycle from title generation to logical structuring, from stylisation to quality control.\nFive priority areas can be identified in which GAI finds effective application in the Social Sciences and Humanities (SSH), promoting not only the acceleration of processes, but also their conceptual and epistemic refinement:",
    "crumbs": [
      "Part 2: Advanced applications of AI in Research and Creativity",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Writing calls, grants, abstracts, and intelligent automations</span>"
    ]
  },
  {
    "objectID": "2-20250830_GrantAutom.html#introduction",
    "href": "2-20250830_GrantAutom.html#introduction",
    "title": "10  Writing calls, grants, abstracts, and intelligent automations",
    "section": "10.2 1. Prompt chaining for titles and abstracts",
    "text": "Prompt chaining for titles and abstracts\n\nScripts/APIs for bibliographic metadata and automatic summaries\nProactive workflows and automated reminders\n\nRapid text quality metrics\n\nCognitive offloading to optimise mental resource management\n\n\n\n\n\nScheme of text production verification areas\n\n\n\n\n\n\n10.2 1. Prompt chaining for titles and abstracts\nIn the process of generating titles and abstracts, one of the most effective applications of GAI is prompt chaining, i.e. the structured concatenation of text commands.\nThrough the use of progressive sequences of controlled prompts, the researcher can guide the linguistic model through distinct but interconnected phases, which simulate the iterative reasoning typical of academic writing.\n\nWe start with a simple thematic statement to generate a list of titles consistent with the disciplinary domain and style required by the editorial context.\nNext, the selected titles are developed into draft abstracts, structured according to the specific rhetorical conventions of the field (objectives, method, expected results, implications).\nFinally, again using targeted prompts, the text is stylistically revised and linguistically optimised to improve readability, impact and adherence to formal requirements.\n\n\n\n\n\n\n\nTipFor example, a typical flow might look like this:\n\n\n\n\nPrompt 1: “Generate 5 Academic titles for a project on the relationship between AI and political discourse analysis.”\n\nPrompt 2: “Write a draft abstract for title X, using a formal style and no more than 150 words.”\n\nPrompt 3: “Reformulate the text with a more persuasive tone, maintaining the academic register.”\n\n\n\n\nIn the context of SSH, prompt chaining is particularly useful for exploring different interpretative angles, verifying consistency between the formulation of the problem and the proposed approach, and refining the tone according to the target audience (Scientific committee, Funder, Journal).\n👉🏻 To reducing the time needed to produce a satisfactory version, this methodology encourages greater reflection on the editorial process, transforming the AI tool into an intellectually stimulating interlocutor and not just a mere automated assistant.\n\n\n10.3 2. Scripts and APIs for metadata and formats\nWithin the Academic writing workflow, certain tasks such as managing bibliographic metadata, converting document formats and generating automatic summaries are operationally intensive yet cognitively low-impact.\nIn this context, the integration of custom scripts and APIs (Application Programming Interfaces) enables the automation of repetitive processes, facilitating standardisation and reducing the risk of errors.\nAPI-based tools such as Zotero, CrossRef, Semantic Scholar or OpenAI can be queried to extract, normalise, and reformat bibliographic metadata according to specific citation styles (APA, MLA, Chicago, etc.), ensuring coherence and accuracy even during the final revision stages.\nSimilarly, Python scripts or plug-ins for editorial environments (such as Word or LaTeX) allow for rapid document conversion between formats (e.g., from Word to PDF with embedded metadata or from LaTeX to XML for online submission).\n\nSee Python Scripts For Web Scraping Metadata From Descriptions About The Datasets Of The International Scenario Of Research Data Repositories\nSee pybliometrics: Scriptable bibliometrics using a Python interface to Scopus\nSee The Modern Methods of Data Analysis in Social Research: Python Programming Language and its Pandas Library as an Example- a Theoretic Study\n\nIn particular, the combined use of APIs and scripts allows you to automate:\n\nThe generation of complete bibliographic references (in APA, MLA, Chicago, etc.);\nThe retrieval of scientific article abstracts\nThe conversion of files (PDF → text, BibTeX → JSON);\nThe creation of summaries of the state of the art.\nThe generation of automatic summaries of scientific articles or regulatory acts, to be used as study material, as a basis for developing original abstracts, or as support for the literature analysis phase, with significant savings in time and cognitive load.\n\nIn the SSH, where researchers often work with heterogeneous sources and multi-methodological approaches, these automations facilitate more robust information management, making it easier to organise sources, track references, and prepare materials ready for submission.\n\n\n10.4 3. Proactive workflows and intelligent time management\nIn today’s Academic environment, characterized by increasing organizational complexity and a multitude of simultaneous tasks (teaching, research, dissemination, planning), the ability to effectively manage time and priorities is a strategic skill.\n👉🏻 In this scenario, AI offers concrete tools for building proactive workflows, i.e. organisational processes in which technologies, often based on AI automation, are able to anticipate user needs and intervene before problems or explicit needs arise.\n\n\n\n\n\n\nTipThese systems not only optimize individual organization, but also build an information ecosystem that reduces the impact of the executive load and stimulates continuous reflection on scientific priorities.\n\n\n\n\n\n\n👉🏻Through the integration of advanced language models, task management systems, and personal productivity tools, it is possible to design semi-automated workflows that do not merely react to deadlines but help to anticipate them and distribute them in a sustainable manner.\nThe adoption of prompt scheduling systems, designed to proactively generate contextualised reminders and preliminary drafts in relation to Academic deadlines (e.g. calls for papers, project submissions or article reviews), is an effective strategy for mitigating procrastination and supporting the continuity and regularity of scientific output.\nIn addition, some AI platforms allow the integration of smart calendars which, in addition to simply recording events, provide personalised suggestions on the most suitable time slots for activities such as writing, data analysis or reading, taking into account both the estimated cognitive load and individual productivity patterns.\nIntegrated with time tracking tools, these systems promote more conscious management of time resources, facilitating an optimal balance between high cognitive value activities (writing, design, reflection) and repetitive operational tasks (formatting, data entry, uploading to platforms).\nFinally, proactive workflows can be used to schedule periodic text reviews and incorporate automatic feedback on consistency, readability, and style.\nAn incremental approach, distributed in short, planned sessions, helps mitigate the mental overload typical of production concentrated around deadlines, promoting a measurable improvement in the overall quality of Academic output.\n\n\n10.5 4. Rapid metrics for text quality\nIn the context of the SSH, where text production is not only a means of communication but also an expression of theoretical argumentation, critical analysis and conceptual density, linguistic and rhetorical quality control takes on strategic importance.\nUnlike disciplinary fields characterised by more standardised textual structures, the SSH field is distinguished by stylistic variety, semantic complexity and a plurality of rhetorical registers.\nIn this context, the adoption of automated text quality metrics, based on linguistic models or Natural Language Processing (NLP) tools, allows for the insertion of an intermediate phase of analysis and revision into the writing process, which is useful for identifying latent critical issues and opportunities for improvement.\n\n\n\n\n\n\nWarningThese metrics have no prescriptive or evaluative value in the strict sense, but can act as “reflective aids”, increasing stylistic awareness and promoting greater adherence of the text to the expectations of the academic audience or evaluation committee.\n\n\n\n\n\n\nText quality assessment metrics can be divided into four main areas:\na. Clarity and readability\nThese include parameters such as average sentence length, frequency of passive voice, density of subordinate clauses, and readability indices (e.g. Flesch-Kincaid or Gunning Fog Index). These measures are particularly relevant when the text is written in a language other than the author’s mother tongue.\nb. Textual consistency and cohesion\nThese relate to the recurrence and distribution of key concepts, semantic consistency between different sections, and the presence and variety of logical and discursive connectives (furthermore, in contrast, consequently), which are essential indicators for ensuring a solid flow of argumentation.\nc. Lexical richness and terminological density\nThese assess lexical diversity, the balance between specialist terminology and the accessibility of the text, and the frequency of rare terms. The latter aspect is useful for identifying phenomena of hyper-specialisation or, conversely, excessive generality.\nd. Tone, register and epistemic markers\nThese consider the appropriateness of the academic tone, the use of attenuating formulas (“it is possible to hypothesise that…”“), intensifiers (“clearly”, “without doubt”) and implicit evaluations, elements that are of significant argumentative relevance in the social sciences and humanities.\n\n\n\n\n\n\nWarningSome digital tools, including advanced language models, can provide concise but detailed assessments based on these criteria, operating both in real time and on demand. Among other things, these tools allow you to compare different versions of the same text or evaluate multiple abstracts, making it easier to choose the most suitable wording for a specific call.\n\n\n\n\n\n\n\n10.5.0.1 Examples of tools for rapid text quality analysis\n\nHemingway Editor\nMeasures readability, flags complex or passive sentences, and suggests stylistic simplifications. It is particularly useful for clarifying argumentative passages and introductory sections.\nWritefull for Overleaf\nAI-based plugin that analyses scientific texts, providing feedback on academic vocabulary, correct use of prepositions and sentence structure. It is a great support for those who write in English as a non-native language.\nGPT-based Quality Assessment (GPT-based assessment through targeted prompts)\nThrough calibrated instructions, models such as ChatGPT or Claude can return:\n• an analysis of tone and rhetorical effectiveness\n• an assessment of logical and argumentative coherence\n• a summary score based on predefined criteria (clarity, relevance, impact)\nTextStat or Linguakit\nThey are open-source NLP tools and provide quantitative analyses relating to:\n• average sentence length\n• frequency and distribution of keywords\n• terminological variety\n• internal cohesion and coherence of the text\nLanguageTool\nMultilingual grammar and style checker that detects spelling, grammar and syntax errors, flags complex or overly long sentences, repetitions and excessive use of the passive voice. Useful for improving the clarity and readability of Academic and popular texts, with add-ons available for browsers, Word, Google Docs and various text editors.\n\n\n\n\n\n\n\nNoteIn the field of SSH, where textual quality is an integral part of scientific authority and where argumentative rhetoric carries decisive weight, these metrics represent an opportunity to systematise formal revision without flattening stylistic originality.They can be adopted not only for individual production, but also in collaborative contexts, serving as a neutral interface for stylistic negotiation and the construction of a shared voice within research groups or collective editorial teams.\n\n\n\n\n\n\n\n\n\n10.6 5. Cognitive offloading and selective attention\nIn the Academic production cycle of the SSH, traditionally characterised by a high incidence of iterative activities, such as annotation, source verification, syntactic editing and terminological standardisation, cognitive offloading to GAI tools represents a targeted strategy for reallocating attentional resources.\nDelegating low-epistemic operations to automated systems not only preserves cognitive load, but also allows researchers to focus their attention on conceptually dense phases: defining research questions, constructing theoretical frameworks, and validating arguments.\nAmong the most significant uses of GAI in SSH are:\n\nControlled semantic reformulation, used to test the conceptual soundness of a paragraph by subjecting it to lexical variations that do not alter its propositional content.\nAutomatic pre-annotation of qualitative texts (e.g. interviews, historical documents, literary sources), aimed at facilitating manual coding according to specific theoretical models, such as Grounded Theory or Frame Analysis.\nSelective verification of intertextual consistency, using models capable of mapping the logical connection between text sections (e.g., between abstracts and conclusions, or between hypotheses and data).\nPerforming meta-editorial tasks, including checking readability, converting to standard editorial formats (APA, MLA) and analysing syntactic dependencies to identify opaque constructions.\n\n👉 In such applications, AI acts as an attentional amplifier: it does not make interpretative decisions, but creates the conditions for a more targeted selection of cognitively salient stimuli.\nThis approach embodies the logic of selective attention scaffolding, in which the algorithm provides a neutral, interference-free context, encouraging the emergence of critical thinking in a clearer form.\nEvidence from learning sciences and human–AI interaction studies indicates that the ability to orchestrate distributed cognitive environments, integrating artificial agents, digital resources and human skills, is a qualifying indicator of advanced epistemological literacy.\n\n\n\n\n\n\nNoteStrategic allocation of structural operations to AI not only increases productivity but also strengthens the quality of conceptual processing and metacognitive awareness throughout the entire research cycle.",
    "crumbs": [
      "Part 2: Advanced applications of AI in Research and Creativity",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Writing calls, grants, abstracts, and intelligent automations</span>"
    ]
  },
  {
    "objectID": "2-20250830_GrantAutom.html#prompt-chaining-for-titles-and-abstracts",
    "href": "2-20250830_GrantAutom.html#prompt-chaining-for-titles-and-abstracts",
    "title": "10  Writing calls, grants, abstracts, and intelligent automations",
    "section": "",
    "text": "In the process of generating titles and abstracts, one of the most effective applications of GAI is prompt chaining, i.e. the structured concatenation of text commands.\nThrough the use of progressive sequences of controlled prompts, the researcher can guide the linguistic model through distinct but interconnected phases, which simulate the iterative reasoning typical of academic writing.\n\nWe start with a simple thematic statement to generate a list of titles consistent with the disciplinary domain and style required by the editorial context.\nNext, the selected titles are developed into draft abstracts, structured according to the specific rhetorical conventions of the field (objectives, method, expected results, implications).\nFinally, again using targeted prompts, the text is stylistically revised and linguistically optimised to improve readability, impact and adherence to formal requirements.\n\n\n\n\n\n\n\nTipFor example, a typical flow might look like this:\n\n\n\n\nPrompt 1: “Generate 5 Academic titles for a project on the relationship between AI and political discourse analysis.”\n\nPrompt 2: “Write a draft abstract for title X, using a formal style and no more than 150 words.”\n\nPrompt 3: “Reformulate the text with a more persuasive tone, maintaining the academic register.”\n\n\n\n\nIn the context of SSH, prompt chaining is particularly useful for exploring different interpretative angles, verifying consistency between the formulation of the problem and the proposed approach, and refining the tone according to the target audience (Scientific committee, Funder, Journal).\n👉🏻 To reducing the time needed to produce a satisfactory version, this methodology encourages greater reflection on the editorial process, transforming the AI tool into an intellectually stimulating interlocutor and not just a mere automated assistant.",
    "crumbs": [
      "Part 2: Advanced applications of AI in Research and Creativity",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Writing calls, grants, abstracts, and intelligent automations</span>"
    ]
  },
  {
    "objectID": "2-20250830_GrantAutom.html#scripts-and-apis-for-metadata-and-formats",
    "href": "2-20250830_GrantAutom.html#scripts-and-apis-for-metadata-and-formats",
    "title": "10  Writing calls, grants, abstracts, and intelligent automations",
    "section": "10.3 2. Scripts and APIs for metadata and formats",
    "text": "10.3 2. Scripts and APIs for metadata and formats\nWithin the Academic writing workflow, certain tasks such as managing bibliographic metadata, converting document formats and generating automatic summaries are operationally intensive yet cognitively low-impact.\nIn this context, the integration of custom scripts and APIs (Application Programming Interfaces) enables the automation of repetitive processes, facilitating standardisation and reducing the risk of errors.\nAPI-based tools such as Zotero, CrossRef, Semantic Scholar or OpenAI can be queried to extract, normalise, and reformat bibliographic metadata according to specific citation styles (APA, MLA, Chicago, etc.), ensuring coherence and accuracy even during the final revision stages.\nSimilarly, Python scripts or plug-ins for editorial environments (such as Word or LaTeX) allow for rapid document conversion between formats (e.g., from Word to PDF with embedded metadata or from LaTeX to XML for online submission).\n\nSee Python Scripts For Web Scraping Metadata From Descriptions About The Datasets Of The International Scenario Of Research Data Repositories\nSee pybliometrics: Scriptable bibliometrics using a Python interface to Scopus\nSee The Modern Methods of Data Analysis in Social Research: Python Programming Language and its Pandas Library as an Example- a Theoretic Study\n\nIn particular, the combined use of APIs and scripts allows you to automate:\n\nThe generation of complete bibliographic references (in APA, MLA, Chicago, etc.);\nThe retrieval of scientific article abstracts\nThe conversion of files (PDF → text, BibTeX → JSON);\nThe creation of summaries of the state of the art.\nThe generation of automatic summaries of scientific articles or regulatory acts, to be used as study material, as a basis for developing original abstracts, or as support for the literature analysis phase, with significant savings in time and cognitive load.\n\nIn the SSH, where researchers often work with heterogeneous sources and multi-methodological approaches, these automations facilitate more robust information management, making it easier to organise sources, track references, and prepare materials ready for submission.",
    "crumbs": [
      "Part 2: Advanced applications of AI in Research and Creativity",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Writing calls, grants, abstracts, and intelligent automations</span>"
    ]
  },
  {
    "objectID": "2-20250830_GrantAutom.html#proactive-workflows-and-intelligent-time-management",
    "href": "2-20250830_GrantAutom.html#proactive-workflows-and-intelligent-time-management",
    "title": "10  Writing calls, grants, abstracts, and intelligent automations",
    "section": "10.4 3. Proactive workflows and intelligent time management",
    "text": "10.4 3. Proactive workflows and intelligent time management\nIn today’s Academic environment, characterized by increasing organizational complexity and a multitude of simultaneous tasks (teaching, research, dissemination, planning), the ability to effectively manage time and priorities is a strategic skill.\n👉🏻 In this scenario, AI offers concrete tools for building proactive workflows, i.e. organisational processes in which technologies, often based on AI automation, are able to anticipate user needs and intervene before problems or explicit needs arise.\n\n\n\n\n\n\nTipThese systems not only optimize individual organization, but also build an information ecosystem that reduces the impact of the executive load and stimulates continuous reflection on scientific priorities.\n\n\n\n\n\n\n👉🏻Through the integration of advanced language models, task management systems, and personal productivity tools, it is possible to design semi-automated workflows that do not merely react to deadlines but help to anticipate them and distribute them in a sustainable manner.\nThe adoption of prompt scheduling systems, designed to proactively generate contextualised reminders and preliminary drafts in relation to Academic deadlines (e.g. calls for papers, project submissions or article reviews), is an effective strategy for mitigating procrastination and supporting the continuity and regularity of scientific output.\nIn addition, some AI platforms allow the integration of smart calendars which, in addition to simply recording events, provide personalised suggestions on the most suitable time slots for activities such as writing, data analysis or reading, taking into account both the estimated cognitive load and individual productivity patterns.\nIntegrated with time tracking tools, these systems promote more conscious management of time resources, facilitating an optimal balance between high cognitive value activities (writing, design, reflection) and repetitive operational tasks (formatting, data entry, uploading to platforms).\nFinally, proactive workflows can be used to schedule periodic text reviews and incorporate automatic feedback on consistency, readability, and style.\nAn incremental approach, distributed in short, planned sessions, helps mitigate the mental overload typical of production concentrated around deadlines, promoting a measurable improvement in the overall quality of Academic output.",
    "crumbs": [
      "Part 2: Advanced applications of AI in Research and Creativity",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Writing calls, grants, abstracts, and intelligent automations</span>"
    ]
  },
  {
    "objectID": "2-20250830_GrantAutom.html#rapid-metrics-for-text-quality",
    "href": "2-20250830_GrantAutom.html#rapid-metrics-for-text-quality",
    "title": "10  Writing calls, grants, abstracts, and intelligent automations",
    "section": "10.5 4. Rapid metrics for text quality",
    "text": "10.5 4. Rapid metrics for text quality\nIn the context of the SSH, where text production is not only a means of communication but also an expression of theoretical argumentation, critical analysis and conceptual density, linguistic and rhetorical quality control takes on strategic importance.\nUnlike disciplinary fields characterised by more standardised textual structures, the SSH field is distinguished by stylistic variety, semantic complexity and a plurality of rhetorical registers.\nIn this context, the adoption of automated text quality metrics, based on linguistic models or Natural Language Processing (NLP) tools, allows for the insertion of an intermediate phase of analysis and revision into the writing process, which is useful for identifying latent critical issues and opportunities for improvement.\n\n\n\n\n\n\nWarningThese metrics have no prescriptive or evaluative value in the strict sense, but can act as “reflective aids”, increasing stylistic awareness and promoting greater adherence of the text to the expectations of the academic audience or evaluation committee.\n\n\n\n\n\n\nText quality assessment metrics can be divided into four main areas:\na. Clarity and readability\nThese include parameters such as average sentence length, frequency of passive voice, density of subordinate clauses, and readability indices (e.g. Flesch-Kincaid or Gunning Fog Index). These measures are particularly relevant when the text is written in a language other than the author’s mother tongue.\nb. Textual consistency and cohesion\nThese relate to the recurrence and distribution of key concepts, semantic consistency between different sections, and the presence and variety of logical and discursive connectives (furthermore, in contrast, consequently), which are essential indicators for ensuring a solid flow of argumentation.\nc. Lexical richness and terminological density\nThese assess lexical diversity, the balance between specialist terminology and the accessibility of the text, and the frequency of rare terms. The latter aspect is useful for identifying phenomena of hyper-specialisation or, conversely, excessive generality.\nd. Tone, register and epistemic markers\nThese consider the appropriateness of the academic tone, the use of attenuating formulas (“it is possible to hypothesise that…”“), intensifiers (“clearly”, “without doubt”) and implicit evaluations, elements that are of significant argumentative relevance in the social sciences and humanities.\n\n\n\n\n\n\nWarningSome digital tools, including advanced language models, can provide concise but detailed assessments based on these criteria, operating both in real time and on demand. Among other things, these tools allow you to compare different versions of the same text or evaluate multiple abstracts, making it easier to choose the most suitable wording for a specific call.\n\n\n\n\n\n\n\n10.5.0.1 Examples of tools for rapid text quality analysis\n\nHemingway Editor\nMeasures readability, flags complex or passive sentences, and suggests stylistic simplifications. It is particularly useful for clarifying argumentative passages and introductory sections.\nWritefull for Overleaf\nAI-based plugin that analyses scientific texts, providing feedback on academic vocabulary, correct use of prepositions and sentence structure. It is a great support for those who write in English as a non-native language.\nGPT-based Quality Assessment (GPT-based assessment through targeted prompts)\nThrough calibrated instructions, models such as ChatGPT or Claude can return:\n• an analysis of tone and rhetorical effectiveness\n• an assessment of logical and argumentative coherence\n• a summary score based on predefined criteria (clarity, relevance, impact)\nTextStat or Linguakit\nThey are open-source NLP tools and provide quantitative analyses relating to:\n• average sentence length\n• frequency and distribution of keywords\n• terminological variety\n• internal cohesion and coherence of the text\nLanguageTool\nMultilingual grammar and style checker that detects spelling, grammar and syntax errors, flags complex or overly long sentences, repetitions and excessive use of the passive voice. Useful for improving the clarity and readability of Academic and popular texts, with add-ons available for browsers, Word, Google Docs and various text editors.\n\n\n\n\n\n\n\nNoteIn the field of SSH, where textual quality is an integral part of scientific authority and where argumentative rhetoric carries decisive weight, these metrics represent an opportunity to systematise formal revision without flattening stylistic originality.They can be adopted not only for individual production, but also in collaborative contexts, serving as a neutral interface for stylistic negotiation and the construction of a shared voice within research groups or collective editorial teams.",
    "crumbs": [
      "Part 2: Advanced applications of AI in Research and Creativity",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Writing calls, grants, abstracts, and intelligent automations</span>"
    ]
  },
  {
    "objectID": "2-20250830_GrantAutom.html#cognitive-offloading-and-selective-attention",
    "href": "2-20250830_GrantAutom.html#cognitive-offloading-and-selective-attention",
    "title": "10  Writing calls, grants, abstracts, and intelligent automations",
    "section": "10.6 5. Cognitive offloading and selective attention",
    "text": "10.6 5. Cognitive offloading and selective attention\nIn the Academic production cycle of the SSH, traditionally characterised by a high incidence of iterative activities, such as annotation, source verification, syntactic editing and terminological standardisation, cognitive offloading to GAI tools represents a targeted strategy for reallocating attentional resources.\nDelegating low-epistemic operations to automated systems not only preserves cognitive load, but also allows researchers to focus their attention on conceptually dense phases: defining research questions, constructing theoretical frameworks, and validating arguments.\nAmong the most significant uses of GAI in SSH are:\n\nControlled semantic reformulation, used to test the conceptual soundness of a paragraph by subjecting it to lexical variations that do not alter its propositional content.\nAutomatic pre-annotation of qualitative texts (e.g. interviews, historical documents, literary sources), aimed at facilitating manual coding according to specific theoretical models, such as Grounded Theory or Frame Analysis.\nSelective verification of intertextual consistency, using models capable of mapping the logical connection between text sections (e.g., between abstracts and conclusions, or between hypotheses and data).\nPerforming meta-editorial tasks, including checking readability, converting to standard editorial formats (APA, MLA) and analysing syntactic dependencies to identify opaque constructions.\n\n👉 In such applications, AI acts as an attentional amplifier: it does not make interpretative decisions, but creates the conditions for a more targeted selection of cognitively salient stimuli.\nThis approach embodies the logic of selective attention scaffolding, in which the algorithm provides a neutral, interference-free context, encouraging the emergence of critical thinking in a clearer form.\nEvidence from learning sciences and human–AI interaction studies indicates that the ability to orchestrate distributed cognitive environments, integrating artificial agents, digital resources and human skills, is a qualifying indicator of advanced epistemological literacy.\n\n\n\n\n\n\nNoteStrategic allocation of structural operations to AI not only increases productivity but also strengthens the quality of conceptual processing and metacognitive awareness throughout the entire research cycle.",
    "crumbs": [
      "Part 2: Advanced applications of AI in Research and Creativity",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Writing calls, grants, abstracts, and intelligent automations</span>"
    ]
  },
  {
    "objectID": "2-20250903_Communic_Scient.html",
    "href": "2-20250903_Communic_Scient.html",
    "title": "11  Scientific and popular communication",
    "section": "",
    "text": "11.1 Introduction\nThe Research Communication is now an essential extension of Academic work.\nWhereas in the past the circulation of knowledge was almost exclusively limited to traditional publishing channels (peer-reviewed journals, conference proceedings, monographs) the current digital ecosystem has brought about a radical change in dissemination practices.\nThe growing demand for transparency, the need to promote open access to results, and the urgency of engaging with non-specialist audiences are forcing researchers to experiment with new formats that balance scientific rigour and readability. In this context, writing is no longer just a tool for intra-disciplinary validation, but also a means of mediation that must respond to the logic of visibility, traceability and social impact.\nHence the centrality of hybrid practices of scientific and popular communication, which are articulated along three main lines: Academic Blogging, Social Academia and Summaries for non-experts.\nWhile specialist writing remains the privileged place for certifying results, these practices of extended communication represent the infrastructure of impact, the vehicle through which research acquires visibility, public relevance and transformative power.\n👉🏻 Far from being ancillary operations, they are at the heart of a new ecology of knowledge, in which authority no longer derives solely from citation indices or impact factors, but also from the ability to generate dialogue, inform decision-making processes and take root in contemporary digital circuits.\nThe adoption of GAI as a support for scientific communication imposes an additional critical responsibility: it is up to the researcher to ensure that any form of algorithmic mediation does not compromise the conceptual fidelity or epistemic quality of the text.",
    "crumbs": [
      "Part 2: Advanced applications of AI in Research and Creativity",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Scientific and popular communication</span>"
    ]
  },
  {
    "objectID": "2-20250903_Communic_Scient.html#introduction",
    "href": "2-20250903_Communic_Scient.html#introduction",
    "title": "11  Scientific and popular communication",
    "section": "",
    "text": "In this way, GAI takes on the role of enabling technology. It does not replace authorial competence, but amplifies its possibilities, offering tools for reformulation, stylistic variation, discourse network analysis, and content optimisation.\nIn other words, the machine can facilitate the transposition of knowledge between different registers and audiences, but it remains the researcher’s task to ensure consistency, accuracy, and integrity.",
    "crumbs": [
      "Part 2: Advanced applications of AI in Research and Creativity",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Scientific and popular communication</span>"
    ]
  },
  {
    "objectID": "2-20250903_Communic_Scient.html#academic-blogging-1",
    "href": "2-20250903_Communic_Scient.html#academic-blogging-1",
    "title": "11  Scientific and popular communication",
    "section": "11.2 1. Academic Blogging",
    "text": "11.2 1. Academic Blogging\nAmong the scientific communication practices that have become established over the last two decades, Academic Blogging occupies a prominent position as a tool for mediating between the language of specialist writing and the readability requirements of digital communication.\nUnlike peer-reviewed journals (which represent the main channel of scientific validation), the primary purpose of Academic Blogs is not the epistemic certification of results, but their dissemination in broader contexts, often hybridising discursive registers and visibility strategies typical of the web.\nThe goal is therefore not to simplify the complexity of research, but to “refocus” it.\n👉🏻Reformulating a technical abstract into a 400-500 word text means translating concepts and data into a narrative structure that preserves methodological accuracy but is accessible to readers who do not necessarily belong to the narrow disciplinary circle.\nIn this sense, blogging serves a dual purpose:\n\nit broadens the potential impact of scientific work by making it accessible through search engines and digital platforms;\nit promotes the construction of an authorial presence that documents the intermediate stages of research over time, fostering transparency in the knowledge process and open science practices.\n\nRegular publications contribute to the creation of a coherent archive that not only reproduces the final results, but also makes visible the moments of reflection, conceptual revision and methodological experimentation.\nFrom a methodological point of view, “rewriting a technical abstract in the form of a post” involves several stages.\n\nIdentification of the conceptual core: the researcher isolates a key thesis or result and reformulates it in order to answer an implicit question from the reader that forms the backbone of the work.\nAI can act as a heuristic support, generating “alternative headlines” that can guide the reception of the text as titles formulated in an interrogative, assertive or informative way that stimulate the reader’s curiosity without sacrificing conceptual accuracy.\n⚠️ The final selection remains the prerogative of the researcher, whose task is to critically evaluate the algorithmic proposals in light of the disciplinary context and target audience.\n👉 Example: from the abstract “We analysed the impact of open peer review practices on the quality of publications in the social sciences”, the headline “Open peer review: does it really improve the quality of articles in the social sciences?”\nNarrative restructuring: the task consists of transforming a text dense with technical terms into a linear narrative that facilitates comprehension.\nAI can intervene here as a cognitive offloading tool, producing alternative drafts that reduce terminological density or suggest more fluid rhetorical patterns, for example by proposing historical analogies or controlled metaphors that make the subject accessible without distorting it.\n⚠️ In this case too the researcher’s supervision is essential. If the algorithm generates excessive simplifications or ambiguous formulations, it is up to the author to reintroduce rigour and epistemic consistency.\n👉 Example: a graph in the article can become a simplified infographic or a descriptive paragraph that explains the trend in a discursive manner.\nOptimisation and dissemination: blog writing is influenced by the logic of the digital ecosystem made by visibility,indexing and interaction.\nThe calls to action at the end of the text aim to encourage the reader to take a specific action (read the full article, share the content, participate in a discussion) and can be refined with the help of GAI tools that suggest linguistic variations tailored to the target audience.\nThe same logic applies to the SEO meta description, a short summary of 150–160 characters that acts as an interface with search engines.\n⚠️ AI can propose multiple alternatives, optimised for different keyword clusters, while the final decision rests with the researcher, who verifies their semantic adherence to the original content.\n👉 Example of a meta description: “A study of open peer review practices shows how the dynamics of editorial quality in the social sciences are changing.”\n\nBlog writing thus becomes a laboratory for reflection in which researchers can experiment with new forms of discourse, test the reception of hypotheses still in the exploratory phase, and interact with wider communities of readers, including policy-makers, students, and the general public.\n\n\n\n\n\n\n\nThe use of AI, if kept within the limits of an instrumental support and never a substitute, enhances this process by offering a range of expressive and operational solutions that the human author filters, controls and critically integrates.\n\n\n\n\n\n11.2.0.0.1 Examples of blogs\n\nLSE Impact Blog (London School of Economics)= platform that collects contributions from researchers, rewritten in accessible language and accompanied by titles optimised for online dissemination.\n\nThe Conversation= offers articles written by academics but edited by communication professionals, with the explicit aim of mediating between scientific rigour and public enjoyment.\n\nScholarly Kitchen Blog= a privileged observatory on academic communication practices, showing how blogs can become not only a tool for dissemination, but also a place for critical reflection on editorial and scientific processes themselves.",
    "crumbs": [
      "Part 2: Advanced applications of AI in Research and Creativity",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Scientific and popular communication</span>"
    ]
  },
  {
    "objectID": "2-20250903_Communic_Scient.html#social-academia-structuring-tweetorials-and-linkedin-threads-1",
    "href": "2-20250903_Communic_Scient.html#social-academia-structuring-tweetorials-and-linkedin-threads-1",
    "title": "11  Scientific and popular communication",
    "section": "11.3 2. Social Academia: structuring tweetorials and LinkedIn threads",
    "text": "11.3 2. Social Academia: structuring tweetorials and LinkedIn threads\nThe growing centrality of social media in the contemporary communication ecosystem has also led to a redefinition of scientific dissemination practices.\nWhat is referred to as Social Academia is not merely an exercise in personal self-promotion, but rather a set of strategies for the micro-dissemination of research content that exploit the logic of digital platforms to broaden the circulation of knowledge.\nUnlike Academic Blogs, which focus on relatively long and narratively complex texts, Social Academia imposes reduced formats, rapid timing and constant competition for the reader’s attention.\n👉 The goal for everyone is to make research results visible, stimulate critical interaction and consolidate communities of practice and interest around scientific topics.\nThe use of platforms such as Twitter/X and LinkedIn has made central the tweetorial or thread format, i.e. a sequence of micro-texts arranged in logical succession, the construction of which requires specific rhetorical and strategic skills.\n\nSegmenting the topic into autonomous but connected units of meaning, capable of maintaining the reader’s attention and gradually guiding them towards the central thesis.\nChoosing the hashtags that plays an indexing and positioning role, as they connect the content to existing discursive networks, facilitating findability and entry into global thematic conversations.\nDeterming the timing the publication, important for the visibility of content that depends on when it is disseminated, in relation to the activity cycles of digital scientific communities.\n\n\n\n\n\n\n\nNoteAI can be incorporated into these practices in different ways, always subject to critical control by the researcher.\n\n\n\n\n\n\nDuring the thread design phase, GAI tools can help produce multiple drafts of the text sequence, suggesting alternative logical orders or identifying transition points between one micro-text and another.\n👉 The author retains the task of verifying epistemic consistency and fidelity to scientific data, correcting undue simplifications or lexical slips.\n👉 As for the choice of hashtags, AI can analyse industry trends and propose terminology clusters that optimise content visibility, avoiding the use of labels that are too generic or, conversely, overly sector-specific.\n👉 Even in the case of timing, predictive analysis algorithms can provide indications of the moments of maximum engagement probability, but it will always be up to the author to decide whether these metrics coincide with the needs of the target community.\n\n\n\n\n\n\nTipThe conscious use of Social Academia is not limited to increasing individual visibility, but contributes to the creation of collective discursive spaces in which research results can be discussed, contextualised and critically evaluated.\n\n\n\nIn this sense, social media become true “community laboratories”, where scientific knowledge meets diverse audiences and is measured against the speed and unpredictability of digital circuits.\n\n\n⚠️ When used as an auxiliary tool for generating drafts and analysing discursive networks, AI amplifies the researcher’s expressive possibilities, but cannot replace their epistemic and authorial responsibility.",
    "crumbs": [
      "Part 2: Advanced applications of AI in Research and Creativity",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Scientific and popular communication</span>"
    ]
  },
  {
    "objectID": "2-20250903_Communic_Scient.html#summary-for-non-experts-tldr-1",
    "href": "2-20250903_Communic_Scient.html#summary-for-non-experts-tldr-1",
    "title": "11  Scientific and popular communication",
    "section": "11.4 3. Summary for non-experts (TL;DR)",
    "text": "11.4 3. Summary for non-experts (TL;DR)\nThe growing complexity of scientific production has highlighted the need for textual devices capable of acting as an interface between academic research and audiences who do not share the same level of specialisation.\nIn this perspective short summaries, frequently referred to by the acronym TL;DR (Too Long; Didn’t Read), have taken on a central role in contemporary knowledge dissemination policies.\nOriginally created as colloquial formulas within digital communities, these summaries are now recognised as essential tools for conveying complex knowledge to audiences such as policy makers, administrators, journalists or interested citizens, who need immediate and pragmatic access to research results.\n\n\n\n\n\n\nTip\n\n\n\nTheir function is not limited to simplification, but rather consists in the construction of a cognitive transfer device capable of preserving the epistemic validity of the content while making it usable in heterogeneous decision-making and communication contexts.\n\n\nFrom a methodological point of view, writing a TL;DR involves a “rigorous selection of materials” which requires a few steps.\n\nStep 1 = Isolate the most significant result, i.e. the conceptual core that represents the contributory claim of the work and which must be understandable without the support of the entire argumentative apparatus. This operation requires an act of authorial interpretation, since not all data or empirical evidence has the same degree of communicative transferability.\nStep 2 = Articulate the content in an accessible linguistic register, free of unexplained technicalities, but at the same time resistant to the risk of trivialisation.\nStep 3 = Explicit the practical or political implications of the research: not a simple conclusion, but a projection that clarifies how the results may affect professional, regulatory or cultural practices.\n\n👉🏻 The introduction of GAI in this process opens up new perspectives, which must nevertheless be evaluated with critical caution. Automatic summarisation algorithms are capable of generating multiple summaries, with varying degrees of granularity, from the original text. These outputs can offer researchers a range of stylistic and structural possibilities, acting as catalysts for the final elaboration.\n\n\n\n\n\n\nWarningThe selection of content to be included cannot be left entirely to the machine: if left to its own devices, AI tends to favour statistically recurring information rather than conceptually decisive information, risking distorting the epistemic hierarchy of the work.\n\n\n\n⚠️ For this reason, the role of the author remains essential, both in validating the generated content and in refining the language to ensure semantic consistency and contextual appropriateness.\n\n\nIn more advanced stages, AI can also support optimisation for different audiences. For example, a 120-word summary is for a policy brief, a 280-character summary for an informative post, an introductory note for an institutional portal.\n👉🏻 In any case, the value of the process lies in the critical dialogue between human and artificial intelligence, not in the replacement of one with the other.\nThe strategic importance of TL;DR summaries can be fully understood by considering their political as well as communicative function. A text of just a few lines can guide regulatory choices, influence public opinion or help define an institution’s agenda.\nFar from being a limitation, brevity becomes a condition for effectiveness, since reducing a complex work to 100-150 words means carrying out a selection process that not only communicates, but also interprets and enhances the research.\n👉 In this sense, short summaries are an autonomous text genre that requires specific writing skills, rhetorical sensitivity and awareness of the social and institutional dynamics within which they will be placed.\n\n11.4.1 Further reading\n• OECD Policy Briefs: paradigmatic examples of texts that condense complex analyses into targeted summaries for policy makers, characterised by rigour and immediacy.\n• Semantic Scholar – TLDR function: open-access platform that automatically generates one-sentence summaries for millions of articles in computer science, biology and medicine, directly in search results.\n• Making Science Simple: Corpora for the Lay Summarisation of Scientific Literature: study proposing datasets with biomedical research texts associated with lay summaries written by experts, offering concrete examples of summaries accessible to non-specialists.\n• TLDR: Extreme Summarisation of Scientific Documents: example of extremely concise summaries — even of a single sentence — capable of capturing the main contribution of a scientific article, accompanied by annotated and validated TL;DR datasets.\n• AGU (American Geophysical Union) Practical Guide to Plain Language Summaries: open access PDF guide illustrating how to develop summaries in plain language, avoiding technical jargon and contextualising research for a non-expert audience.",
    "crumbs": [
      "Part 2: Advanced applications of AI in Research and Creativity",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Scientific and popular communication</span>"
    ]
  },
  {
    "objectID": "3-20250908_epistrasp.html",
    "href": "3-20250908_epistrasp.html",
    "title": "12  Epistemology and Transparency",
    "section": "",
    "text": "12.1 1. AI and the construction of “scientific truth”\nScientific research has historically been based on shared epistemological principles: verifiability, falsifiability and methodological transparency.\nThese criteria, developed within the framework of logical positivism and subsequently consolidated through Karl Popper’s falsificationism, have ensured that scientific results are accumulable, subject to critical scrutiny and open to intersubjective control.\nThe introduction of AI, and in particular generative systems based on LLMs, now forces us to rethink these assumptions.\nAI does not merely provide advanced computational tools, but intervenes directly in the production of knowledge by selecting, synthesising and organising sources.\nThis function reorients the processes of constructing “scientific truth”, redefining the relationship between data, theories and research communities.\nOne of the most significant transformations concerns the way in which AI participates in the selection and synthesis of sources.\nGenerative systems can draw on large text corpora, identify correlations and propose coherent syntheses, presenting them as information with scientific value.\n👉 However, the probabilistic logic that governs these models does not coincide with traditional epistemic criteria: what is produced is not the result of deductive or inductive reasoning, but the projection of a statistical distribution learned from the data.\nThe notion of “scientific truth” risks being progressively replaced by a form of “linguistic plausibility”, a discourse which, while presenting itself with syntactic coherence and rhetorical force, does not necessarily guarantee the verifiability of its content.\nThis dynamic requires a clear distinction between scientifically validated knowledge and output generated by AI systems, to prevent the rhetorical power of AI-generated language from being mistaken for scientific evidence.\nFrom this perspective, AI does not merely produce texts, but acts as a cognitive filtering device that implicitly guides the hierarchy of sources, the salience of concepts and the interpretative trajectories considered relevant.",
    "crumbs": [
      "Part 3: Ethical aspects and responsibility in the use of AI",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Epistemology and Transparency</span>"
    ]
  },
  {
    "objectID": "3-20250908_epistrasp.html#ai-and-the-construction-of-scientific-truth-1",
    "href": "3-20250908_epistrasp.html#ai-and-the-construction-of-scientific-truth-1",
    "title": "12  Epistemology and Transparency",
    "section": "",
    "text": "The result is a redefinition of the criteria of epistemic relevance, capable on the one hand of broadening access to knowledge and on the other of excluding minority perspectives or sources not represented in the training datasets.",
    "crumbs": [
      "Part 3: Ethical aspects and responsibility in the use of AI",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Epistemology and Transparency</span>"
    ]
  },
  {
    "objectID": "3-20250908_epistrasp.html#interpretability-and-explainable-artificial-intelligence-xai-1",
    "href": "3-20250908_epistrasp.html#interpretability-and-explainable-artificial-intelligence-xai-1",
    "title": "12  Epistemology and Transparency",
    "section": "12.2 2. Interpretability and eXplainable Artificial Intelligence (XAI)",
    "text": "12.2 2. Interpretability and eXplainable Artificial Intelligence (XAI)\nThe problem of the opacity of deep learning algorithms has given rise to a specific field of study known as eXplainable Artificial Intelligence (XAI).\nThe aim of this approach is to provide methodological and technical tools that make the decision-making processes of models “interpretable”, allowing researchers to understand why a particular inference or synthesis has been produced.\n👉 Interpretability is not only about the readability of the model, but also has epistemological significance: without the ability to explain the reasons for an output, the traceability necessary to recognise a result as scientific is lost.\nKey strategies include feature attribution techniques (such as LIME and SHAP), which identify which variables have contributed most to a prediction, and intrinsically interpretable models, designed to prioritise readability over computational complexity.\nAlthough none of these approaches completely eliminates opacity, they provide sufficient levels of transparency to reintroduce accountability and verifiability criteria into research processes.",
    "crumbs": [
      "Part 3: Ethical aspects and responsibility in the use of AI",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Epistemology and Transparency</span>"
    ]
  },
  {
    "objectID": "3-20250908_epistrasp.html#the-limits-of-opaque-architectures",
    "href": "3-20250908_epistrasp.html#the-limits-of-opaque-architectures",
    "title": "12  Epistemology and Transparency",
    "section": "12.3 3. The limits of opaque architectures",
    "text": "12.3 3. The limits of opaque architectures\nDespite advances in XAI, the most advanced architectures remain largely “opaque”.\nDeep learning models, especially those based on billions of parameters, cannot be fully interpreted by either developers or users.\nThis black box condition introduces an epistemological divide. Science, traditionally anchored in the reconstructibility of processes, finds itself dependent on systems that produce inferences whose internal mechanisms cannot be explained.\nThe problem is not only technical but also “conceptual”.\nAI does not operate through logical reasoning but through the reproduction of statistical patterns.\nAs a result, it does not distinguish between what is epistemically grounded and what is only probabilistically plausible.\n\n\n\n\n\n\nWarningThis characteristic opens up the possibility of systematic errors, textual hallucinations and bias amplification, phenomena that undermine scientific credibility if not accompanied by rigorous control and validation practices.",
    "crumbs": [
      "Part 3: Ethical aspects and responsibility in the use of AI",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Epistemology and Transparency</span>"
    ]
  },
  {
    "objectID": "3-20250908_epistrasp.html#transparency-as-an-epistemic-condition-1",
    "href": "3-20250908_epistrasp.html#transparency-as-an-epistemic-condition-1",
    "title": "12  Epistemology and Transparency",
    "section": "12.4 4. Transparency as an epistemic condition",
    "text": "12.4 4. Transparency as an epistemic condition\nThe issue of transparency is not limited to the technical readability of algorithms, but concerns the entire cycle of scientific production.\nIt becomes necessary to make explicit the criteria for data selection, processing methods and circumstances of AI use, so that the results can be subjected to intersubjective control.\nTransparency assumes an eminently epistemic function, since only by ensuring the possibility of collective reconstruction can AI outputs be prevented from turning into opaque products, shielded from critical scrutiny.\n\n\n\n\n\n\n Alongside the individual responsibility of researchers, who must accurately document the use of AI in their work, there is also institutional responsibility.\n\n\n\nInternational organisations, funding agencies and Academic communities have begun to define standards and guidelines aimed at preserving epistemic integrity.\n👉 These directives require systematic disclosure of AI use, together with auditing and reporting protocols.\nPublishing houses are also moving in this direction, requiring precise declarations on the use of generative systems in writing and editing processes.",
    "crumbs": [
      "Part 3: Ethical aspects and responsibility in the use of AI",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Epistemology and Transparency</span>"
    ]
  },
  {
    "objectID": "3-20250908_epistrasp.html#further-reading",
    "href": "3-20250908_epistrasp.html#further-reading",
    "title": "12  Epistemology and Transparency",
    "section": "12.5 Further Reading",
    "text": "12.5 Further Reading\n\nSee How the machine ‘thinks’: Understanding opacity in machine learning algorithms\n\n\nSee Towards A Rigorous Science of Interpretable Machine Learning\n\n\nSee Connecting ethics and epistemology of AI\n\n\nSee Digital epistemology: evaluating the credibility of knowledge generated by AI\n\n\nSee Towards a Manifesto for Cyber Humanities: Paradigms, Ethics, and Prospects\n\n\nSee The mythos of model interpretability\n\n\nSee Explanation in Artificial Intelligence: Insights from the Social Sciences",
    "crumbs": [
      "Part 3: Ethical aspects and responsibility in the use of AI",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Epistemology and Transparency</span>"
    ]
  },
  {
    "objectID": "3-20250908_intro.html",
    "href": "3-20250908_intro.html",
    "title": "13  Implications of the use of AI in research",
    "section": "",
    "text": "13.1 Key References\nThe question of epistemology and transparency in the use of AI in research processes is a decisive factor in the very legitimacy of contemporary science.\nThe adoption of Generative Systems is not merely an instrumental update, but has a profound impact on the logic of knowledge production, altering the criteria by which knowledge is constructed, validated and shared.\nThe GAI, in its ability to select, organise and synthesise sources, directly intervenes in the definition of “scientific truth”, expanding the possibilities of access to information but at the same time introducing new areas of opacity and fragility in terms of verifiability.\nThe tools developed in the field of Explainable AI (XAI) are a significant attempt to restore transparency to otherwise uninterpretable processes, but they fail to bridge the gap between the epistemic needs of the scientific community and the opaque complexity of deep architectures.\nThis transformation requires the Academic Community to reflect on the soundness of the founding principles of modern science:\n- the verifiability of results\n- the methodological transparency\n- the neutrality of sources\n- the individual and collective responsibility in the evaluation phases.\nIntroducing AI Systems, particularly their generative applications, tests these assumptions along four main lines:\nThe use of GAI in research cannot be considered a simple technical support, but must be treated as an epistemic object in its own right, capable of influencing the entire knowledge ecosystem.\n👉 On the one hand, AI significantly expands the analytical and synthetic capabilities of researchers, offering tools capable of processing huge amounts of data and generating hypotheses.\n👉 On the other hand, it introduces concrete risks, such as algorithmic opacity, the propagation of errors or biases, the weakening of traditional accountability mechanisms and the potential erosion of public trust in science.\nAnalysing these implications in depth means maintaining a constant tension between technological innovation and scientific integrity as a guiding principle, so that AI can be integrated without compromising the principles that guarantee the credibility of research.",
    "crumbs": [
      "Part 3: Ethical aspects and responsibility in the use of AI",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Implications of the use of AI in research</span>"
    ]
  },
  {
    "objectID": "3-20250908_intro.html#key-references",
    "href": "3-20250908_intro.html#key-references",
    "title": "13  Implications of the use of AI in research",
    "section": "",
    "text": "UNESCO Recommendation on the Ethics of Artificial Intelligence\nFirst global ethical framework on the use of AI, with principles on transparency, accountability and inclusion. (2021 International recommendation)\nEU Artificial Intelligence Act (AI Act)\nBinding regulations governing AI systems, including those used in research, with documentation and monitoring requirements. (2024 Legislation)\nEUI - European University Institute - Guidelines for the Responsible Use of Artificial Intelligence for Research\nDocument specific to the research context: emphasises disclosure, traceability and scientific integrity. (2024 Academic guidelinese)\nOn the Dangers of Stochastic Parrots: Can Language Models Be Too Big?\nCritical analysis of the risks of bias, opacity and epistemic impact of large language models. (2021 Conference paper (FAccT))\nOECD Principles on Artificial Intelligence\nPolicy principles for the responsible use of AI, also adopted by OECD member countries",
    "crumbs": [
      "Part 3: Ethical aspects and responsibility in the use of AI",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Implications of the use of AI in research</span>"
    ]
  },
  {
    "objectID": "3-20250915_biasqualita.html",
    "href": "3-20250915_biasqualita.html",
    "title": "14  Bias and source quality",
    "section": "",
    "text": "14.1 1. Sources of bias in training data\nOne of the most problematic aspects of using AI in research processes is the bias inherent in training data.\nThe data of training of models are never neutral, but reflect quantitative and qualitative imbalances that derive from their very composition.\nThe prevalence of certain languages, geographical areas or disciplines leads to an over-representation of specific contexts, while other perspectives remain marginal or absent.\n👉 The result is partial knowledge, which risks presenting itself as universal despite being based on incomplete foundations.\nAlongside these quantitative imbalances, AI systems incorporate implicit cultural biases.\nThe data reflect the values, conventions and symbolic hierarchies of the communities that generated them.\nWhen these perspectives are assumed to be neutral, algorithms end up replicating gender stereotypes, ethnocentric views or ideologically oriented representations, masking them behind the supposed objectivity of statistical calculation.\nA further source of criticism lies in methodological biases.\nThe collection, selection and normalisation of data respond to criteria that are not always made “explicit”, but which directly influence the outcomes of training.\nChoices relating to sampling methods, corpus cleaning or the definition of conceptual categories determine an invisible structure that guides the model’s inferences.\nThese different levels of distortion converge to generate a significant epistemic risk, i.e. the possibility that scientific research is based on already compromised knowledge, without these limitations being immediately recognisable.\nThe apparent authority of AI-generated texts can conceal systematic imbalances, imposing on the Academic Community the need to develop critical monitoring tools.\nThe issue of the traceability of algorithmic decisions therefore takes on importance. Audit logs, fairness metrics and data documentation practices are fundamental tools for making the path leading to a given output recognisable and verifiable.\nThese mechanisms do not solve the structural problems of datasets, but they introduce elements of accountability that allow researchers to evaluate the consistency of results with criteria of transparency and fairness.\nAnother critical element concerns the nature of the content produced by generative systems, which may include non-existent bibliographic references, invented citations, or distorted summaries of articles that have actually been published.\nThese phenomena, often referred to as algorithmic “hallucinations”, directly undermine scientific credibility if they are not promptly recognised and corrected.\nThe problem of bias, therefore, cannot be reduced to a technical issue, but also has ethical and political implications.\nResearch risks consolidating inequalities already present in society if shared criteria of inclusivity and fairness are not developed in the selection and management of data.\nThe GAI, from a potentially emancipatory resource, can become a device of exclusion if left unchecked.\n👉 For this reason, alongside individual critical vigilance, it is essential to promote institutional policies and editorial guidelines that guarantee the reliability of sources, the transparency of datasets and respect for the principles of pluralism and neutrality that underpin the scientific community.",
    "crumbs": [
      "Part 3: Ethical aspects and responsibility in the use of AI",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Bias and source quality</span>"
    ]
  },
  {
    "objectID": "3-20250915_biasqualita.html#sources-of-bias-in-training-data-1",
    "href": "3-20250915_biasqualita.html#sources-of-bias-in-training-data-1",
    "title": "14  Bias and source quality",
    "section": "",
    "text": "👉 In this sense, bias is not an accidental residue, but rather a structural consequence of the dataset construction process. \n\n\n\n\n\n\n\n\n\n\n\n\n 👉 The distinction between verified knowledge and probabilistic inference becomes an essential condition for preventing automation from compromising the epistemic reliability of research. \n\n\n\n\n\n\n\n\n\n\n The responsibility therefore falls on the researcher, who is called upon to rigorously distinguish between what belongs to the domain of scientific validity and what remains a statistical projection without empirical basis. \n\n\n\n\n\n\n14.1.1 Further Readings\n\nSee Types of bias in AI models Article on bias in AI - (data/dev/interaction bias)\nSee Quantification of bias in pre-existing content USC analysis of “common” facts used by AI\nSee Bibliographic hallucinations Nature research on citations invented by ChatGPT\nSee Frequency of hallucinations in scientific texts Study on the rate of false citations in psychology\nSee Open-source tools for managing and measuring bias AI Fairness 360 Toolkit (IBM)\nSee Generative image production analysis Bias in Generative AI\nSee AI and SSH Can Generative AI improve social science?\nSee Bias embedded in training data On the Dangers of Stochastic Parrots",
    "crumbs": [
      "Part 3: Ethical aspects and responsibility in the use of AI",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Bias and source quality</span>"
    ]
  },
  {
    "objectID": "3-20250915_biasqualita.html#strategies-for-the-traceability-and-verifiability-of-algorithmic-decisions-1",
    "href": "3-20250915_biasqualita.html#strategies-for-the-traceability-and-verifiability-of-algorithmic-decisions-1",
    "title": "14  Bias and source quality",
    "section": "14.2 2. Strategies for the traceability and verifiability of algorithmic decisions",
    "text": "14.2 2. Strategies for the traceability and verifiability of algorithmic decisions\nOne of the key issues in the use of AI systems in scientific research, is the ability to guarantee the reconstruction and validation of the decision-making processes that lead to a given output.\nUnlike human work, where the analysis and inference phases can be explicitly justified and discussed, algorithms operate through chains of calculations that are often opaque and difficult to interpret, even for the developers themselves.\n👉 To reduce this lack of transparency, it is necessary to introduce tools that allow the model’s performance to be systematically documented, monitored and evaluated.\nIn this perspective, audit logs represent a first fundamental mechanism.\nThey consist of udetailed records of the steps taken by the algorithm during data processing, from the input phase to the generation of the output, including any intermediate transformations.\nThese records not only allow the path followed by the system to be traced retrospectively, but also identify any critical issues, such as the use of partial sources or the application of undeclared selection criteria.\n\n\n\n\n\n\n 👉 In Academic context, this traceability has significant epistemic value, because it allows not only the final results to be subjected to intersubjective control, but also the entire process that produced them. \n\n\n\nAlongside process documentation, there is the issue of verifying the fairness of results. Algorithms are not just calculation tools, but devices that incorporate and convey specific hierarchies of relevance.\nFairness metrics have been developed precisely to make these dynamics “measurable”, translating dimensions often considered qualitative, such as inclusion, representativeness or non-discrimination, into numerical parameters.\nThrough comparative indicators, for example, it is possible to verify whether a model tends to favour certain categories of data or users over others, and whether these imbalances are statistically significant.\n👉 The joint application of audit logs and fairness metrics is not limited to technical monitoring, but opens up the possibility of developing shared responsibility in AI management.\n\n\n\n\n\n\n This means shifting the focus from isolated output to the production context, where methodological choices and evaluation criteria must be made explicit and subjected to collective discussion.\nIn this sense, the verifiability of algorithmic decisions becomes a structural element of scientific governance, a device aimed not only at ensuring greater transparency, but also at strengthening the epistemic legitimacy of the results obtained. \n\n\n\nFinally, it is important to emphasise that traceability and fairness assessment cannot be understood as occasional or ancillary practices, but must become an integral part of research procedures.\nThe systematic adoption of these tools is a necessary condition for preserving the reliability of scientific knowledge in the age of automation, preventing the complexity of generative models from translating into a deficit of critical control.\n\n14.2.1 Further Readings\n\nSee From Data to Insight: Why Traceability is Crucial for AI Success\nSee The Rise of AI Audit Trails: Ensuring Traceability in Decision-Making\nSee What is AI Traceability? Benefits, Tools & Best Practices",
    "crumbs": [
      "Part 3: Ethical aspects and responsibility in the use of AI",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Bias and source quality</span>"
    ]
  },
  {
    "objectID": "3-20250922_accountability.html",
    "href": "3-20250922_accountability.html",
    "title": "15  Accountability in peer review and responsibility",
    "section": "",
    "text": "15.1 1. Accountability in peer review and responsibility (theoretical framework)\nThe peer review process has always been one of the main mechanisms for epistemic regulation in science, ensuring that Academic contributions meet shared criteria of rigour, transparency and reliability.\nPeer review is therefore an exercise in responsibility, both individually, on the part of the reviewers called upon to evaluate a contribution, and collectively, on the part of the Academic and editorial communities that establish standards, procedures and codes of conduct.\nThe introduction of GAI in this context substantially changes the traditional framework.\nEntrusting algorithmic tools with tasks that support content evaluation or selection, implies a redistribution of responsibility, which is no longer exclusively entrusted to human expertise, but depends on a technical infrastructure that lacks ethical autonomy but capable of having a concrete impact on evaluation outcomes.\nThe answer cannot be entrusted to the algorithm itself, but must fall on the individuals and institutions that allow its use, defining clear rules of use, supervision protocols and operational limits.\nThis reconfiguration of the evaluation process highlights the risk of a progressive dilution of responsibility, with a consequent compromise of the “epistemic integrity” of peer review.\nTo avoid this drift, accountability must be conceived not as a mere attribution of individual blame, but as the construction of a multi-level governance system.\n👉 This implies that publishers, scientific committees, Universities and funding bodies share responsibility for the choices made through algorithmic tools, establishing binding editorial policies, codes of ethics and institutional guidelines.\nA further element concerns transparency.\nThe use of GAI in peer review must be explicitly stated in order to ensure the “methodological traceability” of the process and to protect the “autonomy of scientific judgement”.\nFailure to explicitly state the use of generative intelligent systems risks undermining the very legitimacy of editorial practices, fuelling suspicions of opacity and reducing the trust of the Academic Community and the public.\nSome journals and scientific societies have already begun to define guidelines that require the use of GAI to be made explicit, placing the human reviewer as the ultimate guarantor of the decision. Without human oversight, automation would result in a structural weakening of scientific responsibility.",
    "crumbs": [
      "Part 3: Ethical aspects and responsibility in the use of AI",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Accountability in peer review and responsibility</span>"
    ]
  },
  {
    "objectID": "3-20250922_accountability.html#accountability-in-peer-review-and-responsibility-theoretical-framework",
    "href": "3-20250922_accountability.html#accountability-in-peer-review-and-responsibility-theoretical-framework",
    "title": "15  Accountability in peer review and responsibility",
    "section": "",
    "text": "Its function goes far beyond the mere evaluation of content: it represents an exercise in collective responsibility involving reviewers, scientific communities and publishing institutions. \n\n\n\n\n\n\n\n\n\n\n\n THIS RAISES A CRUCIAL QUESTION ABOUT WHO IS RESPONSIBLE FOR DECISIONS THAT RESULT, DIRECTLY OR INDIRECTLY, FROM AN AI TOOL. \n\n\n\n\n\n\n\n\n\n\n\n\n\n In this sense, peer review is transformed from an exclusively human device into a hybrid system, in which the role of reviewers is not replaced but redistributed, in a dynamic balance between human control and technological support. \n\n\n\n\n\n15.1.1 Further Readings\n\nSee Accountable Artificial Intelligence: Holding Algorithms to Account\nSee Accountability in Artificial Intelligence: what it is and how it works\nSee Navigating and reviewing ethical dilemmas in AI development: Strategies for transparency, fairness, and accountability\nSee Ethical guidelines for the use of generative artificial intelligence and artificial intelligence-assisted tools in scholarly publishing: a thematic analysis\nSee Ensuring the Quality, Fairness, and Integrity of Journal Peer Review: A Possible Role of Editors\nSee Accountability in Computer Systems and Artificial Intelligence\nSee Closing the AI Accountability Gap: Defining an End-to-End Framework for Internal Algorithmic Auditing",
    "crumbs": [
      "Part 3: Ethical aspects and responsibility in the use of AI",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Accountability in peer review and responsibility</span>"
    ]
  },
  {
    "objectID": "3-20250922_accountability.html#risks-of-false-positivesnegatives-and-real-cases-empirical-problematisation",
    "href": "3-20250922_accountability.html#risks-of-false-positivesnegatives-and-real-cases-empirical-problematisation",
    "title": "15  Accountability in peer review and responsibility",
    "section": "15.2 2. Risks of false positives/negatives and real cases (empirical problematisation)",
    "text": "15.2 2. Risks of false positives/negatives and real cases (empirical problematisation)\nThe use of GAI in peer review processes raises a critical issue related to the reliability of the assessments produced.\nAlgorithms, as statistical systems trained on historical data, operate on the basis of probabilistic correlations and not according to epistemic criteria.\nThis approach entails a structural risk of classification errors, which can result in false positives and false negatives.\n\nA false positive occurs when an algorithm attributes scientific value to a contribution that lacks real substance, legitimising the dissemination of work that does not meet minimum quality criteria.\n👉 This introduces fallacious knowledge into the scientific circuit which, once published, tends to take root and spread, compromising the credibility of the disciplines involved.\nA false negative occurs when a valid and innovative article is unfairly penalised or excluded due to bias in the training data, overly standardised metrics or intrinsic limitations of the model.\n👉 These result in a loss of knowledge opportunities, as they exclude contributions that could have fuelled theoretical or practical advances.\n\nIn both cases, the systemic result is a weakening of the regulatory function of peer review and a deterioration of the trust that the Academic Community and civil society place in the scientific system.\nThese are not mere technical incidents, as they have substantial consequences on an epistemological and institutional level.\nThe reliability of the review depends not only on the technical sophistication of the algorithms, but also on the quality of the training data, the transparency of the metrics adopted, and the ability of institutions to put in place critical oversight mechanisms.\n👉 An empirical approach is needed, that considers specific cases not as isolated anomalies but as indicators of systemic risk, in order to enable the development of corrective mechanisms and safeguard the epistemic integrity of the assessment process.\n\n15.2.1 Further Readings\n\nSee Evaluating the efficacy of AI content detection tools in differentiating between human and AI-generated text\nSee AI is transforming peer review — and many scientists are worried.\nSee Artificial Intelligence to support publishing and peer review: A summary and review\nSee Generative Artificial Intelligence is infiltrating peer review process\nSee Artificial Intelligence in Peer Review: Enhancing Efficiency While Preserving Integrity",
    "crumbs": [
      "Part 3: Ethical aspects and responsibility in the use of AI",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Accountability in peer review and responsibility</span>"
    ]
  },
  {
    "objectID": "3-20250922_accountability.html#roles-conflicts-of-interest-and-self-audit-practices-ethical-institutional-issue",
    "href": "3-20250922_accountability.html#roles-conflicts-of-interest-and-self-audit-practices-ethical-institutional-issue",
    "title": "15  Accountability in peer review and responsibility",
    "section": "15.3 3. Roles, conflicts of interest and self-audit practices (ethical-institutional issue)",
    "text": "15.3 3. Roles, conflicts of interest and self-audit practices (ethical-institutional issue)\nReflection on the integration of GAI into peer review processes raises profound questions about the roles, responsibilities and conflicts of interest that arise when evaluation is no longer entrusted exclusively to human intervention.\nAcademic tradition has always assigned reviewers the task of ensuring, through their scientific and methodological expertise, the quality and soundness of the contributions submitted for evaluation.\n\n\n\n\n\n\n The introduction of algorithmic systems alters this balance, as a significant part of the decision-making process may depend on automated procedures, which are free from subjectivity but not exempt from biases implicit in the training data or operating models. \n\n\n\n👉 In this scenario, there is a potential disconnect between the formal responsibility of the reviewer, who continues to sign the judgement, and the substantive responsibility, which is partly transferred to the technological infrastructure.\nThe risk of conflict of interest is amplified when auditors do not disclose the use of AI tools or use them in an opaque manner, without making the extent of their intervention transparent in relation to the final judgement.\nFailure to disclose not only weakens the reliability of the assessment, but also raises questions of legitimacy, seeking to establish to what extent an auditor can be considered the author of a judgement when part of their argument derives from algorithmic processing.\n👉Clear and shared rules need to be defined that establish the limits of technology use and identify criteria for attributing authorship and scientific responsibility.\nSelf-auditing, understood as a practice of systematic self-assessment, becomes important. This implies that reviewers explicitly declare:\n- how GAI is used\n- the selection criteria applied\n- the methodological limitations encountered\n- any critical issues encountered.\nThis should not be reduced to a formal or bureaucratic requirement, but rather represent a practice of scientific reflection capable of highlighting, at an early stage, possible epistemic or ethical distortions generated by the use of algorithmic tools.\nSelf-monitoring thus takes the form of preventive responsibility, aimed not only at protecting the individual integrity of the reviewer, but also at safeguarding the overall credibility of the peer review process.\nThe institutional dimension emerges strongly alongside the individual one. Universities, Research institutions and Scientific publishers have the task of setting up independent auditing mechanisms, certification tools and binding guidelines to assist reviewers in the management of AI.\n👉 This multi-level approach allows ethical responsibility to be distributed fairly, avoiding it falling exclusively on the individual and instead building an ecosystem regulated by shared standards.\nIt thus becomes a collective governance mechanism:\n- reviewers contribute with their own self-audits\n- institutions define regulatory and operational frameworks\n- the scientific community exercises widespread epistemic control.\n\n\n\n\n\n\n Only through this multi-level architecture is it possible to reduce the risk of technological irresponsibility and transform the use of GAI into an opportunity to strengthen the quality and reliability of scientific judgement.\nTechnological innovation does not become a threat to the integrity of peer review, but an opportunity to consolidate its epistemic function and strengthen the link between individual responsibility, institutional responsibility and collective responsibility. \n\n\n\n\n15.3.1 Further Readings\n\nSee Integrity of Authorship and Peer Review Practices: Challenges and Opportunities for Improvement\nSee The risks of Artificial Intelligence in research: ethical and methodological challenges in the peer review process\nSee Peer Review in the Artificial Intelligence Era: A Call for Developing Responsible Integration Guidelines",
    "crumbs": [
      "Part 3: Ethical aspects and responsibility in the use of AI",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Accountability in peer review and responsibility</span>"
    ]
  },
  {
    "objectID": "3-20250922_accountability.html#documentation-methods-operational-methodological-solutions-logs-notes-metadata",
    "href": "3-20250922_accountability.html#documentation-methods-operational-methodological-solutions-logs-notes-metadata",
    "title": "15  Accountability in peer review and responsibility",
    "section": "15.4 4. Documentation methods (operational methodological solutions: logs, notes, metadata)",
    "text": "15.4 4. Documentation methods (operational methodological solutions: logs, notes, metadata)\nDocumentation is crucial to ensuring the accountability of peer review in an AI-mediated context, since the legitimacy of the evaluation process depends not only on the quality of the judgements made, but also on the possibility of reconstructing and verifying the entire process leading to those outcomes.\nReview cannot therefore be reduced to a final output, but must be accompanied by tools that ensure traceability, transparency and methodological verifiability.\nThe adoption of detailed audit logs becomes an indispensable “safeguard”: they systematically record the interactions between reviewers and algorithmic systems, including both the prompts and outputs generated and the changes and interpretations subsequently introduced by the human reviewer.\n👉 This distinction makes it possible to clearly isolate the human contribution from the algorithmic one, preventing opacity and reducing the risk of individual or institutional irresponsibility.\nIn addition to the logs, methodological notes are also important, not merely as a formal disclosure tool, but as an epistemic device that explains the degree and methods of AI involvement in the evaluation, thus allowing the scientific community to measure the robustness, reliability and limitations of the judgements produced.\nAdded to this is the value of structured metadata, which associates contextual information with each algorithmic intervention, such as:\n- the model used\n- the parameters adopted\n- the software versions\n- the reference datasets.\nStructured metadata, if standardised and made accessible, not only allows for the replicability of procedures, but also for the systematic comparison of different reviews, constituting an additional quality control tool.\n\n\n\n\n\n\n The set of logs, notes and metadata does not perform a purely technical function, but takes on an epistemological and institutional significance, anchoring peer review to a regime of shared responsibility and strengthening the Academic Community’s confidence in the legitimacy of evaluation procedures. \n\n\n\n👉 The development of common traceability and verification protocols is therefore an essential condition for the sustainable integration of AI into editorial processes, preventing it from becoming a factor of opacity or risk and instead transforming it into a resource capable of consolidating the regulatory function and epistemic quality of peer review.\n\n15.4.1 Further Readings\n\nSee The landscape of data and AI documentation approaches in the European policy context\nSee The necessity of AI audit standards boards",
    "crumbs": [
      "Part 3: Ethical aspects and responsibility in the use of AI",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Accountability in peer review and responsibility</span>"
    ]
  },
  {
    "objectID": "3-20250922_eticaselfaudit.html",
    "href": "3-20250922_eticaselfaudit.html",
    "title": "16  Towards a renewed operational ethics",
    "section": "",
    "text": "16.1 1. Tools and operational practices\nThe introduction of Artificial Intelligence systems into peer review processes requires the development of an operational ethics capable of translating abstract principles into concrete protocols, verifiable practices and standardised procedures.\nUnlike traditional “ethical frameworks”, which are predominantly declarative and prescriptive in nature, often limited to the enunciation of abstract values, operational ethics takes on a “performative function”, as it binds the actors involved to daily practices that give effect to the principles of transparency, traceability and accountability.\nThis perspective allows us to recognise that ethics is not an external or accessory constraint, but a “constitutive prerequisite” for the epistemic and institutional legitimacy of peer review.\nIt is clear that the use of algorithmic tools in evaluation processes introduces elements of opacity, automation and potential disempowerment which, if unregulated, risk compromising the regulatory function of peer review.\nOperational ethics is inherently dynamic and multi-layered.\nIt is “dynamic” because it requires continuous adaptation to technological changes in AI tools and to transformations in the contexts in which they operate.\nIt is “multi-layered” because it does not end with the individual responsibility of reviewers, but extends to the collective dimension of institutional governance, which involves publishers, research institutions and academic communities.\nIn order to define the framework for a renewed operational ethics, there are certain elements that are particularly important and deserve to be considered as methodological and applicative pillars.",
    "crumbs": [
      "Part 3: Ethical aspects and responsibility in the use of AI",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Towards a renewed operational ethics</span>"
    ]
  },
  {
    "objectID": "3-20250922_eticaselfaudit.html#tools-and-operational-practices-1",
    "href": "3-20250922_eticaselfaudit.html#tools-and-operational-practices-1",
    "title": "16  Towards a renewed operational ethics",
    "section": "",
    "text": "Mandatory disclosure procedures: reviewers and publishers must clearly report the use of AI systems, specifying the stage of use and distinguishing between human and algorithmic contributions.\nEthical compliance checklist: before validating their judgement, reviewers must answer standardised questions that guide them in assessing bias, proportionality and responsibility.\nRegular training for reviewers: editorial committees must establish ongoing training programmes to ensure understanding of the limitations of AI systems and critical supervision techniques.\nEditorial codes of conduct: journals should include sections dedicated to the use of AI in their guidelines, defining limits, documentation requirements and accountability criteria.\nInstitutional audit and control systems: supervisory committees should randomly check decisions supported by AI to ensure reliability and effective correction.\nMulti-level documentation: logs, methodological notes and metadata must be attached to reports to ensure the traceability of decisions and the replicability of processes.",
    "crumbs": [
      "Part 3: Ethical aspects and responsibility in the use of AI",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Towards a renewed operational ethics</span>"
    ]
  },
  {
    "objectID": "3-20250922_eticaselfaudit.html#ethical-verification-table-1",
    "href": "3-20250922_eticaselfaudit.html#ethical-verification-table-1",
    "title": "16  Towards a renewed operational ethics",
    "section": "16.2 2. Ethical verification table",
    "text": "16.2 2. Ethical verification table\nTables (see example below) can be used as an operational self-audit tool for reviewers and publishing institutions, translating ethical principles into practical questions and mandatory actions.\nEach domain corresponds to a “critical point in the evaluation process”, from initial transparency to final institutional control.\n👉 In this way, ethics does not remain an abstract reference, but becomes an integral part of daily review practice, promoting a balance between automation and human responsibility.\n\n\n\n\n\n\n\n\nDomain\nVerification Questions\nRequired Actions\n\n\n\n\nTransparency\nHas the use of AI been explicitly declared in the review report?\nInclude a methodological note distinguishing human contribution from algorithmic input.\n\n\nBias and Limitations\nHave potential biases in the data or models used been considered?\nDocument any critical issues and specify their impact on the evaluation.\n\n\nProportionality\nHas the intervention of AI been limited to support tasks rather than replacing human judgment?\nSpecify the role of AI and ensure that the final decision remains with the reviewer.\n\n\nAccountability\nWho assumes ultimate responsibility for the judgment (reviewer, editor, committee)?\nIndicate in the report the figure responsible for the final opinion.\n\n\nDocumentation\nHave logs, methodological notes, and metadata related to the use of AI been produced?\nAttach supporting materials to ensure traceability and replicability.\n\n\nTraining\nDoes the reviewer possess the minimum competencies required to understand the functioning and limitations of the AI employed?\nParticipate in training courses or consult editorial guidelines.\n\n\nInstitutional Oversight\nIs a random verification by the editorial board foreseen?\nInclude the review in a periodic compliance audit.\n\n\n\n\n\n\n\n\n\n A renewed operating ethics should not be seen as an “additional constraint”, but as a safeguard that allows AI systems to be integrated into peer review in a sustainable manner.\n\n\n\n\nOnly the combination of disclosure practices, continuous training, institutional audits and multi-level documentation can transform AI from a potential risk into a resource for consolidating scientific quality.\n\n16.2.1 Further Readings\n\nSee Ethics Guidelines for Trustworthy\nSee An Overview of Artificial Intelligence Ethics\nSee Artificial Intelligence, Humanistic Ethics",
    "crumbs": [
      "Part 3: Ethical aspects and responsibility in the use of AI",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Towards a renewed operational ethics</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "References"
    ]
  }
]